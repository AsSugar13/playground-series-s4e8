{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330740b1-5651-44ae-b802-55553948df88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d59ad212-322d-4c15-b934-edddb1842b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa77c4d-197f-4443-aae5-088c33d6049a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchmetrics import MetricCollection, AUROC, Recall, Precision, F1Score, Accuracy, MatthewsCorrCoef\n",
    "from torchmetrics.functional.classification import multiclass_matthews_corrcoef\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "934dc3c4-83fa-424b-a7b4-70a99f1580b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder, LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250bf253-0dbb-4bb8-a5cb-e295a5e784b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b755351-9fb4-4ede-a86d-2cb8b1476a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 13\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PATH = './'\n",
    "EVAL_SIZE = 0.2\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e926ea3-6245-4736-8512-16bb38311ac8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "962ee46f-d200-49b0-864a-ee906ea80f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(PATH+'train.csv', index_col='id')\n",
    "df_test = pd.read_csv(PATH+'test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "165310f6-4378-44bc-8540-071f76f90a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>does-bruise-or-bleed</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "      <th>stem-root</th>\n",
       "      <th>stem-surface</th>\n",
       "      <th>stem-color</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>has-ring</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>habitat</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e</td>\n",
       "      <td>8.80</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>u</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>w</td>\n",
       "      <td>4.51</td>\n",
       "      <td>15.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p</td>\n",
       "      <td>4.51</td>\n",
       "      <td>x</td>\n",
       "      <td>h</td>\n",
       "      <td>o</td>\n",
       "      <td>f</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>4.79</td>\n",
       "      <td>6.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t</td>\n",
       "      <td>z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e</td>\n",
       "      <td>6.94</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>b</td>\n",
       "      <td>f</td>\n",
       "      <td>x</td>\n",
       "      <td>c</td>\n",
       "      <td>w</td>\n",
       "      <td>6.85</td>\n",
       "      <td>9.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>l</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e</td>\n",
       "      <td>3.88</td>\n",
       "      <td>f</td>\n",
       "      <td>y</td>\n",
       "      <td>g</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>4.16</td>\n",
       "      <td>6.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>5.85</td>\n",
       "      <td>x</td>\n",
       "      <td>l</td>\n",
       "      <td>w</td>\n",
       "      <td>f</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>3.37</td>\n",
       "      <td>8.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  cap-diameter cap-shape cap-surface cap-color does-bruise-or-bleed  \\\n",
       "id                                                                            \n",
       "0      e          8.80         f           s         u                    f   \n",
       "1      p          4.51         x           h         o                    f   \n",
       "2      e          6.94         f           s         b                    f   \n",
       "3      e          3.88         f           y         g                    f   \n",
       "4      e          5.85         x           l         w                    f   \n",
       "\n",
       "   gill-attachment gill-spacing gill-color  stem-height  stem-width stem-root  \\\n",
       "id                                                                              \n",
       "0                a            c          w         4.51       15.39       NaN   \n",
       "1                a            c          n         4.79        6.48       NaN   \n",
       "2                x            c          w         6.85        9.93       NaN   \n",
       "3                s          NaN          g         4.16        6.53       NaN   \n",
       "4                d          NaN          w         3.37        8.36       NaN   \n",
       "\n",
       "   stem-surface stem-color veil-type veil-color has-ring ring-type  \\\n",
       "id                                                                   \n",
       "0           NaN          w       NaN        NaN        f         f   \n",
       "1             y          o       NaN        NaN        t         z   \n",
       "2             s          n       NaN        NaN        f         f   \n",
       "3           NaN          w       NaN        NaN        f         f   \n",
       "4           NaN          w       NaN        NaN        f         f   \n",
       "\n",
       "   spore-print-color habitat season  \n",
       "id                                   \n",
       "0                NaN       d      a  \n",
       "1                NaN       d      w  \n",
       "2                NaN       l      w  \n",
       "3                NaN       d      u  \n",
       "4                NaN       g      a  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3116945 entries, 0 to 3116944\n",
      "Data columns (total 21 columns):\n",
      " #   Column                Non-Null Count    Dtype  \n",
      "---  ------                --------------    -----  \n",
      " 0   class                 3116945 non-null  object \n",
      " 1   cap-diameter          3116941 non-null  float64\n",
      " 2   cap-shape             3116905 non-null  object \n",
      " 3   cap-surface           2445922 non-null  object \n",
      " 4   cap-color             3116933 non-null  object \n",
      " 5   does-bruise-or-bleed  3116937 non-null  object \n",
      " 6   gill-attachment       2593009 non-null  object \n",
      " 7   gill-spacing          1858510 non-null  object \n",
      " 8   gill-color            3116888 non-null  object \n",
      " 9   stem-height           3116945 non-null  float64\n",
      " 10  stem-width            3116945 non-null  float64\n",
      " 11  stem-root             359922 non-null   object \n",
      " 12  stem-surface          1136084 non-null  object \n",
      " 13  stem-color            3116907 non-null  object \n",
      " 14  veil-type             159452 non-null   object \n",
      " 15  veil-color            375998 non-null   object \n",
      " 16  has-ring              3116921 non-null  object \n",
      " 17  ring-type             2988065 non-null  object \n",
      " 18  spore-print-color     267263 non-null   object \n",
      " 19  habitat               3116900 non-null  object \n",
      " 20  season                3116945 non-null  object \n",
      "dtypes: float64(3), object(18)\n",
      "memory usage: 523.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.116941e+06</td>\n",
       "      <td>3.116945e+06</td>\n",
       "      <td>3.116945e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.309848e+00</td>\n",
       "      <td>6.348333e+00</td>\n",
       "      <td>1.115379e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.657931e+00</td>\n",
       "      <td>2.699755e+00</td>\n",
       "      <td>8.095477e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.320000e+00</td>\n",
       "      <td>4.670000e+00</td>\n",
       "      <td>4.970000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.750000e+00</td>\n",
       "      <td>5.880000e+00</td>\n",
       "      <td>9.650000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.240000e+00</td>\n",
       "      <td>7.410000e+00</td>\n",
       "      <td>1.563000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.067000e+01</td>\n",
       "      <td>8.872000e+01</td>\n",
       "      <td>1.029000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cap-diameter   stem-height    stem-width\n",
       "count  3.116941e+06  3.116945e+06  3.116945e+06\n",
       "mean   6.309848e+00  6.348333e+00  1.115379e+01\n",
       "std    4.657931e+00  2.699755e+00  8.095477e+00\n",
       "min    3.000000e-02  0.000000e+00  0.000000e+00\n",
       "25%    3.320000e+00  4.670000e+00  4.970000e+00\n",
       "50%    5.750000e+00  5.880000e+00  9.650000e+00\n",
       "75%    8.240000e+00  7.410000e+00  1.563000e+01\n",
       "max    8.067000e+01  8.872000e+01  1.029000e+02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "class                         0\n",
       "cap-diameter                  4\n",
       "cap-shape                    40\n",
       "cap-surface              671023\n",
       "cap-color                    12\n",
       "does-bruise-or-bleed          8\n",
       "gill-attachment          523936\n",
       "gill-spacing            1258435\n",
       "gill-color                   57\n",
       "stem-height                   0\n",
       "stem-width                    0\n",
       "stem-root               2757023\n",
       "stem-surface            1980861\n",
       "stem-color                   38\n",
       "veil-type               2957493\n",
       "veil-color              2740947\n",
       "has-ring                     24\n",
       "ring-type                128880\n",
       "spore-print-color       2849682\n",
       "habitat                      45\n",
       "season                        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train.head())\n",
    "display(df_train.info(show_counts=True))\n",
    "display(df_train.describe())\n",
    "display(df_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2198c320-8f37-4287-a57e-f5f85abd8ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>does-bruise-or-bleed</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "      <th>stem-root</th>\n",
       "      <th>stem-surface</th>\n",
       "      <th>stem-color</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>has-ring</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>habitat</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3116945</th>\n",
       "      <td>8.64</td>\n",
       "      <td>x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>11.13</td>\n",
       "      <td>17.12</td>\n",
       "      <td>b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>u</td>\n",
       "      <td>w</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116946</th>\n",
       "      <td>6.90</td>\n",
       "      <td>o</td>\n",
       "      <td>t</td>\n",
       "      <td>o</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c</td>\n",
       "      <td>y</td>\n",
       "      <td>1.27</td>\n",
       "      <td>10.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116947</th>\n",
       "      <td>2.00</td>\n",
       "      <td>b</td>\n",
       "      <td>g</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>6.18</td>\n",
       "      <td>3.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116948</th>\n",
       "      <td>3.47</td>\n",
       "      <td>x</td>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>4.98</td>\n",
       "      <td>8.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>w</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116949</th>\n",
       "      <td>6.17</td>\n",
       "      <td>x</td>\n",
       "      <td>h</td>\n",
       "      <td>y</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>6.73</td>\n",
       "      <td>13.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>y</td>\n",
       "      <td>t</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cap-diameter cap-shape cap-surface cap-color does-bruise-or-bleed  \\\n",
       "id                                                                           \n",
       "3116945          8.64         x         NaN         n                    t   \n",
       "3116946          6.90         o           t         o                    f   \n",
       "3116947          2.00         b           g         n                    f   \n",
       "3116948          3.47         x           t         n                    f   \n",
       "3116949          6.17         x           h         y                    f   \n",
       "\n",
       "        gill-attachment gill-spacing gill-color  stem-height  stem-width  \\\n",
       "id                                                                         \n",
       "3116945             NaN          NaN          w        11.13       17.12   \n",
       "3116946             NaN            c          y         1.27       10.75   \n",
       "3116947             NaN            c          n         6.18        3.14   \n",
       "3116948               s            c          n         4.98        8.51   \n",
       "3116949               p          NaN          y         6.73       13.70   \n",
       "\n",
       "        stem-root stem-surface stem-color veil-type veil-color has-ring  \\\n",
       "id                                                                        \n",
       "3116945         b          NaN          w         u          w        t   \n",
       "3116946       NaN          NaN          n       NaN        NaN        f   \n",
       "3116947       NaN          NaN          n       NaN        NaN        f   \n",
       "3116948       NaN          NaN          w       NaN          n        t   \n",
       "3116949       NaN          NaN          y       NaN          y        t   \n",
       "\n",
       "        ring-type spore-print-color habitat season  \n",
       "id                                                  \n",
       "3116945         g               NaN       d      a  \n",
       "3116946         f               NaN       d      a  \n",
       "3116947         f               NaN       d      s  \n",
       "3116948         z               NaN       d      u  \n",
       "3116949       NaN               NaN       d      u  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2077964 entries, 3116945 to 5194908\n",
      "Data columns (total 20 columns):\n",
      " #   Column                Non-Null Count    Dtype  \n",
      "---  ------                --------------    -----  \n",
      " 0   cap-diameter          2077957 non-null  float64\n",
      " 1   cap-shape             2077933 non-null  object \n",
      " 2   cap-surface           1631060 non-null  object \n",
      " 3   cap-color             2077951 non-null  object \n",
      " 4   does-bruise-or-bleed  2077954 non-null  object \n",
      " 5   gill-attachment       1728143 non-null  object \n",
      " 6   gill-spacing          1238369 non-null  object \n",
      " 7   gill-color            2077915 non-null  object \n",
      " 8   stem-height           2077963 non-null  float64\n",
      " 9   stem-width            2077964 non-null  float64\n",
      " 10  stem-root             239952 non-null   object \n",
      " 11  stem-surface          756476 non-null   object \n",
      " 12  stem-color            2077943 non-null  object \n",
      " 13  veil-type             106419 non-null   object \n",
      " 14  veil-color            251840 non-null   object \n",
      " 15  has-ring              2077945 non-null  object \n",
      " 16  ring-type             1991769 non-null  object \n",
      " 17  spore-print-color     178347 non-null   object \n",
      " 18  habitat               2077939 non-null  object \n",
      " 19  season                2077964 non-null  object \n",
      "dtypes: float64(3), object(17)\n",
      "memory usage: 332.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap-diameter</th>\n",
       "      <th>stem-height</th>\n",
       "      <th>stem-width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.077957e+06</td>\n",
       "      <td>2.077963e+06</td>\n",
       "      <td>2.077964e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.306192e+00</td>\n",
       "      <td>6.346509e+00</td>\n",
       "      <td>1.114837e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.685462e+00</td>\n",
       "      <td>2.698978e+00</td>\n",
       "      <td>8.100181e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.310000e+00</td>\n",
       "      <td>4.670000e+00</td>\n",
       "      <td>4.970000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.740000e+00</td>\n",
       "      <td>5.880000e+00</td>\n",
       "      <td>9.640000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.230000e+00</td>\n",
       "      <td>7.410000e+00</td>\n",
       "      <td>1.562000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.070000e+02</td>\n",
       "      <td>5.729000e+01</td>\n",
       "      <td>1.029100e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cap-diameter   stem-height    stem-width\n",
       "count  2.077957e+06  2.077963e+06  2.077964e+06\n",
       "mean   6.306192e+00  6.346509e+00  1.114837e+01\n",
       "std    4.685462e+00  2.698978e+00  8.100181e+00\n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00\n",
       "25%    3.310000e+00  4.670000e+00  4.970000e+00\n",
       "50%    5.740000e+00  5.880000e+00  9.640000e+00\n",
       "75%    8.230000e+00  7.410000e+00  1.562000e+01\n",
       "max    6.070000e+02  5.729000e+01  1.029100e+02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cap-diameter                  7\n",
       "cap-shape                    31\n",
       "cap-surface              446904\n",
       "cap-color                    13\n",
       "does-bruise-or-bleed         10\n",
       "gill-attachment          349821\n",
       "gill-spacing             839595\n",
       "gill-color                   49\n",
       "stem-height                   1\n",
       "stem-width                    0\n",
       "stem-root               1838012\n",
       "stem-surface            1321488\n",
       "stem-color                   21\n",
       "veil-type               1971545\n",
       "veil-color              1826124\n",
       "has-ring                     19\n",
       "ring-type                 86195\n",
       "spore-print-color       1899617\n",
       "habitat                      25\n",
       "season                        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_test.head())\n",
    "display(df_test.info(show_counts=True))\n",
    "display(df_test.describe())\n",
    "display(df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ebdc0b28-6788-4366-b3a5-0c3647d9fdfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in df_test.select_dtypes(include='object').columns:\n",
    "#     print(i)\n",
    "#     display(df_test[i].value_counts().head(10))\n",
    "#     display(df_train[i].value_counts().head(10))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "421f2478-ccdb-4763-8e04-a43812a792e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# veil-type category doesnt look so good\n",
    "# we do not treat NaN as different characteristic\n",
    "\n",
    "df_train.drop('veil-type', axis=1, inplace=True)\n",
    "df_test.drop('veil-type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4cc08a11-b596-4ffe-b76c-03af11eb5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['class']\n",
    "num_cols = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "cat_cols = ['cap-shape', 'cap-surface', 'cap-color',\n",
    "            'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'gill-color',\n",
    "            'stem-root', 'stem-surface', 'stem-color', #'veil-color',\n",
    "            'has-ring', 'ring-type', 'spore-print-color', 'habitat', 'season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4c1542ed-209a-46af-b14d-8832e148f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class2idx = {'e':0, 'p':1}\n",
    "df_train['class'].replace(class2idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "33a5bb4d-2704-4a82-84fa-ba8f94cf9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reindex(columns=num_cols+cat_cols+target, copy=False)\n",
    "df_test = df_test.reindex(columns=num_cols+cat_cols, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ca834bec-5576-4e80-8840-510490b0afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail_cutter(df, column, edge):\n",
    "    value_counts = df[column].value_counts()\n",
    "    idx2drop = value_counts[value_counts < edge].index\n",
    "    df.loc[df[column].isin(idx2drop), column] = np.nan\n",
    "\n",
    "for col in df_test.select_dtypes(include='object'):\n",
    "    tail_cutter(df_train, col, 1000)\n",
    "    tail_cutter(df_test, col, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "84dc5999-feb9-46db-a40f-f81c09bdafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_col in num_cols:\n",
    "    df_test.loc[df_test[num_col].isna(), num_col] = df_test[num_col].median()\n",
    "df_train.dropna(subset=num_cols, axis=0, inplace=True)\n",
    "df_test.dropna(subset=num_cols, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "53a4e2b9-10ee-43c4-b1de-825c94eedf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.fillna('nan', inplace=True)\n",
    "df_test.fillna('nan', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd2b7b3-99ac-4b1a-beab-fa77e539995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float32(x):\n",
    "    return(x.astype(np.float32))\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    ColumnTransformer([('num_encode',\n",
    "                        make_pipeline(StandardScaler(),\n",
    "                                      # FunctionTransformer(lambda x: x.astype(np.float32))\n",
    "                                      FunctionTransformer(func=to_float32),\n",
    "                                     ), num_cols),\n",
    "\n",
    "                       ('cat_encode',\n",
    "                       make_pipeline(OrdinalEncoder(),\n",
    "                                     # FunctionTransformer(lambda x: x.astype(np.int32))\n",
    "                                     FunctionTransformer(func=to_float32),\n",
    "                                    ), cat_cols)],\n",
    "\n",
    "                       remainder='drop')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e882cf-a78f-417d-b44c-053440d75bd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Embeddings Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab654893-b7db-49a3-8d0a-85b126a91c6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1202ec70-c421-44cd-9223-fd3c22e243e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Dataset(Dataset):\n",
    "    def __init__(self, df, is_eval=False, is_test=False):\n",
    "        \n",
    "        self.df = df\n",
    "        self.is_eval = is_eval\n",
    "        self.is_test = is_test\n",
    "\n",
    "        if self.is_test:\n",
    "            self.X = self.df\n",
    "        else:\n",
    "            self.X, self.y = self.df.drop(target, axis=1), self.df[target].values\n",
    "        \n",
    "        if self.is_test or self.is_eval:\n",
    "            self.X = pipeline.transform(self.X)\n",
    "        else:\n",
    "            self.X = pipeline.fit_transform(self.X)\n",
    "        gc.collect()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.is_test: return self.X[index], -1\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a51becf-4842-4735-8f43-2896832336b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim = None,\n",
    "                 layers_num = 2,\n",
    "                 layers_dim = 32,\n",
    "                 activation = nn.ReLU,\n",
    "                 emb_szs = None,\n",
    "                 dropout: float = 0.,\n",
    "                ):\n",
    "        super(FCNNet, self).__init__()\n",
    "    \n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(in_sz, out_sz) for in_sz, out_sz in emb_szs])\n",
    "    \n",
    "        fc_layers = []\n",
    "        fc_layers.append(nn.Linear(input_dim, layers_dim))\n",
    "        fc_layers.append(nn.LazyBatchNorm1d())\n",
    "        fc_layers.append(activation())\n",
    "        fc_layers.append(nn.Dropout(p=dropout))\n",
    "        for i in range(layers_num):\n",
    "            fc_layers.append(nn.Linear(layers_dim, layers_dim))\n",
    "            fc_layers.append(nn.LazyBatchNorm1d())\n",
    "            fc_layers.append(activation())\n",
    "            fc_layers.append(nn.Dropout(p=dropout))\n",
    "        fc_layers.append(nn.Linear(layers_dim, 1))\n",
    "    \n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_num = x[:, :3]\n",
    "        x_cat = x[:, 3:].long()\n",
    "        x_cat = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.embeddings)]\n",
    "        x_cat = torch.cat(x_cat, dim=-1)\n",
    "\n",
    "        x = torch.cat([x_num, x_cat], dim=-1).float()\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb0a2047-0e28-48bc-8837-f0c83d4c3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_fcnn(model, optimizer, loss_fn, train_dataloader, altmetric=None):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for X, y in tqdm.tqdm(train_dataloader):\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        preds = model(X)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(preds, y.float())\n",
    "        if altmetric: altmetric.update(preds,y.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "    \n",
    "def evaluate_fcnn(model, loss_fn, test_dataloader, altmetric=None):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y.float())\n",
    "        if altmetric: altmetric.update(preds,y.float())\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe6ea25-412b-48a0-8fee-0a280435485e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee2fcb96-4e52-4afe-9b61-5a9544bd1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_train, df_train_eval = train_test_split(df_train, test_size=EVAL_SIZE, random_state=SEED,\n",
    "#                                                  shuffle=True, stratify=df_train[target])\n",
    "\n",
    "# dataset_train = Base_Dataset(df_train_train)\n",
    "# dataset_eval = Base_Dataset(df_train_eval, is_eval=True)\n",
    "\n",
    "# display(len(dataset_train))\n",
    "# display(len(dataset_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0459e662-5181-48e0-8d65-14d85dc3a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# INPUT_DIM = sum([df_train_train[cat_col].nunique()//2+1\n",
    "#                  for cat_col in cat_cols]) + 3\n",
    "# LAYERS_NUM = 2\n",
    "# LAYERS_DIM = 256\n",
    "# ACTIVATION = nn.ReLU\n",
    "# EMB_SZS = [[df_train_train[cat_col].nunique(),\n",
    "#             df_train_train[cat_col].nunique()//2+1]\n",
    "#            for cat_col in cat_cols]\n",
    "# DROPOUT = 0.25\n",
    "\n",
    "# NUM_EPOCHS = 32\n",
    "# BATCH_SIZE = 1024*8 #2048\n",
    "# LR = 0.001 #0.001\n",
    "# WEIGHT_DECAY = 2e-5\n",
    "\n",
    "# fcnn = FCNNet(input_dim=INPUT_DIM,\n",
    "#               layers_num=LAYERS_NUM,\n",
    "#               layers_dim=LAYERS_DIM,\n",
    "#               activation=ACTIVATION,\n",
    "#               emb_szs=EMB_SZS,\n",
    "#               dropout=DROPOUT\n",
    "#               )\n",
    "\n",
    "# altmetric_train = MetricCollection([AUROC(task='binary'),\n",
    "#                                     Recall(task='binary'),\n",
    "#                                     Precision(task='binary'),\n",
    "#                                     F1Score(task='binary'),\n",
    "#                                     Accuracy(task='binary'),\n",
    "#                                     MatthewsCorrCoef(task='binary')\n",
    "#                                    ])\n",
    "# altmetric_eval = MetricCollection([AUROC(task='binary'),\n",
    "#                                    Recall(task='binary'),\n",
    "#                                    Precision(task='binary'),\n",
    "#                                    F1Score(task='binary'),\n",
    "#                                    Accuracy(task='binary'),\n",
    "#                                    MatthewsCorrCoef(task='binary')\n",
    "#                                   ])\n",
    "\n",
    "# fcnn.to(DEVICE)\n",
    "# altmetric_train.to(DEVICE)\n",
    "# altmetric_eval.to(DEVICE)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(dataset_train,\n",
    "#                                           batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                                           num_workers=8, drop_last=False)\n",
    "# evalloader = torch.utils.data.DataLoader(dataset_eval,\n",
    "#                                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                                          num_workers=8, drop_last=False)\n",
    "\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.AdamW(fcnn.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a30511f-c89a-43bd-899d-eabfe3ee2863",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# PRINT_EVERY = 4\n",
    "\n",
    "# for epoch in range(1, NUM_EPOCHS+1):\n",
    "#     train_loss = train_epoch_fcnn(fcnn, optimizer, loss_fn, trainloader, altmetric=altmetric_train)\n",
    "#     eval_loss = evaluate_fcnn(fcnn, loss_fn, evalloader, altmetric=altmetric_eval)\n",
    "#     print((f\"Epoch: {epoch}, Train loss: {train_loss:.5f}, Val loss: {eval_loss:.5f}\"))\n",
    "\n",
    "#     if ((epoch)%PRINT_EVERY==0):\n",
    "#         print('Train')\n",
    "#         for j in [(i, round(altmetric_train[i].compute().item(), 5))\n",
    "#                   for i in altmetric_train.keys()]: print(j)\n",
    "#         print()\n",
    "#         print('Test')\n",
    "#         for j in [(i, round(altmetric_eval[i].compute().item(), 5))\n",
    "#                   for i in altmetric_eval.keys()]: print(j)\n",
    "#         print()\n",
    "\n",
    "#     altmetric_train.reset()\n",
    "#     altmetric_eval.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddaa9354-f062-48b5-97ff-486ebfcc177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(PATH+'pipeline', 'wb') as fp:\n",
    "#     pickle.dump(pipeline, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dcdb81b-ab34-4e8e-bf47-ccd0ab1e1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(PATH+'embeddings', 'wb') as fp:\n",
    "#     pickle.dump(fcnn.embeddings, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cb84f-11eb-4caa-896e-2f6bff2fbdb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### optuna fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02e79830-e01a-4d82-aa41-cd8a00732250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_train, df_train_eval = train_test_split(df_train, test_size=EVAL_SIZE, random_state=SEED,\n",
    "#                                                  shuffle=True, stratify=df_train[target])\n",
    "\n",
    "# dataset_train = Base_Dataset(df_train_train)\n",
    "# dataset_eval = Base_Dataset(df_train_eval, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eab76062-58d5-474f-8824-81e2a838df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATIONS = {'ReLU': nn.ReLU,\n",
    "               'SELU': nn.SELU,\n",
    "               'GELU': nn.GELU,\n",
    "               'RReLU': nn.RReLU,\n",
    "               'SiLU': nn.SiLU,\n",
    "               'LeakyReLU': nn.LeakyReLU,\n",
    "               'IDENTITY': nn.Identity,\n",
    "              }\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # model's params\n",
    "    INPUT_DIM = sum([df_train_train[cat_col].nunique()//2+1\n",
    "                 for cat_col in cat_cols]) + 3\n",
    "    LAYERS_NUM = trial.suggest_int('LAYERS_NUM', 2, 4, step=1)\n",
    "    LAYERS_DIM = trial.suggest_int('LAYERS_DIM', 256, 512, step=64) #64\n",
    "    ACTIVATION_OPTIONS = trial.suggest_categorical('ACTIVATION', ['ReLU', 'SELU', 'GELU', 'RReLU'])\n",
    "    ACTIVATION = ACTIVATIONS[ACTIVATION_OPTIONS]\n",
    "    EMB_SZS = [[df_train_train[cat_col].nunique(),\n",
    "                df_train_train[cat_col].nunique()//2+1]\n",
    "               for cat_col in cat_cols]\n",
    "    DROPOUT = trial.suggest_float('DROPOUT', 0, 0.5)\n",
    "    \n",
    "    # learning params\n",
    "    NUM_EPOCHS = trial.suggest_int('NUM_EPOCHS', 8, 32, step=4)\n",
    "    BATCH_SIZE = trial.suggest_int('BATCH_SIZE', 1024, 4096, step=1024)\n",
    "    LR = trial.suggest_float('LR', 1e-5, 1e-3, log=True)\n",
    "    WEIGHT_DECAY = trial.suggest_float('WEIGHT_DECAY', 1e-8, 1e-4, log=True)\n",
    "    \n",
    "    fcnn = FCNNet(input_dim=INPUT_DIM,\n",
    "                  layers_num=LAYERS_NUM,\n",
    "                  layers_dim=LAYERS_DIM,\n",
    "                  activation=ACTIVATION,\n",
    "                  emb_szs=EMB_SZS,\n",
    "                  dropout=DROPOUT\n",
    "                  )\n",
    "    # altmetric_train = MetricCollection([\n",
    "    #                                     AUROC(task='binary'),\n",
    "    #                                     Recall(task='binary'),\n",
    "    #                                     Precision(task='binary'),\n",
    "    #                                     F1Score(task='binary'),\n",
    "    #                                     Accuracy(task='binary'),\n",
    "    #                                     MatthewsCorrCoef(task='binary')\n",
    "    #                                    ])\n",
    "    \n",
    "    altmetric_eval = MetricCollection([\n",
    "                                       # AUROC(task='binary'),\n",
    "                                       # Recall(task='binary'),\n",
    "                                       # Precision(task='binary'),\n",
    "                                       # F1Score(task='binary'),\n",
    "                                       # Accuracy(task='binary'),\n",
    "                                       MatthewsCorrCoef(task='binary')\n",
    "                                      ])\n",
    "    \n",
    "    fcnn.to(DEVICE)\n",
    "    # altmetric_train.to(DEVICE)\n",
    "    altmetric_eval.to(DEVICE)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                              batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                              num_workers=8, drop_last=False)\n",
    "    evalloader = torch.utils.data.DataLoader(dataset_eval,\n",
    "                                             batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                             num_workers=8, drop_last=False)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(fcnn.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        train_loss = train_epoch_fcnn(fcnn, optimizer, loss_fn, trainloader)\n",
    "        \n",
    "        if ((epoch)%4==0):\n",
    "            eval_loss = evaluate_fcnn(fcnn, loss_fn, evalloader, altmetric=altmetric_eval)\n",
    "            intermid_value = altmetric_eval['BinaryMatthewsCorrCoef'].compute().item()\n",
    "            # trial.report(eval_loss, epoch)\n",
    "            trial.report(intermid_value, epoch)\n",
    "            altmetric_eval.reset()\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "    eval_loss = evaluate_fcnn(fcnn, loss_fn, evalloader, altmetric=altmetric_eval)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # return eval_loss\n",
    "    return altmetric_eval['BinaryMatthewsCorrCoef'].compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "615e8b8a-ed92-4050-b9bb-503ddc3ebdb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "# # storage = optuna.storages.InMemoryStorage()\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=sampler,\n",
    "#                             study_name='fcnn-study_matthews1', storage='sqlite:///fcnn-study_matthews.db', load_if_exists=True,\n",
    "#                             pruner=optuna.pruners.MedianPruner(n_startup_trials=16,\n",
    "#                                                                n_warmup_steps=8)\n",
    "#                            )\n",
    "# study.optimize(objective, n_trials=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a36d5b-517e-414e-9527-6d1860a59d67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecd3e5-b00c-4943-8281-43c4cd1936e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00c2923d-6b89-458f-af97-38a2d16b04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim = None,\n",
    "                 layers_num: int = 3,\n",
    "                 layers_dim: int = 64,\n",
    "                 activation = nn.ReLU,\n",
    "                 emb_szs = None,\n",
    "                 emb_weights = False,\n",
    "                 dropout: float = 0.,\n",
    "                 swapnoise_ratio = 0.15,\n",
    "                 return_obfuscation_mask = False,\n",
    "                ):\n",
    "        super(DAE, self).__init__()\n",
    "    \n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(in_sz, out_sz) for in_sz, out_sz in emb_szs])\n",
    "        if (emb_weights==True):\n",
    "            for i, emb in enumerate(embeddings):\n",
    "                emb.weight = emb_weights[i].weight\n",
    "            for p in embeddings.parameters():\n",
    "                p.requires_grad_(False)\n",
    "        self.swapnoise_ratio = swapnoise_ratio\n",
    "        self.return_obfuscation_mask = return_obfuscation_mask\n",
    "    \n",
    "        dae_layers = []\n",
    "        dae_layers.append(nn.Linear(input_dim, layers_dim))\n",
    "        dae_layers.append(activation())\n",
    "        dae_layers.append(nn.Dropout(p=dropout))\n",
    "        for i in range(layers_num):\n",
    "            dae_layers.append(nn.Linear(layers_dim, layers_dim))\n",
    "            dae_layers.append(activation())\n",
    "            dae_layers.append(nn.Dropout(p=dropout))\n",
    "        dae_layers.append(nn.Linear(layers_dim, input_dim))\n",
    "    \n",
    "        self.dae = nn.Sequential(*dae_layers)\n",
    "\n",
    "        ###########################\n",
    "        # distinctions for make_denoise\n",
    "        self.dae_layers = []\n",
    "        self.dae_layers.append([list(self.dae.children())[0]])\n",
    "        for i in range(1, len(self.dae)-3, 3):\n",
    "            self.dae_layers.append(list(self.dae.children())[:i+2+1])\n",
    "        self.dae_layers = [nn.Sequential(*i) for i in self.dae_layers]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_orig = x.clone().detach()\n",
    "            x_orig, _ = self.add_swapnoise(x_orig, ratio=self.swapnoise_ratio)\n",
    "            x_orig = self.make_embedded(x_orig).detach()\n",
    "\n",
    "        x = self.make_embedded(x)\n",
    "        x = self.dae(x)\n",
    "        # if (self.return_obfuscation_mask==True): return x, x_orig, _\n",
    "        # return x, x_orig, None\n",
    "        return x, x_orig\n",
    "\n",
    "    def make_denoise(self, x: torch.Tensor):\n",
    "        if len(x.shape) == 1: x = x.unsqueeze(0)\n",
    "        x = self.make_embedded(x)\n",
    "        return torch.cat([i(x) for i in self.dae_layers][1:], dim=-1) #dropping first output\n",
    "\n",
    "    def make_embedded(self, x: torch.Tensor):\n",
    "        x_num = x[:, :3]\n",
    "        x_cat = x[:, 3:].long()\n",
    "        x_cat = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.embeddings)]\n",
    "        x_cat = torch.cat(x_cat, dim=-1)\n",
    "        x = torch.cat([x_num, x_cat], dim=-1).float()\n",
    "        return x\n",
    "\n",
    "    # https://www.kaggle.com/code/ryanzhang/pytorch-dae-starter-code\n",
    "    # it's a row permutation noising by the way\n",
    "    def add_swapnoise(self, x, ratio=0.15):\n",
    "        obfuscation_mask = torch.bernoulli(ratio * torch.ones(x.shape)).to(DEVICE)\n",
    "        \n",
    "        # for row-column-wise noising\n",
    "        # obfuscated_x = torch.where(obfuscation_mask == 1, x[torch.randperm(x.shape[0])][:, torch.randperm(x.shape[1])], x)\n",
    "        # for column-wise noising\n",
    "        # obfuscated_x = torch.where(obfuscation_mask == 1, x[torch.randperm([:, torch.randperm(x.shape[1])], x)\n",
    "        # for row-wise noising \n",
    "        obfuscated_x = torch.where(obfuscation_mask == 1, x[torch.randperm(x.shape[0])], x)\n",
    "        \n",
    "        return obfuscated_x, obfuscation_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c09a9b49-bb18-43e5-b1fa-04bcbade56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dae_model,\n",
    "                 dae_out_dim: int = 1024*4,\n",
    "                 # layers_num: int = 2,\n",
    "                 # layers_dim: int = 32,\n",
    "                 activation = nn.ReLU,\n",
    "                 dropout: float = 0.,\n",
    "                 feature_dim = 256,\n",
    "                ):\n",
    "        super(FCHead, self).__init__()\n",
    "\n",
    "        self.dae_model = dae_model\n",
    "        for p in self.dae_model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        \n",
    "        fc_layers = []\n",
    "        fc_layers.append(nn.Linear(dae_out_dim, dae_out_dim//4))\n",
    "        fc_layers.append(nn.LazyBatchNorm1d())\n",
    "        fc_layers.append(activation())\n",
    "        fc_layers.append(nn.Dropout(p=dropout))\n",
    "        fc_layers.append(nn.Linear(dae_out_dim//4, dae_out_dim//16))\n",
    "        fc_layers.append(nn.LazyBatchNorm1d())\n",
    "        fc_layers.append(activation())\n",
    "        fc_layers.append(nn.Dropout(p=dropout))\n",
    "        fc_layers.append(nn.Linear(dae_out_dim//16, 1))\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.fc_layers.children())[0:5])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dae_model.make_denoise(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    def feature_maker(self, x: torch.Tensor):\n",
    "        x = self.dae_model.make_denoise(x)\n",
    "        x = self.feature_extractor(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bc85cb2-c573-4045-aee0-345ac8e92a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_dae(model, optimizer, loss_fn, train_dataloader):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for X, y in tqdm.tqdm(train_dataloader):\n",
    "        X = X.to(DEVICE)\n",
    "\n",
    "        # preds, orig, mask = model(X)\n",
    "        preds, orig = model(X)\n",
    "        optimizer.zero_grad()\n",
    "        # loss = loss_fn(preds, orig, mask)\n",
    "        loss = loss_fn(preds, orig)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "    \n",
    "def evaluate_dae(model, loss_fn, test_dataloader):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for X, y in test_dataloader:\n",
    "        X = X.to(DEVICE)\n",
    "        \n",
    "        # preds, orig, mask = model(X)\n",
    "        preds, orig = model(X)\n",
    "        # loss = loss_fn(preds, orig, mask)\n",
    "        loss = loss_fn(preds, orig)\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56726075-cc37-45f2-8991-a00717805e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE_Weighted(nn.Module):\n",
    "    # when no mask and emphasis = 1 - equvivalent to MSE\n",
    "    def __init__(self, emphasis=1):\n",
    "        self.emphasis = emphasis\n",
    "        # emphasis between 0 and 1\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, actual, mask=None):\n",
    "        if (mask is None): mask = torch.ones(pred.shape).to(DEVICE)\n",
    "        loss_weights = mask * self.emphasis + (1 - mask) * (1 - self.emphasis)\n",
    "        unweighted_loss = nn.functional.mse_loss(pred, actual, reduction='none')\n",
    "        weighted_loss = loss_weights * unweighted_loss\n",
    "        return weighted_loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d6701d-776c-4ce6-9ee5-57a756fc2fbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68203616-c6dd-483b-bfc4-e2c6e04a65e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e1ba395-8fff-47e3-8ef7-59ef73ef9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(PATH+'pipeline', 'rb') as fp:\n",
    "#     pipeline = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "410f8a6c-cf37-491e-a069-b1427a12e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(PATH+'embeddings', 'rb') as fp:\n",
    "#     embeddings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "051046ec-faca-40b7-a660-ef940e6943cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined = pd.concat([df_train, df_test], axis=0)\n",
    "# df_combined['class'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98052b05-838d-4f5c-ab49-fd0a23aa0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined_train, df_combined_eval = train_test_split(df_combined, test_size=EVAL_SIZE, random_state=SEED, shuffle=True)\n",
    "\n",
    "# # is_eval to not to override the pipeline and to not return the y\n",
    "# dataset_dae_train = Base_Dataset(df_combined_train, is_eval=True)\n",
    "# dataset_dae_eval = Base_Dataset(df_combined_eval, is_eval=True)\n",
    "\n",
    "# display(len(dataset_train))\n",
    "# display(len(dataset_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f72822a8-b412-478b-a050-b3d63e985ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# INPUT_DIM = sum([df_combined_train[cat_col].nunique()//2+1\n",
    "#                  for cat_col in cat_cols]) + 3\n",
    "# LAYERS_NUM = 3\n",
    "# LAYERS_DIM = 2048\n",
    "# ACTIVATION = nn.ReLU\n",
    "# EMB_SZS = [[df_combined_train[cat_col].nunique(),\n",
    "#             df_combined_train[cat_col].nunique()//2+1]\n",
    "#            for cat_col in cat_cols]\n",
    "# EMB_WEIGHTS = embeddings\n",
    "# DROPOUT = 0.\n",
    "# SWAPNOISE_RATIO = 0.15\n",
    "\n",
    "# NUM_EPOCHS = 16\n",
    "# BATCH_SIZE = 1024*4 #96\n",
    "# LR = 1e-4 #2e-4 \n",
    "# WEIGHT_DECAY = 1e-6 #6e-5\n",
    "\n",
    "# dae = DAE(input_dim = INPUT_DIM,\n",
    "#           layers_num = LAYERS_NUM,\n",
    "#           layers_dim = LAYERS_DIM,\n",
    "#           activation = ACTIVATION,\n",
    "#           emb_szs = EMB_SZS,\n",
    "#           emb_weights = EMB_WEIGHTS,\n",
    "#           dropout = DROPOUT,\n",
    "#           swapnoise_ratio = SWAPNOISE_RATIO,\n",
    "#           return_obfuscation_mask=False)\n",
    "# dae.to(DEVICE)\n",
    "\n",
    "# trainloader_dae = torch.utils.data.DataLoader(dataset_dae_train,\n",
    "#                                           batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                                           num_workers=8, drop_last=False)\n",
    "# evalloader_dae = torch.utils.data.DataLoader(dataset_dae_eval,\n",
    "#                                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                                          num_workers=8, drop_last=False)\n",
    "\n",
    "# # loss_fn = MSE_Weighted(emphasis=4/5)\n",
    "# # loss_fn = MSE_Weighted(emphasis=1)\n",
    "# loss_fn = nn.MSELoss()\n",
    "# optimizer = torch.optim.AdamW(dae.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4493afc-91ca-4716-96df-857101d8d2f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# for epoch in range(1, NUM_EPOCHS+1):\n",
    "#     train_loss = train_epoch_dae(dae, optimizer, loss_fn, trainloader_dae)\n",
    "#     eval_loss = evaluate_dae(dae, loss_fn, evalloader_dae)\n",
    "#     print((f\"Epoch: {epoch}, Train loss: {train_loss:.5f}, Val loss: {eval_loss:.5f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c597e2c2-999f-4b3c-ae00-a9b779a77b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_train, df_train_eval = train_test_split(df_train, test_size=EVAL_SIZE, random_state=SEED,\n",
    "#                                                  shuffle=True, stratify=df_train[target])\n",
    "\n",
    "# dataset_train = Base_Dataset(df_train_train, is_eval=True)\n",
    "# dataset_eval = Base_Dataset(df_train_eval, is_eval=True)\n",
    "\n",
    "# display(len(dataset_train))\n",
    "# display(len(dataset_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7aec4f5-8872-4879-94aa-052c515330a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAE_MODEL = dae\n",
    "# DAE_OUT_DUM = 2048*3\n",
    "# FEATURE_DIM = 256\n",
    "# ACTIVATION = nn.ReLU\n",
    "# DROPOUT = 0.\n",
    "\n",
    "# NUM_EPOCHS = 8\n",
    "# BATCH_SIZE = 1024*4 #96\n",
    "# LR = 1e-3 #2e-4 \n",
    "# WEIGHT_DECAY = 1e-6 #6e-5\n",
    "\n",
    "\n",
    "# fchead = FCHead(dae_model = DAE_MODEL,\n",
    "#                 dae_out_dim = DAE_OUT_DUM,\n",
    "#                 feature_dim = FEATURE_DIM,\n",
    "#                 activation = ACTIVATION,\n",
    "#                 dropout = DROPOUT)\n",
    "\n",
    "# altmetric_train = MetricCollection([AUROC(task='binary'),\n",
    "#                                     Recall(task='binary'),\n",
    "#                                     Precision(task='binary'),\n",
    "#                                     F1Score(task='binary'),\n",
    "#                                     Accuracy(task='binary'),\n",
    "#                                     MatthewsCorrCoef(task='binary')\n",
    "#                                    ])\n",
    "# altmetric_eval = MetricCollection([AUROC(task='binary'),\n",
    "#                                    Recall(task='binary'),\n",
    "#                                    Precision(task='binary'),\n",
    "#                                    F1Score(task='binary'),\n",
    "#                                    Accuracy(task='binary'),\n",
    "#                                    MatthewsCorrCoef(task='binary')\n",
    "#                                   ])\n",
    "\n",
    "# fchead.to(DEVICE)\n",
    "# altmetric_train.to(DEVICE)\n",
    "# altmetric_eval.to(DEVICE)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(dataset_train,\n",
    "#                                           batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                                           num_workers=8, drop_last=False)\n",
    "# evalloader = torch.utils.data.DataLoader(dataset_eval,\n",
    "#                                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                                          num_workers=8, drop_last=False)\n",
    "\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.AdamW(fchead.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78b1fb2d-eb40-4153-b344-b75f4205629a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# PRINT_EVERY = 1\n",
    "\n",
    "# for epoch in range(1, NUM_EPOCHS+1):\n",
    "#     train_loss = train_epoch_fcnn(fchead, optimizer, loss_fn, trainloader, altmetric=altmetric_train)\n",
    "#     eval_loss = evaluate_fcnn(fchead, loss_fn, evalloader, altmetric=altmetric_eval)\n",
    "#     print((f\"Epoch: {epoch}, Train loss: {train_loss:.5f}, Val loss: {eval_loss:.5f}\"))\n",
    "\n",
    "#     if ((epoch)%PRINT_EVERY==0):\n",
    "#         print('Train')\n",
    "#         for j in [(i, round(altmetric_train[i].compute().item(), 5))\n",
    "#                   for i in altmetric_train.keys()]: print(j)\n",
    "#         print()\n",
    "#         print('Test')\n",
    "#         for j in [(i, round(altmetric_eval[i].compute().item(), 5))\n",
    "#                   for i in altmetric_eval.keys()]: print(j)\n",
    "#         print()\n",
    "\n",
    "#     altmetric_train.reset()\n",
    "#     altmetric_eval.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c515f-88ec-4cba-8046-36249451648e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### optuna fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7dfa8da-5205-4ef9-b966-9160febb14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(PATH+'pipeline', 'rb') as fp:\n",
    "#     pipeline = pickle.load(fp)\n",
    "# with open(PATH+'embeddings', 'rb') as fp:\n",
    "#     embeddings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec9c789e-e3d9-4da4-a5b6-a040ef245368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined = pd.concat([df_train, df_test], axis=0)\n",
    "# df_combined['class'].fillna(-1, inplace=True)\n",
    "\n",
    "# df_combined_train, df_combined_eval = train_test_split(df_combined, test_size=EVAL_SIZE, random_state=SEED, shuffle=True)\n",
    "# dataset_dae_train = Base_Dataset(df_combined, is_eval=True) #df_combined_train\n",
    "# dataset_dae_eval = Base_Dataset(df_combined_eval, is_eval=True)\n",
    "\n",
    "# df_train_train, df_train_eval = train_test_split(df_train, test_size=EVAL_SIZE, random_state=SEED,\n",
    "#                                                  shuffle=True, stratify=df_train[target])\n",
    "# dataset_train = Base_Dataset(df_train_train, is_eval=True)\n",
    "# dataset_eval = Base_Dataset(df_train_eval, is_eval=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9d21def-09ac-4949-a8a8-c13481fb81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVATIONS = {'ReLU': nn.ReLU,\n",
    "#                'SELU': nn.SELU,\n",
    "#                'GELU': nn.GELU,\n",
    "#                'RReLU': nn.RReLU,\n",
    "#                'SiLU': nn.SiLU,\n",
    "#                'LeakyReLU': nn.LeakyReLU,\n",
    "#                'IDENTITY': nn.Identity,\n",
    "#               }\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     # model's params\n",
    "#     INPUT_DIM = sum([df_train_train[cat_col].nunique()//2+1\n",
    "#                  for cat_col in cat_cols]) + 3\n",
    "#     LAYERS_NUM_DAE = trial.suggest_int('LAYERS_NUM_DAE', 2, 4, step=1)\n",
    "#     LAYERS_DIM_DAE = trial.suggest_int('LAYERS_DIM_DAE', 512, 2048, step=512) #64\n",
    "#     ACTIVATION_OPTIONS = trial.suggest_categorical('ACTIVATION_DAE', ['ReLU', 'SELU', 'GELU', 'RReLU'])\n",
    "#     ACTIVATION_DAE = ACTIVATIONS[ACTIVATION_OPTIONS]\n",
    "#     EMB_SZS = [[df_combined_train[cat_col].nunique(),\n",
    "#                 df_combined_train[cat_col].nunique()//2+1]\n",
    "#                for cat_col in cat_cols]\n",
    "#     EMB_WEIGHTS = embeddings\n",
    "#     DROPOUT_DAE = trial.suggest_float('DROPOUT_DAE', 0, 0.3)\n",
    "#     SWAPNOISE_RATIO = trial.suggest_float('SWAPNOISE_RATIO', 0.1, 0.3)\n",
    "    \n",
    "#     # learning params\n",
    "#     NUM_EPOCHS_DAE = trial.suggest_int('NUM_EPOCHS_DAE', 16, 64, step=8)\n",
    "#     BATCH_SIZE_DAE = trial.suggest_int('BATCH_SIZE_DAE', 1024, 4096, step=1024)\n",
    "#     LR_DAE = trial.suggest_float('LR_DAE', 1e-5, 1e-3, log=True)\n",
    "#     WEIGHT_DECAY_DAE = trial.suggest_float('WEIGHT_DECAY_DAE', 1e-8, 1e-4, log=True)\n",
    "    \n",
    "#     dae = DAE(input_dim = INPUT_DIM,\n",
    "#               layers_num = LAYERS_NUM_DAE,\n",
    "#               layers_dim = LAYERS_DIM_DAE,\n",
    "#               activation = ACTIVATION_DAE,\n",
    "#               emb_szs = EMB_SZS,\n",
    "#               emb_weights = EMB_WEIGHTS,\n",
    "#               dropout = DROPOUT_DAE,\n",
    "#               swapnoise_ratio = SWAPNOISE_RATIO,\n",
    "#               return_obfuscation_mask=False)\n",
    "#     dae.to(DEVICE)\n",
    "    \n",
    "#     trainloader_dae = torch.utils.data.DataLoader(dataset_dae_train,\n",
    "#                                                   batch_size=BATCH_SIZE_DAE, shuffle=True,\n",
    "#                                                   num_workers=4, drop_last=False)\n",
    "#     evalloader_dae = torch.utils.data.DataLoader(dataset_dae_eval,\n",
    "#                                                  batch_size=BATCH_SIZE_DAE, shuffle=True,\n",
    "#                                                  num_workers=4, drop_last=False)\n",
    "    \n",
    "#     loss_fn = nn.MSELoss()\n",
    "#     optimizer = torch.optim.AdamW(dae.parameters(), lr=LR_DAE, weight_decay=WEIGHT_DECAY_DAE)\n",
    "\n",
    "#     for epoch in range(1, NUM_EPOCHS_DAE+1):\n",
    "#         train_loss = train_epoch_dae(dae, optimizer, loss_fn, trainloader_dae)\n",
    "        \n",
    "#         if ((epoch)%4==0):\n",
    "#             eval_loss = evaluate_dae(dae, loss_fn, evalloader_dae)\n",
    "#             trial.report(eval_loss, epoch)\n",
    "#             # if trial.should_prune():\n",
    "#             #     raise optuna.TrialPruned()\n",
    "\n",
    "#     DAE_MODEL = dae\n",
    "#     DAE_OUT_DIM = LAYERS_NUM_DAE*LAYERS_DIM_DAE\n",
    "#     FEATURE_DIM = 1024 # not using\n",
    "#     ACTIVATION = nn.ReLU\n",
    "#     DROPOUT = 0.\n",
    "\n",
    "#     NUM_EPOCHS = trial.suggest_int('NUM_EPOCHS', 8, 16, step=4)\n",
    "#     BATCH_SIZE = trial.suggest_int('BATCH_SIZE', 1024, 4096, step=1024)\n",
    "#     LR = trial.suggest_float('LR', 1e-4, 1e-2, log=True)\n",
    "#     WEIGHT_DECAY = trial.suggest_float('WEIGHT_DECAY', 1e-6, 1e-3, log=True)\n",
    "\n",
    "#     fchead = FCHead(dae_model = DAE_MODEL,\n",
    "#                     dae_out_dim = DAE_OUT_DIM,\n",
    "#                     feature_dim = FEATURE_DIM,\n",
    "#                     activation = ACTIVATION,\n",
    "#                     dropout = DROPOUT)\n",
    "    \n",
    "#     altmetric_eval = MetricCollection([\n",
    "#                                        # AUROC(task='binary'),\n",
    "#                                        # Recall(task='binary'),\n",
    "#                                        # Precision(task='binary'),\n",
    "#                                        # F1Score(task='binary'),\n",
    "#                                        # Accuracy(task='binary'),\n",
    "#                                        MatthewsCorrCoef(task='binary')\n",
    "#                                       ])\n",
    "    \n",
    "#     fchead.to(DEVICE)\n",
    "#     altmetric_eval.to(DEVICE)\n",
    "\n",
    "#     trainloader = torch.utils.data.DataLoader(dataset_train,\n",
    "#                                               batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                                               num_workers=4, drop_last=False)\n",
    "#     evalloader = torch.utils.data.DataLoader(dataset_eval,\n",
    "#                                              batch_size=BATCH_SIZE, shuffle=True,\n",
    "#                                              num_workers=4, drop_last=False)\n",
    "\n",
    "#     loss_fn = nn.BCEWithLogitsLoss()\n",
    "#     optimizer = torch.optim.AdamW(fchead.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "#     for epoch in range(1, NUM_EPOCHS+1):\n",
    "#         train_loss = train_epoch_fcnn(fchead, optimizer, loss_fn, trainloader)\n",
    "\n",
    "#     eval_loss = evaluate_fcnn(fchead, loss_fn, evalloader, altmetric=altmetric_eval)\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "#     # return eval_loss\n",
    "#     return altmetric_eval['BinaryMatthewsCorrCoef'].compute().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49ea2d9c-9d86-4df3-bbd4-a816458257e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "# # storage = optuna.storages.InMemoryStorage()\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=sampler,\n",
    "#                             study_name='fchead-study_matthews1', storage='sqlite:///fcnn-study_matthews.db', load_if_exists=True,\n",
    "#                             pruner=optuna.pruners.MedianPruner(n_startup_trials=16,\n",
    "#                                                                n_warmup_steps=8)\n",
    "#                            )\n",
    "# study.optimize(objective, n_trials=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc5b14-8fb5-481e-a16e-5819aa5191b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TabNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c251a2-42ee-46a6-9d2b-e0f9c2194e7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61e3843e-ce66-46b9-8b10-87ba70859778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "with open(PATH+'pipeline', 'rb') as fp:\n",
    "    pipeline = pickle.load(fp)\n",
    "with open(PATH+'embeddings', 'rb') as fp:\n",
    "    embeddings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50e21797-900c-4549-83a7-5a957e303718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.metrics import Metric as TabMetric\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ee2d491e-389c-47a1-b8cb-7f030d0de02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3116941"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "623389"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2077956"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_train, df_train_eval = train_test_split(df_train, test_size=EVAL_SIZE, random_state=SEED,\n",
    "                                                 shuffle=True, stratify=df_train[target])\n",
    "\n",
    "dataset_train = Base_Dataset(df_train) #df_train_train\n",
    "dataset_eval = Base_Dataset(df_train_eval, is_eval=True)\n",
    "\n",
    "display(len(dataset_train))\n",
    "display(len(dataset_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b4724c63-e9c6-474d-b77a-b3939f0c0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4096\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset_train,\n",
    "                                          batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                          num_workers=8, drop_last=False)\n",
    "evalloader = torch.utils.data.DataLoader(dataset_eval,\n",
    "                                         batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                         num_workers=8, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0aaa271b-9c52-4e07-a763-a817c8870a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedded(embeddings, loader):\n",
    "    embeddings.eval()\n",
    "    X_array, y_array = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            X_num = X[:, :3]\n",
    "            X_cat = X[:, 3:].long()\n",
    "            X_cat = [emb_layer(X_cat[:, i]) for i, emb_layer in enumerate(embeddings)]\n",
    "            X_cat = torch.cat(X_cat, dim=-1)\n",
    "            # print(X_num.shape, X_cat.shape)\n",
    "            X = torch.cat([X_num, X_cat], dim=-1).float()\n",
    "\n",
    "            X, y = X.detach().cpu().numpy(), y.detach().cpu().numpy()\n",
    "            X_array.append(X)\n",
    "            y_array.append(y)\n",
    "            \n",
    "    X_array = np.concatenate(X_array, axis=0)\n",
    "    y_array = np.concatenate(y_array, axis=0)\n",
    "    return X_array, y_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ffaf3b87-3f78-48cf-b60a-80760c60cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = make_embedded(embeddings, trainloader)\n",
    "X_eval, y_eval = make_embedded(embeddings, evalloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fc17b187-bf27-4a75-988e-df3d3761c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matthews(TabMetric):\n",
    "    def __init__(self):\n",
    "        self._name = \"matthews\"\n",
    "        self._maximize = True\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        # return matthews_corrcoef((y_true > 0.5).astype('int'), y_score)\n",
    "        return multiclass_matthews_corrcoef(torch.Tensor(y_score),\n",
    "                                            torch.Tensor(y_true),\n",
    "                                            num_classes=2)\\\n",
    "               .item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7999a-6927-4cb2-a4e7-bd61ed0e1c4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f89863d-d366-4cc7-aa33-eb8700f42a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for direct training\n",
    "# X_train, y_train = df_train_train[num_cols+cat_cols], df_train_train[target].values\n",
    "# X_train = pipeline.transform(X_train)\n",
    "# X_eval, y_eval = df_train_eval[num_cols+cat_cols], df_train_eval[target].values\n",
    "# X_eval = pipeline.transform(X_eval)\n",
    "\n",
    "# cat_idxs = list(range(3, 18))\n",
    "# cat_dims = [df_train_train[cat_col].nunique() for cat_col in cat_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "50da4875-dbcb-4961-9f67-174502d7e858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.05603 | val_0_unsup_loss_numpy: 0.9530799984931946|  0:02:31s\n",
      "epoch 1  | loss: 0.83271 | val_0_unsup_loss_numpy: 0.9931700229644775|  0:04:57s\n",
      "epoch 2  | loss: 0.76134 | val_0_unsup_loss_numpy: 0.8679800033569336|  0:07:24s\n",
      "epoch 3  | loss: 0.71029 | val_0_unsup_loss_numpy: 0.7047399878501892|  0:10:00s\n",
      "epoch 4  | loss: 0.66579 | val_0_unsup_loss_numpy: 0.6416100263595581|  0:12:34s\n",
      "epoch 5  | loss: 0.62994 | val_0_unsup_loss_numpy: 0.546209990978241|  0:15:05s\n",
      "epoch 6  | loss: 0.60968 | val_0_unsup_loss_numpy: 0.5282899737358093|  0:17:34s\n",
      "epoch 7  | loss: 0.58982 | val_0_unsup_loss_numpy: 0.5473200082778931|  0:20:02s\n",
      "epoch 8  | loss: 0.57451 | val_0_unsup_loss_numpy: 0.5228999853134155|  0:22:32s\n",
      "epoch 9  | loss: 0.55794 | val_0_unsup_loss_numpy: 0.5154399871826172|  0:25:00s\n",
      "epoch 10 | loss: 0.54012 | val_0_unsup_loss_numpy: 0.5223900079727173|  0:27:30s\n",
      "epoch 11 | loss: 0.52764 | val_0_unsup_loss_numpy: 0.5306500196456909|  0:29:58s\n",
      "epoch 12 | loss: 0.52567 | val_0_unsup_loss_numpy: 0.52947998046875|  0:32:26s\n",
      "epoch 13 | loss: 0.51413 | val_0_unsup_loss_numpy: 0.5122100114822388|  0:34:48s\n",
      "epoch 14 | loss: 0.52499 | val_0_unsup_loss_numpy: 0.5435900092124939|  0:37:09s\n",
      "epoch 15 | loss: 0.52057 | val_0_unsup_loss_numpy: 0.5319100022315979|  0:39:32s\n",
      "epoch 16 | loss: 0.5014  | val_0_unsup_loss_numpy: 0.5064799785614014|  0:41:59s\n",
      "epoch 17 | loss: 0.49069 | val_0_unsup_loss_numpy: 0.5155799984931946|  0:44:24s\n",
      "epoch 18 | loss: 0.49024 | val_0_unsup_loss_numpy: 0.4936800003051758|  0:47:01s\n",
      "epoch 19 | loss: 0.48187 | val_0_unsup_loss_numpy: 0.4932900071144104|  0:49:32s\n",
      "epoch 20 | loss: 0.47132 | val_0_unsup_loss_numpy: 0.5239099860191345|  0:51:55s\n",
      "epoch 21 | loss: 0.47395 | val_0_unsup_loss_numpy: 0.513159990310669|  0:54:11s\n",
      "epoch 22 | loss: 0.46626 | val_0_unsup_loss_numpy: 0.4710099995136261|  0:56:27s\n",
      "epoch 23 | loss: 0.48545 | val_0_unsup_loss_numpy: 0.6146500110626221|  0:58:41s\n",
      "epoch 24 | loss: 0.47625 | val_0_unsup_loss_numpy: 0.5049899816513062|  1:00:59s\n",
      "epoch 25 | loss: 0.46814 | val_0_unsup_loss_numpy: 0.5127999782562256|  1:03:22s\n",
      "epoch 26 | loss: 0.46597 | val_0_unsup_loss_numpy: 0.5316799879074097|  1:05:44s\n",
      "epoch 27 | loss: 0.46162 | val_0_unsup_loss_numpy: 0.4656899869441986|  1:08:06s\n",
      "epoch 28 | loss: 0.47185 | val_0_unsup_loss_numpy: 0.492900013923645|  1:10:28s\n",
      "epoch 29 | loss: 0.45724 | val_0_unsup_loss_numpy: 0.5313799977302551|  1:12:50s\n",
      "epoch 30 | loss: 0.44874 | val_0_unsup_loss_numpy: 0.5212299823760986|  1:15:11s\n",
      "epoch 31 | loss: 0.44507 | val_0_unsup_loss_numpy: 0.5065699815750122|  1:17:35s\n",
      "epoch 32 | loss: 0.44632 | val_0_unsup_loss_numpy: 0.4827499985694885|  1:20:00s\n",
      "epoch 33 | loss: 0.44182 | val_0_unsup_loss_numpy: 0.4962500035762787|  1:22:27s\n",
      "epoch 34 | loss: 0.44583 | val_0_unsup_loss_numpy: 0.4734399914741516|  1:24:53s\n",
      "epoch 35 | loss: 0.44223 | val_0_unsup_loss_numpy: 0.49639999866485596|  1:27:18s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 27 and best_val_0_unsup_loss_numpy = 0.4656899869441986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "MASK_TYPE = 'entmax'\n",
    "LR = 0.04\n",
    "PRETRAIN_RATIO = 0.8\n",
    "N_D = 64\n",
    "N_A = N_D\n",
    "N_STEPS = 6\n",
    "GAMMA = 1.5\n",
    "N_INDEPENDENT = 5\n",
    "N_SHARED = 3\n",
    "\n",
    "MAX_EPOCHES = 64\n",
    "BATCH_SIZE = 1024*32\n",
    "VIRT_BATCH_SIZE = 256\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(n_d=N_D,\n",
    "                                      n_a=N_A,\n",
    "                                      n_steps=N_STEPS,\n",
    "                                      gamma=GAMMA,\n",
    "                                      n_independent=N_INDEPENDENT,\n",
    "                                      n_shared=N_SHARED,\n",
    "                                      optimizer_fn=torch.optim.Adam,\n",
    "                                      optimizer_params=dict(lr=LR),\n",
    "                                      mask_type=MASK_TYPE,\n",
    "                                      device_name=DEVICE,\n",
    "                                      seed=SEED)\n",
    "\n",
    "unsupervised_model.fit(X_train=X_train,\n",
    "                       eval_set=[X_eval],\n",
    "                       pretraining_ratio=PRETRAIN_RATIO,\n",
    "                       max_epochs=MAX_EPOCHES,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       virtual_batch_size=VIRT_BATCH_SIZE,\n",
    "                       num_workers=4,\n",
    "                       patience=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "393cf16d-c68f-4c33-9a46-da29ec296ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.53561 | val_0_matthews: 0.85514 |  0:01:56s\n",
      "epoch 1  | loss: 0.05024 | val_0_matthews: 0.96693 |  0:03:55s\n",
      "epoch 2  | loss: 0.04503 | val_0_matthews: 0.98182 |  0:05:53s\n",
      "epoch 3  | loss: 0.04313 | val_0_matthews: 0.98279 |  0:07:50s\n",
      "epoch 4  | loss: 0.0417  | val_0_matthews: 0.98349 |  0:09:48s\n",
      "epoch 5  | loss: 0.04087 | val_0_matthews: 0.98374 |  0:11:47s\n",
      "epoch 6  | loss: 0.04028 | val_0_matthews: 0.98429 |  0:13:45s\n",
      "epoch 7  | loss: 0.0398  | val_0_matthews: 0.98434 |  0:15:44s\n",
      "epoch 8  | loss: 0.03964 | val_0_matthews: 0.98457 |  0:17:43s\n",
      "epoch 9  | loss: 0.03921 | val_0_matthews: 0.98463 |  0:19:43s\n",
      "epoch 10 | loss: 0.03886 | val_0_matthews: 0.98475 |  0:21:41s\n",
      "epoch 11 | loss: 0.03839 | val_0_matthews: 0.9851  |  0:23:39s\n",
      "epoch 12 | loss: 0.03843 | val_0_matthews: 0.98508 |  0:25:37s\n",
      "epoch 13 | loss: 0.03813 | val_0_matthews: 0.98504 |  0:27:35s\n",
      "epoch 14 | loss: 0.03808 | val_0_matthews: 0.98544 |  0:29:32s\n",
      "epoch 15 | loss: 0.03752 | val_0_matthews: 0.98538 |  0:31:31s\n",
      "epoch 16 | loss: 0.0374  | val_0_matthews: 0.98553 |  0:33:29s\n",
      "epoch 17 | loss: 0.03716 | val_0_matthews: 0.98572 |  0:35:27s\n",
      "epoch 18 | loss: 0.03745 | val_0_matthews: 0.98547 |  0:37:20s\n",
      "epoch 19 | loss: 0.03784 | val_0_matthews: 0.98545 |  0:39:11s\n",
      "epoch 20 | loss: 0.03697 | val_0_matthews: 0.98574 |  0:41:07s\n",
      "epoch 21 | loss: 0.03692 | val_0_matthews: 0.98575 |  0:43:04s\n",
      "epoch 22 | loss: 0.0366  | val_0_matthews: 0.98578 |  0:45:01s\n",
      "epoch 23 | loss: 0.03661 | val_0_matthews: 0.98582 |  0:46:59s\n",
      "epoch 24 | loss: 0.03638 | val_0_matthews: 0.98569 |  0:48:55s\n",
      "epoch 25 | loss: 0.03644 | val_0_matthews: 0.98602 |  0:50:49s\n",
      "epoch 26 | loss: 0.03635 | val_0_matthews: 0.98599 |  0:52:45s\n",
      "epoch 27 | loss: 0.03606 | val_0_matthews: 0.98596 |  0:54:40s\n",
      "epoch 28 | loss: 0.0359  | val_0_matthews: 0.98609 |  0:56:32s\n",
      "epoch 29 | loss: 0.03569 | val_0_matthews: 0.98618 |  0:58:25s\n",
      "epoch 30 | loss: 0.03573 | val_0_matthews: 0.98617 |  1:00:19s\n",
      "epoch 31 | loss: 0.0361  | val_0_matthews: 0.98556 |  1:02:18s\n",
      "epoch 32 | loss: 0.03814 | val_0_matthews: 0.98453 |  1:04:14s\n",
      "epoch 33 | loss: 0.0389  | val_0_matthews: 0.98352 |  1:06:08s\n",
      "epoch 34 | loss: 0.03953 | val_0_matthews: 0.98497 |  1:08:01s\n",
      "epoch 35 | loss: 0.04228 | val_0_matthews: 0.98317 |  1:09:56s\n",
      "epoch 36 | loss: 0.03939 | val_0_matthews: 0.98467 |  1:11:53s\n",
      "epoch 37 | loss: 0.03755 | val_0_matthews: 0.98495 |  1:13:52s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 29 and best_val_0_matthews = 0.98618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetClassifier(n_d=N_D,\n",
    "                       n_a=N_A,\n",
    "                       n_steps=N_STEPS,\n",
    "                       gamma=GAMMA,\n",
    "                       n_independent=N_INDEPENDENT,\n",
    "                       n_shared=N_SHARED,\n",
    "                       optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=LR),\n",
    "                       mask_type=MASK_TYPE,\n",
    "                       device_name=DEVICE,\n",
    "                       seed=SEED)\n",
    "\n",
    "clf.fit(X_train, y_train.reshape(-1,),\n",
    "        eval_set=[(X_eval, y_eval.reshape(-1,))],\n",
    "        eval_metric=[matthews],\n",
    "        max_epochs=MAX_EPOCHES,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        virtual_batch_size=VIRT_BATCH_SIZE,\n",
    "        from_unsupervised=unsupervised_model,\n",
    "        num_workers=4,\n",
    "        patience=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea34b03-d3ab-4770-989d-1ff64899deae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### optuna fune-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0c46f4f-8163-4130-80d9-a5ed051d6d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = make_embedded(embeddings, trainloader)\n",
    "# X_eval, y_eval = make_embedded(embeddings, evalloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db2530f1-c287-483b-8b6d-e822969cfb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    MASK_TYPE = trial.suggest_categorical('MASK_TYPE', ['entmax', 'sparsemax'])\n",
    "    LR = trial.suggest_float('LR', 1e-3, 1e-1, log=True)\n",
    "    PRETRAIN_RATIO = trial.suggest_float('PRETRAIN_RATIO', 0.2, 0.8, log=True)\n",
    "    N_D = trial.suggest_int('N_D', 16, 64, step=16)\n",
    "    N_A = N_D\n",
    "    N_STEPS = trial.suggest_int('N_STEPS', 3, 6, step=1)\n",
    "    GAMMA = trial.suggest_float('GAMMA', 1.0, 2.0, log=True)\n",
    "    N_INDEPENDENT = trial.suggest_int('N_INDEPENDENT', 1, 5, step=1)\n",
    "    N_SHARED = trial.suggest_int('N_SHARED', 1, 5, step=1)\n",
    "    \n",
    "    MAX_EPOCHES = 32 #trial.suggest_int('MAX_EPOCHES_UNSUP', 8, 32, step=4)\n",
    "    BATCH_SIZE = trial.suggest_int('BATCH_SIZE', 1024*8, 1024*32, step=1024*4)\n",
    "    VIRT_BATCH_SIZE = trial.suggest_int('VIRT_BATCH_SIZE', 512, 2048, step=512)\n",
    "    \n",
    "    unsupervised_model = TabNetPretrainer(n_d=N_D,\n",
    "                                          n_a=N_A,\n",
    "                                          n_steps=N_STEPS,\n",
    "                                          gamma=GAMMA,\n",
    "                                          n_independent=N_INDEPENDENT,\n",
    "                                          n_shared=N_SHARED,\n",
    "                                          optimizer_fn=torch.optim.Adam,\n",
    "                                          optimizer_params=dict(lr=LR),\n",
    "                                          mask_type=MASK_TYPE,\n",
    "                                          device_name=DEVICE,\n",
    "                                          seed=SEED)\n",
    "    \n",
    "    unsupervised_model.fit(X_train=X_train,\n",
    "                           eval_set=[X_eval],\n",
    "                           pretraining_ratio=PRETRAIN_RATIO,\n",
    "                           max_epochs=MAX_EPOCHES,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           virtual_batch_size=VIRT_BATCH_SIZE,\n",
    "                           num_workers=8,\n",
    "                           patience=4)\n",
    "\n",
    "    clf = TabNetClassifier(n_d=N_D,\n",
    "                           n_a=N_A,\n",
    "                           n_steps=N_STEPS,\n",
    "                           gamma=GAMMA,\n",
    "                           n_independent=N_INDEPENDENT,\n",
    "                           n_shared=N_SHARED,\n",
    "                           optimizer_fn=torch.optim.Adam,\n",
    "                           optimizer_params=dict(lr=LR),\n",
    "                           mask_type=MASK_TYPE,\n",
    "                           device_name=DEVICE,\n",
    "                           seed=SEED)\n",
    "    \n",
    "    clf.fit(X_train, y_train.reshape(-1,),\n",
    "            eval_set=[(X_eval, y_eval.reshape(-1,))],\n",
    "            eval_metric=[matthews],\n",
    "            max_epochs=MAX_EPOCHES,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            virtual_batch_size=VIRT_BATCH_SIZE,\n",
    "            from_unsupervised=unsupervised_model,\n",
    "            num_workers=8,\n",
    "            patience=4)\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "    # gc.collect()\n",
    "    return clf.best_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff154db2-7024-41ff-9dbd-5a47ae3f2217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-26 23:02:34,256] A new study created in RDB with name: tabnet\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.22341 | val_0_unsup_loss_numpy: 0.9936100244522095|  0:00:47s\n",
      "epoch 1  | loss: 0.87779 | val_0_unsup_loss_numpy: 1.0253299474716187|  0:01:35s\n",
      "epoch 2  | loss: 0.79187 | val_0_unsup_loss_numpy: 0.9890300035476685|  0:02:22s\n",
      "epoch 3  | loss: 0.72855 | val_0_unsup_loss_numpy: 0.8093500137329102|  0:03:10s\n",
      "epoch 4  | loss: 0.67873 | val_0_unsup_loss_numpy: 0.6462500095367432|  0:03:58s\n",
      "epoch 5  | loss: 0.63331 | val_0_unsup_loss_numpy: 0.5638300180435181|  0:04:45s\n",
      "epoch 6  | loss: 0.59683 | val_0_unsup_loss_numpy: 0.51569002866745|  0:05:33s\n",
      "epoch 7  | loss: 0.56295 | val_0_unsup_loss_numpy: 0.46432000398635864|  0:06:21s\n",
      "epoch 8  | loss: 0.53297 | val_0_unsup_loss_numpy: 0.49028998613357544|  0:07:08s\n",
      "epoch 9  | loss: 0.50923 | val_0_unsup_loss_numpy: 0.4300999939441681|  0:07:56s\n",
      "epoch 10 | loss: 0.50127 | val_0_unsup_loss_numpy: 0.40501999855041504|  0:08:45s\n",
      "epoch 11 | loss: 0.49459 | val_0_unsup_loss_numpy: 0.44238999485969543|  0:09:35s\n",
      "epoch 12 | loss: 0.46908 | val_0_unsup_loss_numpy: 0.3969799876213074|  0:10:24s\n",
      "epoch 13 | loss: 0.48637 | val_0_unsup_loss_numpy: 0.43160000443458557|  0:11:13s\n",
      "epoch 14 | loss: 0.4437  | val_0_unsup_loss_numpy: 0.43143999576568604|  0:12:02s\n",
      "epoch 15 | loss: 0.42463 | val_0_unsup_loss_numpy: 0.4174500107765198|  0:12:52s\n",
      "epoch 16 | loss: 0.41406 | val_0_unsup_loss_numpy: 0.42065000534057617|  0:13:41s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 12 and best_val_0_unsup_loss_numpy = 0.3969799876213074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.23313 | val_0_matthews: 0.91206 |  0:00:42s\n",
      "epoch 1  | loss: 0.04581 | val_0_matthews: 0.98031 |  0:01:24s\n",
      "epoch 2  | loss: 0.04314 | val_0_matthews: 0.98251 |  0:02:06s\n",
      "epoch 3  | loss: 0.0419  | val_0_matthews: 0.98299 |  0:02:48s\n",
      "epoch 4  | loss: 0.04117 | val_0_matthews: 0.98307 |  0:03:31s\n",
      "epoch 5  | loss: 0.04107 | val_0_matthews: 0.98304 |  0:04:12s\n",
      "epoch 6  | loss: 0.04203 | val_0_matthews: 0.98315 |  0:04:54s\n",
      "epoch 7  | loss: 0.04    | val_0_matthews: 0.98382 |  0:05:37s\n",
      "epoch 8  | loss: 0.04086 | val_0_matthews: 0.98351 |  0:06:19s\n",
      "epoch 9  | loss: 0.03965 | val_0_matthews: 0.98384 |  0:07:01s\n",
      "epoch 10 | loss: 0.03914 | val_0_matthews: 0.98309 |  0:07:43s\n",
      "epoch 11 | loss: 0.03919 | val_0_matthews: 0.9841  |  0:08:24s\n",
      "epoch 12 | loss: 0.03847 | val_0_matthews: 0.9843  |  0:09:07s\n",
      "epoch 13 | loss: 0.03802 | val_0_matthews: 0.98397 |  0:09:52s\n",
      "epoch 14 | loss: 0.03784 | val_0_matthews: 0.98414 |  0:10:34s\n",
      "epoch 15 | loss: 0.03756 | val_0_matthews: 0.9842  |  0:11:16s\n",
      "epoch 16 | loss: 0.03742 | val_0_matthews: 0.98413 |  0:11:58s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 12 and best_val_0_matthews = 0.9843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-26 23:33:52,355] Trial 0 finished with value: 0.9843040704727173 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.04452019576176219, 'PRETRAIN_RATIO': 0.7629023443609105, 'N_D': 64, 'N_STEPS': 4, 'GAMMA': 1.5252465450906483, 'N_INDEPENDENT': 4, 'N_SHARED': 4, 'BATCH_SIZE': 28672, 'VIRT_BATCH_SIZE': 512}. Best is trial 0 with value: 0.9843040704727173.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.97859 | val_0_unsup_loss_numpy: 0.7658900022506714|  0:00:27s\n",
      "epoch 1  | loss: 0.7222  | val_0_unsup_loss_numpy: 0.5882099866867065|  0:00:54s\n",
      "epoch 2  | loss: 0.5933  | val_0_unsup_loss_numpy: 0.4421199858188629|  0:01:21s\n",
      "epoch 3  | loss: 0.49845 | val_0_unsup_loss_numpy: 0.36754000186920166|  0:01:48s\n",
      "epoch 4  | loss: 0.4319  | val_0_unsup_loss_numpy: 0.3180899918079376|  0:02:15s\n",
      "epoch 5  | loss: 0.38211 | val_0_unsup_loss_numpy: 0.2925400137901306|  0:02:42s\n",
      "epoch 6  | loss: 0.34839 | val_0_unsup_loss_numpy: 0.2752799987792969|  0:03:09s\n",
      "epoch 7  | loss: 0.31944 | val_0_unsup_loss_numpy: 0.265720009803772|  0:03:36s\n",
      "epoch 8  | loss: 0.29711 | val_0_unsup_loss_numpy: 0.25376999378204346|  0:04:03s\n",
      "epoch 9  | loss: 0.27957 | val_0_unsup_loss_numpy: 0.24987000226974487|  0:04:30s\n",
      "epoch 10 | loss: 0.26256 | val_0_unsup_loss_numpy: 0.2505599856376648|  0:04:57s\n",
      "epoch 11 | loss: 0.24855 | val_0_unsup_loss_numpy: 0.25780999660491943|  0:05:23s\n",
      "epoch 12 | loss: 0.23393 | val_0_unsup_loss_numpy: 0.2634199857711792|  0:05:50s\n",
      "epoch 13 | loss: 0.22167 | val_0_unsup_loss_numpy: 0.2675800025463104|  0:06:18s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 9 and best_val_0_unsup_loss_numpy = 0.24987000226974487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.18178 | val_0_matthews: 0.97916 |  0:00:20s\n",
      "epoch 1  | loss: 0.04512 | val_0_matthews: 0.98198 |  0:00:41s\n",
      "epoch 2  | loss: 0.04298 | val_0_matthews: 0.98253 |  0:01:02s\n",
      "epoch 3  | loss: 0.04164 | val_0_matthews: 0.98328 |  0:01:23s\n",
      "epoch 4  | loss: 0.04093 | val_0_matthews: 0.98347 |  0:01:44s\n",
      "epoch 5  | loss: 0.04043 | val_0_matthews: 0.9836  |  0:02:05s\n",
      "epoch 6  | loss: 0.03999 | val_0_matthews: 0.98361 |  0:02:25s\n",
      "epoch 7  | loss: 0.0396  | val_0_matthews: 0.98374 |  0:02:45s\n",
      "epoch 8  | loss: 0.0394  | val_0_matthews: 0.9839  |  0:03:06s\n",
      "epoch 9  | loss: 0.03897 | val_0_matthews: 0.9839  |  0:03:27s\n",
      "epoch 10 | loss: 0.03877 | val_0_matthews: 0.98382 |  0:03:48s\n",
      "epoch 11 | loss: 0.03855 | val_0_matthews: 0.98403 |  0:04:09s\n",
      "epoch 12 | loss: 0.03841 | val_0_matthews: 0.98401 |  0:04:29s\n",
      "epoch 13 | loss: 0.03813 | val_0_matthews: 0.98393 |  0:04:50s\n",
      "epoch 14 | loss: 0.03813 | val_0_matthews: 0.98377 |  0:05:10s\n",
      "epoch 15 | loss: 0.03811 | val_0_matthews: 0.9841  |  0:05:31s\n",
      "epoch 16 | loss: 0.03809 | val_0_matthews: 0.98396 |  0:05:52s\n",
      "epoch 17 | loss: 0.03765 | val_0_matthews: 0.98403 |  0:06:13s\n",
      "epoch 18 | loss: 0.03753 | val_0_matthews: 0.98397 |  0:06:33s\n",
      "epoch 19 | loss: 0.0376  | val_0_matthews: 0.98398 |  0:06:54s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 15 and best_val_0_matthews = 0.9841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-26 23:54:10,371] Trial 1 finished with value: 0.9841035008430481 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.05177521192046732, 'PRETRAIN_RATIO': 0.33535940346856735, 'N_D': 48, 'N_STEPS': 4, 'GAMMA': 1.2724255225713796, 'N_INDEPENDENT': 1, 'N_SHARED': 2, 'BATCH_SIZE': 32768, 'VIRT_BATCH_SIZE': 512}. Best is trial 0 with value: 0.9843040704727173.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 12.97664| val_0_unsup_loss_numpy: 0.952489972114563|  0:00:45s\n",
      "epoch 1  | loss: 0.82532 | val_0_unsup_loss_numpy: 0.6208000183105469|  0:01:30s\n",
      "epoch 2  | loss: 0.62313 | val_0_unsup_loss_numpy: 0.4847800135612488|  0:02:16s\n",
      "epoch 3  | loss: 0.52428 | val_0_unsup_loss_numpy: 0.42309001088142395|  0:03:01s\n",
      "epoch 4  | loss: 0.46515 | val_0_unsup_loss_numpy: 0.385670006275177|  0:03:47s\n",
      "epoch 5  | loss: 0.42241 | val_0_unsup_loss_numpy: 0.3626300096511841|  0:04:32s\n",
      "epoch 6  | loss: 0.38823 | val_0_unsup_loss_numpy: 0.3461500108242035|  0:05:17s\n",
      "epoch 7  | loss: 0.35985 | val_0_unsup_loss_numpy: 0.3411000072956085|  0:06:03s\n",
      "epoch 8  | loss: 0.33709 | val_0_unsup_loss_numpy: 0.3414599895477295|  0:06:48s\n",
      "epoch 9  | loss: 0.31821 | val_0_unsup_loss_numpy: 0.3509899973869324|  0:07:34s\n",
      "epoch 10 | loss: 0.30254 | val_0_unsup_loss_numpy: 0.36155998706817627|  0:08:19s\n",
      "epoch 11 | loss: 0.28993 | val_0_unsup_loss_numpy: 0.36131998896598816|  0:09:05s\n",
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 7 and best_val_0_unsup_loss_numpy = 0.3411000072956085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.16536 | val_0_matthews: 0.97491 |  0:00:39s\n",
      "epoch 1  | loss: 0.05308 | val_0_matthews: 0.97809 |  0:01:19s\n",
      "epoch 2  | loss: 0.04905 | val_0_matthews: 0.97947 |  0:01:58s\n",
      "epoch 3  | loss: 0.04632 | val_0_matthews: 0.98047 |  0:02:38s\n",
      "epoch 4  | loss: 0.04547 | val_0_matthews: 0.98084 |  0:03:17s\n",
      "epoch 5  | loss: 0.04427 | val_0_matthews: 0.98149 |  0:03:57s\n",
      "epoch 6  | loss: 0.04348 | val_0_matthews: 0.98186 |  0:04:37s\n",
      "epoch 7  | loss: 0.04272 | val_0_matthews: 0.98202 |  0:05:16s\n",
      "epoch 8  | loss: 0.04217 | val_0_matthews: 0.98229 |  0:05:56s\n",
      "epoch 9  | loss: 0.0419  | val_0_matthews: 0.98263 |  0:06:36s\n",
      "epoch 10 | loss: 0.04164 | val_0_matthews: 0.9828  |  0:07:15s\n",
      "epoch 11 | loss: 0.04247 | val_0_matthews: 0.98231 |  0:07:55s\n",
      "epoch 12 | loss: 0.04154 | val_0_matthews: 0.98288 |  0:08:34s\n",
      "epoch 13 | loss: 0.04127 | val_0_matthews: 0.98296 |  0:09:15s\n",
      "epoch 14 | loss: 0.04074 | val_0_matthews: 0.98295 |  0:09:54s\n",
      "epoch 15 | loss: 0.0404  | val_0_matthews: 0.98312 |  0:10:34s\n",
      "epoch 16 | loss: 0.03998 | val_0_matthews: 0.98319 |  0:11:13s\n",
      "epoch 17 | loss: 0.03974 | val_0_matthews: 0.98331 |  0:11:54s\n",
      "epoch 18 | loss: 0.03949 | val_0_matthews: 0.9834  |  0:12:33s\n",
      "epoch 19 | loss: 0.03932 | val_0_matthews: 0.9834  |  0:13:13s\n",
      "epoch 20 | loss: 0.03918 | val_0_matthews: 0.98341 |  0:13:52s\n",
      "epoch 21 | loss: 0.03887 | val_0_matthews: 0.98341 |  0:14:32s\n",
      "epoch 22 | loss: 0.03866 | val_0_matthews: 0.98362 |  0:15:12s\n",
      "epoch 23 | loss: 0.03845 | val_0_matthews: 0.98365 |  0:15:52s\n",
      "epoch 24 | loss: 0.03838 | val_0_matthews: 0.98351 |  0:16:31s\n",
      "epoch 25 | loss: 0.03815 | val_0_matthews: 0.98346 |  0:17:12s\n",
      "epoch 26 | loss: 0.03824 | val_0_matthews: 0.9838  |  0:17:52s\n",
      "epoch 27 | loss: 0.03804 | val_0_matthews: 0.98374 |  0:18:31s\n",
      "epoch 28 | loss: 0.03775 | val_0_matthews: 0.98364 |  0:19:11s\n",
      "epoch 29 | loss: 0.03769 | val_0_matthews: 0.9837  |  0:19:51s\n",
      "epoch 30 | loss: 0.03747 | val_0_matthews: 0.98381 |  0:20:31s\n",
      "epoch 31 | loss: 0.03736 | val_0_matthews: 0.98354 |  0:21:10s\n",
      "Stop training because you reached max_epochs = 32 with best_epoch = 30 and best_val_0_matthews = 0.98381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 00:53:43,159] Trial 2 finished with value: 0.9838061332702637 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.0011582633989429404, 'PRETRAIN_RATIO': 0.21888439051680197, 'N_D': 48, 'N_STEPS': 6, 'GAMMA': 1.0060595706405868, 'N_INDEPENDENT': 4, 'N_SHARED': 5, 'BATCH_SIZE': 8192, 'VIRT_BATCH_SIZE': 1536}. Best is trial 0 with value: 0.9843040704727173.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.91762 | val_0_unsup_loss_numpy: 0.8167099952697754|  0:00:42s\n",
      "epoch 1  | loss: 0.77357 | val_0_unsup_loss_numpy: 0.7089999914169312|  0:01:25s\n",
      "epoch 2  | loss: 0.70572 | val_0_unsup_loss_numpy: 0.6526399850845337|  0:02:08s\n",
      "epoch 3  | loss: 0.67329 | val_0_unsup_loss_numpy: 0.632390022277832|  0:02:51s\n",
      "epoch 4  | loss: 0.64781 | val_0_unsup_loss_numpy: 0.6293299794197083|  0:03:34s\n",
      "epoch 5  | loss: 0.63398 | val_0_unsup_loss_numpy: 0.6268399953842163|  0:04:16s\n",
      "epoch 6  | loss: 0.63471 | val_0_unsup_loss_numpy: 0.6197699904441833|  0:05:00s\n",
      "epoch 7  | loss: 0.61774 | val_0_unsup_loss_numpy: 0.6104099750518799|  0:05:43s\n",
      "epoch 8  | loss: 0.6028  | val_0_unsup_loss_numpy: 0.602869987487793|  0:06:26s\n",
      "epoch 9  | loss: 0.58645 | val_0_unsup_loss_numpy: 0.6052899956703186|  0:07:09s\n",
      "epoch 10 | loss: 0.57466 | val_0_unsup_loss_numpy: 0.6065899729728699|  0:07:52s\n",
      "epoch 11 | loss: 0.57711 | val_0_unsup_loss_numpy: 0.609220027923584|  0:08:35s\n",
      "epoch 12 | loss: 0.55701 | val_0_unsup_loss_numpy: 0.6175299882888794|  0:09:18s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 8 and best_val_0_unsup_loss_numpy = 0.602869987487793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.2708  | val_0_matthews: 0.97483 |  0:00:35s\n",
      "epoch 1  | loss: 0.05012 | val_0_matthews: 0.98    |  0:01:11s\n",
      "epoch 2  | loss: 0.04526 | val_0_matthews: 0.98114 |  0:01:46s\n",
      "epoch 3  | loss: 0.04342 | val_0_matthews: 0.98214 |  0:02:22s\n",
      "epoch 4  | loss: 0.04371 | val_0_matthews: 0.98202 |  0:02:57s\n",
      "epoch 5  | loss: 0.04241 | val_0_matthews: 0.98271 |  0:03:33s\n",
      "epoch 6  | loss: 0.04118 | val_0_matthews: 0.98321 |  0:04:09s\n",
      "epoch 7  | loss: 0.04069 | val_0_matthews: 0.98291 |  0:04:44s\n",
      "epoch 8  | loss: 0.0404  | val_0_matthews: 0.98323 |  0:05:20s\n",
      "epoch 9  | loss: 0.03998 | val_0_matthews: 0.9833  |  0:05:55s\n",
      "epoch 10 | loss: 0.04051 | val_0_matthews: 0.98313 |  0:06:31s\n",
      "epoch 11 | loss: 0.03988 | val_0_matthews: 0.9832  |  0:07:07s\n",
      "epoch 12 | loss: 0.03923 | val_0_matthews: 0.98358 |  0:07:42s\n",
      "epoch 13 | loss: 0.03892 | val_0_matthews: 0.98358 |  0:08:18s\n",
      "epoch 14 | loss: 0.03883 | val_0_matthews: 0.9837  |  0:08:54s\n",
      "epoch 15 | loss: 0.03845 | val_0_matthews: 0.9835  |  0:09:29s\n",
      "epoch 16 | loss: 0.03833 | val_0_matthews: 0.98394 |  0:10:04s\n",
      "epoch 17 | loss: 0.03826 | val_0_matthews: 0.98356 |  0:10:40s\n",
      "epoch 18 | loss: 0.03823 | val_0_matthews: 0.98369 |  0:11:16s\n",
      "epoch 19 | loss: 0.03821 | val_0_matthews: 0.98382 |  0:11:51s\n",
      "epoch 20 | loss: 0.03809 | val_0_matthews: 0.98381 |  0:12:27s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 16 and best_val_0_matthews = 0.98394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 01:21:17,080] Trial 3 finished with value: 0.983942985534668 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.08149825123176183, 'PRETRAIN_RATIO': 0.20000333644173357, 'N_D': 16, 'N_STEPS': 5, 'GAMMA': 1.2523015930295252, 'N_INDEPENDENT': 2, 'N_SHARED': 4, 'BATCH_SIZE': 32768, 'VIRT_BATCH_SIZE': 512}. Best is trial 0 with value: 0.9843040704727173.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.64606 | val_0_unsup_loss_numpy: 0.8087999820709229|  0:00:31s\n",
      "epoch 1  | loss: 0.73114 | val_0_unsup_loss_numpy: 0.5806499719619751|  0:01:01s\n",
      "epoch 2  | loss: 0.59355 | val_0_unsup_loss_numpy: 0.49055999517440796|  0:01:32s\n",
      "epoch 3  | loss: 0.49961 | val_0_unsup_loss_numpy: 0.420740008354187|  0:02:03s\n",
      "epoch 4  | loss: 0.42599 | val_0_unsup_loss_numpy: 0.3620699942111969|  0:02:34s\n",
      "epoch 5  | loss: 0.3672  | val_0_unsup_loss_numpy: 0.43149998784065247|  0:03:06s\n",
      "epoch 6  | loss: 0.31989 | val_0_unsup_loss_numpy: 0.39789000153541565|  0:03:37s\n",
      "epoch 7  | loss: 0.28084 | val_0_unsup_loss_numpy: 0.4309999942779541|  0:04:08s\n",
      "epoch 8  | loss: 0.2478  | val_0_unsup_loss_numpy: 0.4206799864768982|  0:04:39s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 4 and best_val_0_unsup_loss_numpy = 0.3620699942111969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.0687  | val_0_matthews: 0.98171 |  0:00:25s\n",
      "epoch 1  | loss: 0.04347 | val_0_matthews: 0.98319 |  0:00:51s\n",
      "epoch 2  | loss: 0.04166 | val_0_matthews: 0.98342 |  0:01:16s\n",
      "epoch 3  | loss: 0.04096 | val_0_matthews: 0.98389 |  0:01:41s\n",
      "epoch 4  | loss: 0.04031 | val_0_matthews: 0.98383 |  0:02:07s\n",
      "epoch 5  | loss: 0.03986 | val_0_matthews: 0.9839  |  0:02:32s\n",
      "epoch 6  | loss: 0.03942 | val_0_matthews: 0.98382 |  0:02:57s\n",
      "epoch 7  | loss: 0.03903 | val_0_matthews: 0.98399 |  0:03:23s\n",
      "epoch 8  | loss: 0.0388  | val_0_matthews: 0.98403 |  0:03:48s\n",
      "epoch 9  | loss: 0.03851 | val_0_matthews: 0.98416 |  0:04:13s\n",
      "epoch 10 | loss: 0.03818 | val_0_matthews: 0.98397 |  0:04:38s\n",
      "epoch 11 | loss: 0.03789 | val_0_matthews: 0.9841  |  0:05:04s\n",
      "epoch 12 | loss: 0.03758 | val_0_matthews: 0.98404 |  0:05:29s\n",
      "epoch 13 | loss: 0.03736 | val_0_matthews: 0.98399 |  0:05:54s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 9 and best_val_0_matthews = 0.98416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 01:38:09,315] Trial 4 finished with value: 0.9841607809066772 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.0057367587003765595, 'PRETRAIN_RATIO': 0.4623795708794746, 'N_D': 64, 'N_STEPS': 3, 'GAMMA': 1.6089116204639282, 'N_INDEPENDENT': 3, 'N_SHARED': 1, 'BATCH_SIZE': 8192, 'VIRT_BATCH_SIZE': 512}. Best is trial 0 with value: 0.9843040704727173.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.87276 | val_0_unsup_loss_numpy: 0.46355000138282776|  0:00:57s\n",
      "epoch 1  | loss: 0.40422 | val_0_unsup_loss_numpy: 0.30410000681877136|  0:01:55s\n",
      "epoch 2  | loss: 0.29376 | val_0_unsup_loss_numpy: 0.2790899872779846|  0:02:52s\n",
      "epoch 3  | loss: 0.25174 | val_0_unsup_loss_numpy: 0.2829599976539612|  0:03:50s\n",
      "epoch 4  | loss: 0.23333 | val_0_unsup_loss_numpy: 0.28220000863075256|  0:04:48s\n",
      "epoch 5  | loss: 0.22073 | val_0_unsup_loss_numpy: 0.2897599935531616|  0:05:45s\n",
      "epoch 6  | loss: 0.21843 | val_0_unsup_loss_numpy: 0.3172700107097626|  0:06:44s\n",
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 2 and best_val_0_unsup_loss_numpy = 0.2790899872779846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.10946 | val_0_matthews: 0.97889 |  0:00:49s\n",
      "epoch 1  | loss: 0.04735 | val_0_matthews: 0.98099 |  0:01:38s\n",
      "epoch 2  | loss: 0.04435 | val_0_matthews: 0.98271 |  0:02:28s\n",
      "epoch 3  | loss: 0.04259 | val_0_matthews: 0.9825  |  0:03:18s\n",
      "epoch 4  | loss: 0.0416  | val_0_matthews: 0.98346 |  0:04:07s\n",
      "epoch 5  | loss: 0.04084 | val_0_matthews: 0.98359 |  0:04:57s\n",
      "epoch 6  | loss: 0.04055 | val_0_matthews: 0.98376 |  0:05:46s\n",
      "epoch 7  | loss: 0.03997 | val_0_matthews: 0.98376 |  0:06:35s\n",
      "epoch 8  | loss: 0.03967 | val_0_matthews: 0.98348 |  0:07:25s\n",
      "epoch 9  | loss: 0.04007 | val_0_matthews: 0.98359 |  0:08:14s\n",
      "epoch 10 | loss: 0.03943 | val_0_matthews: 0.98395 |  0:09:04s\n",
      "epoch 11 | loss: 0.03881 | val_0_matthews: 0.98401 |  0:09:54s\n",
      "epoch 12 | loss: 0.03868 | val_0_matthews: 0.98398 |  0:10:43s\n",
      "epoch 13 | loss: 0.03845 | val_0_matthews: 0.98402 |  0:11:33s\n",
      "epoch 14 | loss: 0.03876 | val_0_matthews: 0.98385 |  0:12:23s\n",
      "epoch 15 | loss: 0.03793 | val_0_matthews: 0.98428 |  0:13:12s\n",
      "epoch 16 | loss: 0.03981 | val_0_matthews: 0.98366 |  0:14:02s\n",
      "epoch 17 | loss: 0.03816 | val_0_matthews: 0.98425 |  0:14:52s\n",
      "epoch 18 | loss: 0.03786 | val_0_matthews: 0.98417 |  0:15:41s\n",
      "epoch 19 | loss: 0.03705 | val_0_matthews: 0.98435 |  0:16:30s\n",
      "epoch 20 | loss: 0.03668 | val_0_matthews: 0.98436 |  0:17:20s\n",
      "epoch 21 | loss: 0.03622 | val_0_matthews: 0.98411 |  0:18:09s\n",
      "epoch 22 | loss: 0.03606 | val_0_matthews: 0.98434 |  0:18:58s\n",
      "epoch 23 | loss: 0.03576 | val_0_matthews: 0.98433 |  0:19:48s\n",
      "epoch 24 | loss: 0.03567 | val_0_matthews: 0.98448 |  0:20:37s\n",
      "epoch 25 | loss: 0.03559 | val_0_matthews: 0.98429 |  0:21:27s\n",
      "epoch 26 | loss: 0.03547 | val_0_matthews: 0.98407 |  0:22:16s\n",
      "epoch 27 | loss: 0.03526 | val_0_matthews: 0.98403 |  0:23:06s\n",
      "epoch 28 | loss: 0.03525 | val_0_matthews: 0.98413 |  0:23:55s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 24 and best_val_0_matthews = 0.98448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 02:21:16,545] Trial 5 finished with value: 0.9844775795936584 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.0229197581805364, 'PRETRAIN_RATIO': 0.2780448939884285, 'N_D': 48, 'N_STEPS': 5, 'GAMMA': 1.0517101911301066, 'N_INDEPENDENT': 5, 'N_SHARED': 3, 'BATCH_SIZE': 12288, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.53531 | val_0_unsup_loss_numpy: 0.9700300097465515|  0:00:23s\n",
      "epoch 1  | loss: 0.93629 | val_0_unsup_loss_numpy: 0.9398900270462036|  0:00:47s\n",
      "epoch 2  | loss: 0.87278 | val_0_unsup_loss_numpy: 0.9209700226783752|  0:01:11s\n",
      "epoch 3  | loss: 0.82744 | val_0_unsup_loss_numpy: 0.8045099973678589|  0:01:35s\n",
      "epoch 4  | loss: 0.80474 | val_0_unsup_loss_numpy: 0.7067800164222717|  0:01:58s\n",
      "epoch 5  | loss: 0.78587 | val_0_unsup_loss_numpy: 0.6767799854278564|  0:02:22s\n",
      "epoch 6  | loss: 0.77453 | val_0_unsup_loss_numpy: 0.676360011100769|  0:02:46s\n",
      "epoch 7  | loss: 0.76811 | val_0_unsup_loss_numpy: 0.7166100144386292|  0:03:10s\n",
      "epoch 8  | loss: 0.77055 | val_0_unsup_loss_numpy: 0.6693599820137024|  0:03:34s\n",
      "epoch 9  | loss: 0.75854 | val_0_unsup_loss_numpy: 0.671019971370697|  0:03:58s\n",
      "epoch 10 | loss: 0.75209 | val_0_unsup_loss_numpy: 0.6676899790763855|  0:04:21s\n",
      "epoch 11 | loss: 0.75568 | val_0_unsup_loss_numpy: 0.6711000204086304|  0:04:45s\n",
      "epoch 12 | loss: 0.74971 | val_0_unsup_loss_numpy: 0.6687899827957153|  0:05:09s\n",
      "epoch 13 | loss: 0.75078 | val_0_unsup_loss_numpy: 0.6505200266838074|  0:05:33s\n",
      "epoch 14 | loss: 0.74987 | val_0_unsup_loss_numpy: 0.6493899822235107|  0:05:57s\n",
      "epoch 15 | loss: 0.74115 | val_0_unsup_loss_numpy: 0.6309800148010254|  0:06:20s\n",
      "epoch 16 | loss: 0.73772 | val_0_unsup_loss_numpy: 0.6721199750900269|  0:06:44s\n",
      "epoch 17 | loss: 0.77511 | val_0_unsup_loss_numpy: 0.676800012588501|  0:07:08s\n",
      "epoch 18 | loss: 0.75664 | val_0_unsup_loss_numpy: 0.660099983215332|  0:07:32s\n",
      "epoch 19 | loss: 0.75371 | val_0_unsup_loss_numpy: 0.6794300079345703|  0:07:55s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 15 and best_val_0_unsup_loss_numpy = 0.6309800148010254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.56528 | val_0_matthews: 0.61688 |  0:00:19s\n",
      "epoch 1  | loss: 0.23291 | val_0_matthews: 0.87052 |  0:00:38s\n",
      "epoch 2  | loss: 0.14777 | val_0_matthews: 0.85945 |  0:00:58s\n",
      "epoch 3  | loss: 0.13966 | val_0_matthews: 0.91359 |  0:01:17s\n",
      "epoch 4  | loss: 0.11259 | val_0_matthews: 0.92275 |  0:01:36s\n",
      "epoch 5  | loss: 0.1012  | val_0_matthews: 0.9422  |  0:01:55s\n",
      "epoch 6  | loss: 0.0852  | val_0_matthews: 0.95196 |  0:02:15s\n",
      "epoch 7  | loss: 0.07702 | val_0_matthews: 0.95452 |  0:02:34s\n",
      "epoch 8  | loss: 0.07157 | val_0_matthews: 0.96719 |  0:02:54s\n",
      "epoch 9  | loss: 0.06443 | val_0_matthews: 0.96781 |  0:03:14s\n",
      "epoch 10 | loss: 0.0612  | val_0_matthews: 0.97184 |  0:03:33s\n",
      "epoch 11 | loss: 0.05666 | val_0_matthews: 0.97438 |  0:03:52s\n",
      "epoch 12 | loss: 0.0538  | val_0_matthews: 0.97828 |  0:04:12s\n",
      "epoch 13 | loss: 0.04838 | val_0_matthews: 0.97937 |  0:04:31s\n",
      "epoch 14 | loss: 0.04757 | val_0_matthews: 0.97697 |  0:04:50s\n",
      "epoch 15 | loss: 0.04783 | val_0_matthews: 0.9804  |  0:05:10s\n",
      "epoch 16 | loss: 0.04497 | val_0_matthews: 0.98102 |  0:05:29s\n",
      "epoch 17 | loss: 0.04442 | val_0_matthews: 0.98129 |  0:05:48s\n",
      "epoch 18 | loss: 0.04346 | val_0_matthews: 0.98148 |  0:06:07s\n",
      "epoch 19 | loss: 0.04321 | val_0_matthews: 0.98164 |  0:06:26s\n",
      "epoch 20 | loss: 0.04255 | val_0_matthews: 0.98174 |  0:06:46s\n",
      "epoch 21 | loss: 0.04909 | val_0_matthews: 0.98082 |  0:07:05s\n",
      "epoch 22 | loss: 0.04356 | val_0_matthews: 0.98151 |  0:07:24s\n",
      "epoch 23 | loss: 0.04228 | val_0_matthews: 0.98203 |  0:07:43s\n",
      "epoch 24 | loss: 0.10621 | val_0_matthews: 0.9636  |  0:08:03s\n",
      "epoch 25 | loss: 0.07177 | val_0_matthews: 0.9657  |  0:08:22s\n",
      "epoch 26 | loss: 0.06822 | val_0_matthews: 0.9649  |  0:08:41s\n",
      "epoch 27 | loss: 0.06272 | val_0_matthews: 0.97428 |  0:09:01s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 23 and best_val_0_matthews = 0.98203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 02:40:41,939] Trial 6 finished with value: 0.9820330142974854 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.09329898420966148, 'PRETRAIN_RATIO': 0.6725180566058904, 'N_D': 32, 'N_STEPS': 3, 'GAMMA': 1.8832346980766994, 'N_INDEPENDENT': 2, 'N_SHARED': 2, 'BATCH_SIZE': 32768, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.90575 | val_0_unsup_loss_numpy: 0.985289990901947|  0:00:14s\n",
      "epoch 1  | loss: 0.961   | val_0_unsup_loss_numpy: 0.9287999868392944|  0:00:29s\n",
      "epoch 2  | loss: 0.93134 | val_0_unsup_loss_numpy: 0.8415300250053406|  0:00:44s\n",
      "epoch 3  | loss: 0.90568 | val_0_unsup_loss_numpy: 0.7952600121498108|  0:00:59s\n",
      "epoch 4  | loss: 0.8824  | val_0_unsup_loss_numpy: 0.7642199993133545|  0:01:14s\n",
      "epoch 5  | loss: 0.86013 | val_0_unsup_loss_numpy: 0.73403000831604|  0:01:29s\n",
      "epoch 6  | loss: 0.83611 | val_0_unsup_loss_numpy: 0.7079100012779236|  0:01:44s\n",
      "epoch 7  | loss: 0.8141  | val_0_unsup_loss_numpy: 0.6870200037956238|  0:01:59s\n",
      "epoch 8  | loss: 0.79692 | val_0_unsup_loss_numpy: 0.6717100143432617|  0:02:14s\n",
      "epoch 9  | loss: 0.77944 | val_0_unsup_loss_numpy: 0.6693099737167358|  0:02:29s\n",
      "epoch 10 | loss: 0.75918 | val_0_unsup_loss_numpy: 0.6583600044250488|  0:02:44s\n",
      "epoch 11 | loss: 0.73885 | val_0_unsup_loss_numpy: 0.6457899808883667|  0:02:59s\n",
      "epoch 12 | loss: 0.72221 | val_0_unsup_loss_numpy: 0.6338000297546387|  0:03:14s\n",
      "epoch 13 | loss: 0.70446 | val_0_unsup_loss_numpy: 0.6303099989891052|  0:03:29s\n",
      "epoch 14 | loss: 0.68656 | val_0_unsup_loss_numpy: 0.613319993019104|  0:03:44s\n",
      "epoch 15 | loss: 0.67159 | val_0_unsup_loss_numpy: 0.6002100110054016|  0:03:59s\n",
      "epoch 16 | loss: 0.65682 | val_0_unsup_loss_numpy: 0.5777900218963623|  0:04:14s\n",
      "epoch 17 | loss: 0.6452  | val_0_unsup_loss_numpy: 0.5716400146484375|  0:04:29s\n",
      "epoch 18 | loss: 0.63379 | val_0_unsup_loss_numpy: 0.5875099897384644|  0:04:44s\n",
      "epoch 19 | loss: 0.62348 | val_0_unsup_loss_numpy: 0.578819990158081|  0:04:59s\n",
      "epoch 20 | loss: 0.61534 | val_0_unsup_loss_numpy: 0.5726000070571899|  0:05:14s\n",
      "epoch 21 | loss: 0.60873 | val_0_unsup_loss_numpy: 0.5490400195121765|  0:05:29s\n",
      "epoch 22 | loss: 0.60348 | val_0_unsup_loss_numpy: 0.5893700122833252|  0:05:44s\n",
      "epoch 23 | loss: 0.59847 | val_0_unsup_loss_numpy: 0.5627099871635437|  0:05:58s\n",
      "epoch 24 | loss: 0.59289 | val_0_unsup_loss_numpy: 0.5502899885177612|  0:06:14s\n",
      "epoch 25 | loss: 0.58845 | val_0_unsup_loss_numpy: 0.5463200211524963|  0:06:29s\n",
      "epoch 26 | loss: 0.58379 | val_0_unsup_loss_numpy: 0.537090003490448|  0:06:44s\n",
      "epoch 27 | loss: 0.57893 | val_0_unsup_loss_numpy: 0.5392799973487854|  0:06:59s\n",
      "epoch 28 | loss: 0.57442 | val_0_unsup_loss_numpy: 0.5396999716758728|  0:07:14s\n",
      "epoch 29 | loss: 0.57096 | val_0_unsup_loss_numpy: 0.5401099920272827|  0:07:29s\n",
      "epoch 30 | loss: 0.56679 | val_0_unsup_loss_numpy: 0.525950014591217|  0:07:44s\n",
      "epoch 31 | loss: 0.56477 | val_0_unsup_loss_numpy: 0.5570099949836731|  0:07:59s\n",
      "Stop training because you reached max_epochs = 32 with best_epoch = 30 and best_val_0_unsup_loss_numpy = 0.525950014591217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.12802 | val_0_matthews: 0.87113 |  0:00:12s\n",
      "epoch 1  | loss: 0.05737 | val_0_matthews: 0.96098 |  0:00:25s\n",
      "epoch 2  | loss: 0.04821 | val_0_matthews: 0.98125 |  0:00:38s\n",
      "epoch 3  | loss: 0.04616 | val_0_matthews: 0.98163 |  0:00:50s\n",
      "epoch 4  | loss: 0.04424 | val_0_matthews: 0.98214 |  0:01:03s\n",
      "epoch 5  | loss: 0.04294 | val_0_matthews: 0.98263 |  0:01:16s\n",
      "epoch 6  | loss: 0.04219 | val_0_matthews: 0.98292 |  0:01:28s\n",
      "epoch 7  | loss: 0.04264 | val_0_matthews: 0.98305 |  0:01:41s\n",
      "epoch 8  | loss: 0.04065 | val_0_matthews: 0.98316 |  0:01:54s\n",
      "epoch 9  | loss: 0.0403  | val_0_matthews: 0.98334 |  0:02:07s\n",
      "epoch 10 | loss: 0.03988 | val_0_matthews: 0.98354 |  0:02:20s\n",
      "epoch 11 | loss: 0.0399  | val_0_matthews: 0.9832  |  0:02:32s\n",
      "epoch 12 | loss: 0.03955 | val_0_matthews: 0.98356 |  0:02:45s\n",
      "epoch 13 | loss: 0.03934 | val_0_matthews: 0.98347 |  0:02:58s\n",
      "epoch 14 | loss: 0.03962 | val_0_matthews: 0.97955 |  0:03:10s\n",
      "epoch 15 | loss: 0.04021 | val_0_matthews: 0.98359 |  0:03:23s\n",
      "epoch 16 | loss: 0.03885 | val_0_matthews: 0.9837  |  0:03:36s\n",
      "epoch 17 | loss: 0.03981 | val_0_matthews: 0.97936 |  0:03:49s\n",
      "epoch 18 | loss: 0.04047 | val_0_matthews: 0.98367 |  0:04:02s\n",
      "epoch 19 | loss: 0.03852 | val_0_matthews: 0.98373 |  0:04:14s\n",
      "epoch 20 | loss: 0.03824 | val_0_matthews: 0.98364 |  0:04:27s\n",
      "epoch 21 | loss: 0.03928 | val_0_matthews: 0.98372 |  0:04:40s\n",
      "epoch 22 | loss: 0.03826 | val_0_matthews: 0.98368 |  0:04:53s\n",
      "epoch 23 | loss: 0.03789 | val_0_matthews: 0.98376 |  0:05:05s\n",
      "epoch 24 | loss: 0.03837 | val_0_matthews: 0.98379 |  0:05:18s\n",
      "epoch 25 | loss: 0.03768 | val_0_matthews: 0.98379 |  0:05:31s\n",
      "epoch 26 | loss: 0.03746 | val_0_matthews: 0.98389 |  0:05:44s\n",
      "epoch 27 | loss: 0.03729 | val_0_matthews: 0.9837  |  0:05:57s\n",
      "epoch 28 | loss: 0.03728 | val_0_matthews: 0.98395 |  0:06:10s\n",
      "epoch 29 | loss: 0.0372  | val_0_matthews: 0.98392 |  0:06:22s\n",
      "epoch 30 | loss: 0.03694 | val_0_matthews: 0.98376 |  0:06:35s\n",
      "epoch 31 | loss: 0.03704 | val_0_matthews: 0.98403 |  0:06:48s\n",
      "Stop training because you reached max_epochs = 32 with best_epoch = 31 and best_val_0_matthews = 0.98403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 02:58:47,110] Trial 7 finished with value: 0.9840347766876221 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.013422864871720619, 'PRETRAIN_RATIO': 0.7557292508050206, 'N_D': 32, 'N_STEPS': 3, 'GAMMA': 1.6941571165474785, 'N_INDEPENDENT': 2, 'N_SHARED': 1, 'BATCH_SIZE': 20480, 'VIRT_BATCH_SIZE': 1024}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.13202 | val_0_unsup_loss_numpy: 0.8613799810409546|  0:00:14s\n",
      "epoch 1  | loss: 0.81093 | val_0_unsup_loss_numpy: 0.7103000283241272|  0:00:27s\n",
      "epoch 2  | loss: 0.73423 | val_0_unsup_loss_numpy: 0.6508700251579285|  0:00:42s\n",
      "epoch 3  | loss: 0.69959 | val_0_unsup_loss_numpy: 0.6380100250244141|  0:00:56s\n",
      "epoch 4  | loss: 0.67829 | val_0_unsup_loss_numpy: 0.6300100088119507|  0:01:10s\n",
      "epoch 5  | loss: 0.66574 | val_0_unsup_loss_numpy: 0.624779999256134|  0:01:25s\n",
      "epoch 6  | loss: 0.65436 | val_0_unsup_loss_numpy: 0.6267600059509277|  0:01:40s\n",
      "epoch 7  | loss: 0.64915 | val_0_unsup_loss_numpy: 0.6246500015258789|  0:01:53s\n",
      "epoch 8  | loss: 0.64321 | val_0_unsup_loss_numpy: 0.6240699887275696|  0:02:08s\n",
      "epoch 9  | loss: 0.64019 | val_0_unsup_loss_numpy: 0.6206300258636475|  0:02:22s\n",
      "epoch 10 | loss: 0.63424 | val_0_unsup_loss_numpy: 0.620199978351593|  0:02:36s\n",
      "epoch 11 | loss: 0.63122 | val_0_unsup_loss_numpy: 0.6183000206947327|  0:02:50s\n",
      "epoch 12 | loss: 0.63071 | val_0_unsup_loss_numpy: 0.616349995136261|  0:03:05s\n",
      "epoch 13 | loss: 0.62767 | val_0_unsup_loss_numpy: 0.6172199845314026|  0:03:19s\n",
      "epoch 14 | loss: 0.63044 | val_0_unsup_loss_numpy: 0.6159899830818176|  0:03:33s\n",
      "epoch 15 | loss: 0.62516 | val_0_unsup_loss_numpy: 0.6146000027656555|  0:03:47s\n",
      "epoch 16 | loss: 0.62262 | val_0_unsup_loss_numpy: 0.6135900020599365|  0:04:02s\n",
      "epoch 17 | loss: 0.61964 | val_0_unsup_loss_numpy: 0.6094200015068054|  0:04:16s\n",
      "epoch 18 | loss: 0.61944 | val_0_unsup_loss_numpy: 0.6131700277328491|  0:04:30s\n",
      "epoch 19 | loss: 0.61694 | val_0_unsup_loss_numpy: 0.613860011100769|  0:04:44s\n",
      "epoch 20 | loss: 0.61984 | val_0_unsup_loss_numpy: 0.6143900156021118|  0:04:58s\n",
      "epoch 21 | loss: 0.61687 | val_0_unsup_loss_numpy: 0.6089400053024292|  0:05:13s\n",
      "epoch 22 | loss: 0.61472 | val_0_unsup_loss_numpy: 0.6062800288200378|  0:05:27s\n",
      "epoch 23 | loss: 0.62443 | val_0_unsup_loss_numpy: 0.6151599884033203|  0:05:42s\n",
      "epoch 24 | loss: 0.61581 | val_0_unsup_loss_numpy: 0.6122099757194519|  0:05:56s\n",
      "epoch 25 | loss: 0.61336 | val_0_unsup_loss_numpy: 0.610480010509491|  0:06:09s\n",
      "epoch 26 | loss: 0.60946 | val_0_unsup_loss_numpy: 0.6108300089836121|  0:06:24s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 22 and best_val_0_unsup_loss_numpy = 0.6062800288200378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.1125  | val_0_matthews: 0.97685 |  0:00:11s\n",
      "epoch 1  | loss: 0.04722 | val_0_matthews: 0.98045 |  0:00:23s\n",
      "epoch 2  | loss: 0.04441 | val_0_matthews: 0.98117 |  0:00:34s\n",
      "epoch 3  | loss: 0.04311 | val_0_matthews: 0.98185 |  0:00:45s\n",
      "epoch 4  | loss: 0.042   | val_0_matthews: 0.98243 |  0:00:56s\n",
      "epoch 5  | loss: 0.04148 | val_0_matthews: 0.98266 |  0:01:08s\n",
      "epoch 6  | loss: 0.04099 | val_0_matthews: 0.98259 |  0:01:19s\n",
      "epoch 7  | loss: 0.04058 | val_0_matthews: 0.98307 |  0:01:30s\n",
      "epoch 8  | loss: 0.04044 | val_0_matthews: 0.98299 |  0:01:42s\n",
      "epoch 9  | loss: 0.04007 | val_0_matthews: 0.98317 |  0:01:54s\n",
      "epoch 10 | loss: 0.03973 | val_0_matthews: 0.98329 |  0:02:05s\n",
      "epoch 11 | loss: 0.03953 | val_0_matthews: 0.98345 |  0:02:16s\n",
      "epoch 12 | loss: 0.0394  | val_0_matthews: 0.98365 |  0:02:28s\n",
      "epoch 13 | loss: 0.03909 | val_0_matthews: 0.98375 |  0:02:39s\n",
      "epoch 14 | loss: 0.03906 | val_0_matthews: 0.98346 |  0:02:50s\n",
      "epoch 15 | loss: 0.03897 | val_0_matthews: 0.98322 |  0:03:01s\n",
      "epoch 16 | loss: 0.03876 | val_0_matthews: 0.9837  |  0:03:13s\n",
      "epoch 17 | loss: 0.03857 | val_0_matthews: 0.98314 |  0:03:24s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 13 and best_val_0_matthews = 0.98375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 03:13:14,722] Trial 8 finished with value: 0.9837465882301331 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.04887840950671322, 'PRETRAIN_RATIO': 0.5058859879804221, 'N_D': 16, 'N_STEPS': 3, 'GAMMA': 1.9540427645572254, 'N_INDEPENDENT': 1, 'N_SHARED': 1, 'BATCH_SIZE': 12288, 'VIRT_BATCH_SIZE': 1536}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.21198 | val_0_unsup_loss_numpy: 0.7141000032424927|  0:00:50s\n",
      "epoch 1  | loss: 0.70246 | val_0_unsup_loss_numpy: 0.6797800064086914|  0:01:40s\n",
      "epoch 2  | loss: 0.66758 | val_0_unsup_loss_numpy: 0.6875600218772888|  0:02:29s\n",
      "epoch 3  | loss: 0.64813 | val_0_unsup_loss_numpy: 0.6765199899673462|  0:03:20s\n",
      "epoch 4  | loss: 0.63873 | val_0_unsup_loss_numpy: 0.7062399983406067|  0:04:10s\n",
      "epoch 5  | loss: 0.62944 | val_0_unsup_loss_numpy: 0.704259991645813|  0:05:00s\n",
      "epoch 6  | loss: 0.61999 | val_0_unsup_loss_numpy: 0.7126700282096863|  0:05:50s\n",
      "epoch 7  | loss: 0.61544 | val_0_unsup_loss_numpy: 0.6791599988937378|  0:06:40s\n",
      "\n",
      "Early stopping occurred at epoch 7 with best_epoch = 3 and best_val_0_unsup_loss_numpy = 0.6765199899673462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29075 | val_0_matthews: 0.90325 |  0:00:39s\n",
      "epoch 1  | loss: 0.08468 | val_0_matthews: 0.96964 |  0:01:19s\n",
      "epoch 2  | loss: 0.06596 | val_0_matthews: 0.97567 |  0:01:59s\n",
      "epoch 3  | loss: 0.05507 | val_0_matthews: 0.97874 |  0:02:39s\n",
      "epoch 4  | loss: 0.04834 | val_0_matthews: 0.96617 |  0:03:19s\n",
      "epoch 5  | loss: 0.04545 | val_0_matthews: 0.98116 |  0:03:59s\n",
      "epoch 6  | loss: 0.04375 | val_0_matthews: 0.98165 |  0:04:39s\n",
      "epoch 7  | loss: 0.0434  | val_0_matthews: 0.98196 |  0:05:18s\n",
      "epoch 8  | loss: 0.04218 | val_0_matthews: 0.98228 |  0:05:58s\n",
      "epoch 9  | loss: 0.04154 | val_0_matthews: 0.98213 |  0:06:38s\n",
      "epoch 10 | loss: 0.04096 | val_0_matthews: 0.98259 |  0:07:19s\n",
      "epoch 11 | loss: 0.044   | val_0_matthews: 0.98248 |  0:07:59s\n",
      "epoch 12 | loss: 0.04101 | val_0_matthews: 0.98282 |  0:08:39s\n",
      "epoch 13 | loss: 0.04032 | val_0_matthews: 0.98261 |  0:09:19s\n",
      "epoch 14 | loss: 0.04009 | val_0_matthews: 0.98296 |  0:09:58s\n",
      "epoch 15 | loss: 0.0398  | val_0_matthews: 0.98155 |  0:10:38s\n",
      "epoch 16 | loss: 0.03993 | val_0_matthews: 0.98319 |  0:11:19s\n",
      "epoch 17 | loss: 0.03953 | val_0_matthews: 0.98298 |  0:11:59s\n",
      "epoch 18 | loss: 0.03926 | val_0_matthews: 0.98333 |  0:12:38s\n",
      "epoch 19 | loss: 0.03924 | val_0_matthews: 0.98326 |  0:13:18s\n",
      "epoch 20 | loss: 0.03915 | val_0_matthews: 0.98328 |  0:13:58s\n",
      "epoch 21 | loss: 0.03899 | val_0_matthews: 0.98326 |  0:14:38s\n",
      "epoch 22 | loss: 0.03893 | val_0_matthews: 0.9835  |  0:15:18s\n",
      "epoch 23 | loss: 0.03857 | val_0_matthews: 0.98344 |  0:15:58s\n",
      "epoch 24 | loss: 0.0386  | val_0_matthews: 0.98331 |  0:16:38s\n",
      "epoch 25 | loss: 0.03836 | val_0_matthews: 0.98369 |  0:17:18s\n",
      "epoch 26 | loss: 0.0385  | val_0_matthews: 0.98306 |  0:17:58s\n",
      "epoch 27 | loss: 0.03842 | val_0_matthews: 0.98377 |  0:18:37s\n",
      "epoch 28 | loss: 0.03843 | val_0_matthews: 0.98312 |  0:19:17s\n",
      "epoch 29 | loss: 0.03809 | val_0_matthews: 0.98344 |  0:19:57s\n",
      "epoch 30 | loss: 0.03839 | val_0_matthews: 0.98282 |  0:20:37s\n",
      "epoch 31 | loss: 0.03819 | val_0_matthews: 0.98361 |  0:21:17s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 27 and best_val_0_matthews = 0.98377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 03:54:58,169] Trial 9 finished with value: 0.983772337436676 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.08437157481807553, 'PRETRAIN_RATIO': 0.2939239784426557, 'N_D': 16, 'N_STEPS': 6, 'GAMMA': 1.1681394178262812, 'N_INDEPENDENT': 4, 'N_SHARED': 1, 'BATCH_SIZE': 12288, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.10349 | val_0_unsup_loss_numpy: 0.7509400248527527|  0:00:23s\n",
      "epoch 1  | loss: 0.6358  | val_0_unsup_loss_numpy: 0.5214899778366089|  0:00:47s\n",
      "epoch 2  | loss: 0.50498 | val_0_unsup_loss_numpy: 0.3989199995994568|  0:01:11s\n",
      "epoch 3  | loss: 0.41786 | val_0_unsup_loss_numpy: 0.33917999267578125|  0:01:35s\n",
      "epoch 4  | loss: 0.37317 | val_0_unsup_loss_numpy: 0.32315999269485474|  0:01:59s\n",
      "epoch 5  | loss: 0.32741 | val_0_unsup_loss_numpy: 0.30972999334335327|  0:02:23s\n",
      "epoch 6  | loss: 0.29176 | val_0_unsup_loss_numpy: 0.3078100085258484|  0:02:47s\n",
      "epoch 7  | loss: 0.26631 | val_0_unsup_loss_numpy: 0.3121199905872345|  0:03:11s\n",
      "epoch 8  | loss: 0.24881 | val_0_unsup_loss_numpy: 0.3095000088214874|  0:03:35s\n",
      "epoch 9  | loss: 0.23795 | val_0_unsup_loss_numpy: 0.30605998635292053|  0:03:59s\n",
      "epoch 10 | loss: 0.22107 | val_0_unsup_loss_numpy: 0.3049600124359131|  0:04:23s\n",
      "epoch 11 | loss: 0.21179 | val_0_unsup_loss_numpy: 0.3023500144481659|  0:04:47s\n",
      "epoch 12 | loss: 0.20309 | val_0_unsup_loss_numpy: 0.29881998896598816|  0:05:11s\n",
      "epoch 13 | loss: 0.19472 | val_0_unsup_loss_numpy: 0.3026300072669983|  0:05:35s\n",
      "epoch 14 | loss: 0.18726 | val_0_unsup_loss_numpy: 0.30785998702049255|  0:05:59s\n",
      "epoch 15 | loss: 0.18039 | val_0_unsup_loss_numpy: 0.3210900127887726|  0:06:23s\n",
      "epoch 16 | loss: 0.17398 | val_0_unsup_loss_numpy: 0.32186999917030334|  0:06:47s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 12 and best_val_0_unsup_loss_numpy = 0.29881998896598816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.09553 | val_0_matthews: 0.97952 |  0:00:20s\n",
      "epoch 1  | loss: 0.04678 | val_0_matthews: 0.98145 |  0:00:41s\n",
      "epoch 2  | loss: 0.0438  | val_0_matthews: 0.98229 |  0:01:02s\n",
      "epoch 3  | loss: 0.04275 | val_0_matthews: 0.98249 |  0:01:24s\n",
      "epoch 4  | loss: 0.0418  | val_0_matthews: 0.98286 |  0:01:45s\n",
      "epoch 5  | loss: 0.04115 | val_0_matthews: 0.98301 |  0:02:05s\n",
      "epoch 6  | loss: 0.04087 | val_0_matthews: 0.98326 |  0:02:26s\n",
      "epoch 7  | loss: 0.04045 | val_0_matthews: 0.98317 |  0:02:46s\n",
      "epoch 8  | loss: 0.04022 | val_0_matthews: 0.98323 |  0:03:07s\n",
      "epoch 9  | loss: 0.04041 | val_0_matthews: 0.98319 |  0:03:27s\n",
      "epoch 10 | loss: 0.03994 | val_0_matthews: 0.98328 |  0:03:48s\n",
      "epoch 11 | loss: 0.0396  | val_0_matthews: 0.98336 |  0:04:09s\n",
      "epoch 12 | loss: 0.03934 | val_0_matthews: 0.9836  |  0:04:30s\n",
      "epoch 13 | loss: 0.03906 | val_0_matthews: 0.98339 |  0:04:51s\n",
      "epoch 14 | loss: 0.0389  | val_0_matthews: 0.98328 |  0:05:12s\n",
      "epoch 15 | loss: 0.03917 | val_0_matthews: 0.98366 |  0:05:32s\n",
      "epoch 16 | loss: 0.03883 | val_0_matthews: 0.98362 |  0:05:53s\n",
      "epoch 17 | loss: 0.03868 | val_0_matthews: 0.98382 |  0:06:14s\n",
      "epoch 18 | loss: 0.03851 | val_0_matthews: 0.98357 |  0:06:34s\n",
      "epoch 19 | loss: 0.03835 | val_0_matthews: 0.9838  |  0:06:55s\n",
      "epoch 20 | loss: 0.03831 | val_0_matthews: 0.98344 |  0:07:16s\n",
      "epoch 21 | loss: 0.03806 | val_0_matthews: 0.98352 |  0:07:37s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 17 and best_val_0_matthews = 0.98382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 04:20:40,097] Trial 10 finished with value: 0.9838235378265381 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.01101255376421729, 'PRETRAIN_RATIO': 0.2782404414836022, 'N_D': 48, 'N_STEPS': 5, 'GAMMA': 1.0006548532386634, 'N_INDEPENDENT': 5, 'N_SHARED': 3, 'BATCH_SIZE': 20480, 'VIRT_BATCH_SIZE': 2048}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.63057 | val_0_unsup_loss_numpy: 0.914650022983551|  0:00:29s\n",
      "epoch 1  | loss: 0.73476 | val_0_unsup_loss_numpy: 0.7121300101280212|  0:00:58s\n",
      "epoch 2  | loss: 0.60238 | val_0_unsup_loss_numpy: 0.4980100095272064|  0:01:26s\n",
      "epoch 3  | loss: 0.51505 | val_0_unsup_loss_numpy: 0.38923001289367676|  0:01:55s\n",
      "epoch 4  | loss: 0.44354 | val_0_unsup_loss_numpy: 0.32019999623298645|  0:02:25s\n",
      "epoch 5  | loss: 0.37832 | val_0_unsup_loss_numpy: 0.2679400146007538|  0:02:53s\n",
      "epoch 6  | loss: 0.33072 | val_0_unsup_loss_numpy: 0.24591000378131866|  0:03:22s\n",
      "epoch 7  | loss: 0.29397 | val_0_unsup_loss_numpy: 0.22915999591350555|  0:03:51s\n",
      "epoch 8  | loss: 0.27836 | val_0_unsup_loss_numpy: 0.19242000579833984|  0:04:21s\n",
      "epoch 9  | loss: 0.23881 | val_0_unsup_loss_numpy: 0.17564000189304352|  0:04:49s\n",
      "epoch 10 | loss: 0.21475 | val_0_unsup_loss_numpy: 0.16798000037670135|  0:05:18s\n",
      "epoch 11 | loss: 0.19821 | val_0_unsup_loss_numpy: 0.15062999725341797|  0:05:47s\n",
      "epoch 12 | loss: 0.18522 | val_0_unsup_loss_numpy: 0.14952999353408813|  0:06:16s\n",
      "epoch 13 | loss: 0.17664 | val_0_unsup_loss_numpy: 0.1400900036096573|  0:06:45s\n",
      "epoch 14 | loss: 0.16872 | val_0_unsup_loss_numpy: 0.13490000367164612|  0:07:14s\n",
      "epoch 15 | loss: 0.16068 | val_0_unsup_loss_numpy: 0.13728000223636627|  0:07:43s\n",
      "epoch 16 | loss: 0.15317 | val_0_unsup_loss_numpy: 0.13816000521183014|  0:08:12s\n",
      "epoch 17 | loss: 0.14803 | val_0_unsup_loss_numpy: 0.15696999430656433|  0:08:41s\n",
      "epoch 18 | loss: 0.14723 | val_0_unsup_loss_numpy: 0.13907000422477722|  0:09:10s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 14 and best_val_0_unsup_loss_numpy = 0.13490000367164612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.15824 | val_0_matthews: 0.96023 |  0:00:26s\n",
      "epoch 1  | loss: 0.04563 | val_0_matthews: 0.98131 |  0:00:51s\n",
      "epoch 2  | loss: 0.04354 | val_0_matthews: 0.98233 |  0:01:17s\n",
      "epoch 3  | loss: 0.04222 | val_0_matthews: 0.98297 |  0:01:43s\n",
      "epoch 4  | loss: 0.04104 | val_0_matthews: 0.98324 |  0:02:09s\n",
      "epoch 5  | loss: 0.04052 | val_0_matthews: 0.98277 |  0:02:35s\n",
      "epoch 6  | loss: 0.04025 | val_0_matthews: 0.9837  |  0:03:00s\n",
      "epoch 7  | loss: 0.03985 | val_0_matthews: 0.98355 |  0:03:26s\n",
      "epoch 8  | loss: 0.03975 | val_0_matthews: 0.98379 |  0:03:52s\n",
      "epoch 9  | loss: 0.03902 | val_0_matthews: 0.98387 |  0:04:18s\n",
      "epoch 10 | loss: 0.0388  | val_0_matthews: 0.98393 |  0:04:43s\n",
      "epoch 11 | loss: 0.03845 | val_0_matthews: 0.98401 |  0:05:09s\n",
      "epoch 12 | loss: 0.03829 | val_0_matthews: 0.98399 |  0:05:35s\n",
      "epoch 13 | loss: 0.03815 | val_0_matthews: 0.984   |  0:06:01s\n",
      "epoch 14 | loss: 0.03793 | val_0_matthews: 0.98397 |  0:06:26s\n",
      "epoch 15 | loss: 0.03769 | val_0_matthews: 0.98398 |  0:06:52s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 11 and best_val_0_matthews = 0.98401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 04:43:25,820] Trial 11 finished with value: 0.9840074777603149 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.024241404213293075, 'PRETRAIN_RATIO': 0.579885946256226, 'N_D': 64, 'N_STEPS': 4, 'GAMMA': 1.457306941491727, 'N_INDEPENDENT': 5, 'N_SHARED': 4, 'BATCH_SIZE': 24576, 'VIRT_BATCH_SIZE': 1024}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.8761  | val_0_unsup_loss_numpy: 0.8228899836540222|  0:00:31s\n",
      "epoch 1  | loss: 0.75145 | val_0_unsup_loss_numpy: 0.641789972782135|  0:01:02s\n",
      "epoch 2  | loss: 0.65847 | val_0_unsup_loss_numpy: 0.5242699980735779|  0:01:34s\n",
      "epoch 3  | loss: 0.59201 | val_0_unsup_loss_numpy: 0.4303700029850006|  0:02:05s\n",
      "epoch 4  | loss: 0.52774 | val_0_unsup_loss_numpy: 0.3560900092124939|  0:02:37s\n",
      "epoch 5  | loss: 0.51051 | val_0_unsup_loss_numpy: 0.3281700015068054|  0:03:08s\n",
      "epoch 6  | loss: 0.48366 | val_0_unsup_loss_numpy: 0.3601199984550476|  0:03:39s\n",
      "epoch 7  | loss: 0.46531 | val_0_unsup_loss_numpy: 0.29276999831199646|  0:04:11s\n",
      "epoch 8  | loss: 0.42137 | val_0_unsup_loss_numpy: 0.2769100069999695|  0:04:43s\n",
      "epoch 9  | loss: 0.39627 | val_0_unsup_loss_numpy: 0.26774001121520996|  0:05:14s\n",
      "epoch 10 | loss: 0.37523 | val_0_unsup_loss_numpy: 0.2589299976825714|  0:05:45s\n",
      "epoch 11 | loss: 0.35591 | val_0_unsup_loss_numpy: 0.2503199875354767|  0:06:17s\n",
      "epoch 12 | loss: 0.3618  | val_0_unsup_loss_numpy: 0.24537000060081482|  0:06:48s\n",
      "epoch 13 | loss: 0.33529 | val_0_unsup_loss_numpy: 0.2287299931049347|  0:07:20s\n",
      "epoch 14 | loss: 0.31555 | val_0_unsup_loss_numpy: 0.2195499986410141|  0:07:51s\n",
      "epoch 15 | loss: 0.30168 | val_0_unsup_loss_numpy: 0.21067999303340912|  0:08:23s\n",
      "epoch 16 | loss: 0.2893  | val_0_unsup_loss_numpy: 0.20855000615119934|  0:08:54s\n",
      "epoch 17 | loss: 0.27924 | val_0_unsup_loss_numpy: 0.2014400064945221|  0:09:25s\n",
      "epoch 18 | loss: 0.27206 | val_0_unsup_loss_numpy: 0.19809000194072723|  0:09:57s\n",
      "epoch 19 | loss: 0.2592  | val_0_unsup_loss_numpy: 0.19964000582695007|  0:10:29s\n",
      "epoch 20 | loss: 0.25205 | val_0_unsup_loss_numpy: 0.18574999272823334|  0:11:00s\n",
      "epoch 21 | loss: 0.2403  | val_0_unsup_loss_numpy: 0.18601000308990479|  0:11:31s\n",
      "epoch 22 | loss: 0.23235 | val_0_unsup_loss_numpy: 0.1841599941253662|  0:12:03s\n",
      "epoch 23 | loss: 0.23257 | val_0_unsup_loss_numpy: 0.1863500028848648|  0:12:34s\n",
      "epoch 24 | loss: 0.2302  | val_0_unsup_loss_numpy: 0.18286000192165375|  0:13:06s\n",
      "epoch 25 | loss: 0.22104 | val_0_unsup_loss_numpy: 0.1862799972295761|  0:13:37s\n",
      "epoch 26 | loss: 0.21807 | val_0_unsup_loss_numpy: 0.1840900033712387|  0:14:09s\n",
      "epoch 27 | loss: 0.21536 | val_0_unsup_loss_numpy: 0.18574999272823334|  0:14:40s\n",
      "epoch 28 | loss: 0.22233 | val_0_unsup_loss_numpy: 0.1886100023984909|  0:15:11s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 24 and best_val_0_unsup_loss_numpy = 0.18286000192165375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.1315  | val_0_matthews: 0.97813 |  0:00:27s\n",
      "epoch 1  | loss: 0.04451 | val_0_matthews: 0.98203 |  0:00:55s\n",
      "epoch 2  | loss: 0.04188 | val_0_matthews: 0.98304 |  0:01:22s\n",
      "epoch 3  | loss: 0.04056 | val_0_matthews: 0.9831  |  0:01:49s\n",
      "epoch 4  | loss: 0.04025 | val_0_matthews: 0.98369 |  0:02:17s\n",
      "epoch 5  | loss: 0.03975 | val_0_matthews: 0.98381 |  0:02:45s\n",
      "epoch 6  | loss: 0.03954 | val_0_matthews: 0.98333 |  0:03:13s\n",
      "epoch 7  | loss: 0.04003 | val_0_matthews: 0.98358 |  0:03:40s\n",
      "epoch 8  | loss: 0.04298 | val_0_matthews: 0.98372 |  0:04:07s\n",
      "epoch 9  | loss: 0.03842 | val_0_matthews: 0.98404 |  0:04:35s\n",
      "epoch 10 | loss: 0.0377  | val_0_matthews: 0.98429 |  0:05:03s\n",
      "epoch 11 | loss: 0.03743 | val_0_matthews: 0.98407 |  0:05:30s\n",
      "epoch 12 | loss: 0.03975 | val_0_matthews: 0.96563 |  0:05:57s\n",
      "epoch 13 | loss: 0.04363 | val_0_matthews: 0.98352 |  0:06:25s\n",
      "epoch 14 | loss: 0.04073 | val_0_matthews: 0.98384 |  0:06:53s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 10 and best_val_0_matthews = 0.98429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 05:14:16,031] Trial 12 finished with value: 0.9842938780784607 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.025968805038611074, 'PRETRAIN_RATIO': 0.38234469186296965, 'N_D': 64, 'N_STEPS': 5, 'GAMMA': 1.481263370029012, 'N_INDEPENDENT': 4, 'N_SHARED': 4, 'BATCH_SIZE': 24576, 'VIRT_BATCH_SIZE': 1024}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.02853 | val_0_unsup_loss_numpy: 0.7715399861335754|  0:00:28s\n",
      "epoch 1  | loss: 0.72774 | val_0_unsup_loss_numpy: 0.5714600086212158|  0:00:57s\n",
      "epoch 2  | loss: 0.57478 | val_0_unsup_loss_numpy: 0.4307200014591217|  0:01:26s\n",
      "epoch 3  | loss: 0.47881 | val_0_unsup_loss_numpy: 0.3632600009441376|  0:01:56s\n",
      "epoch 4  | loss: 0.40961 | val_0_unsup_loss_numpy: 0.32210999727249146|  0:02:25s\n",
      "epoch 5  | loss: 0.35436 | val_0_unsup_loss_numpy: 0.30619001388549805|  0:02:55s\n",
      "epoch 6  | loss: 0.31414 | val_0_unsup_loss_numpy: 0.33831000328063965|  0:03:24s\n",
      "epoch 7  | loss: 0.28149 | val_0_unsup_loss_numpy: 0.36531999707221985|  0:03:53s\n",
      "epoch 8  | loss: 0.25087 | val_0_unsup_loss_numpy: 0.40062999725341797|  0:04:23s\n",
      "epoch 9  | loss: 0.22842 | val_0_unsup_loss_numpy: 0.4408699870109558|  0:04:52s\n",
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 5 and best_val_0_unsup_loss_numpy = 0.30619001388549805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.0851  | val_0_matthews: 0.98121 |  0:00:26s\n",
      "epoch 1  | loss: 0.0438  | val_0_matthews: 0.98307 |  0:00:52s\n",
      "epoch 2  | loss: 0.04199 | val_0_matthews: 0.9834  |  0:01:18s\n",
      "epoch 3  | loss: 0.04124 | val_0_matthews: 0.98356 |  0:01:43s\n",
      "epoch 4  | loss: 0.04073 | val_0_matthews: 0.98368 |  0:02:09s\n",
      "epoch 5  | loss: 0.04007 | val_0_matthews: 0.98408 |  0:02:35s\n",
      "epoch 6  | loss: 0.03956 | val_0_matthews: 0.98388 |  0:03:00s\n",
      "epoch 7  | loss: 0.03929 | val_0_matthews: 0.98402 |  0:03:26s\n",
      "epoch 8  | loss: 0.03896 | val_0_matthews: 0.98411 |  0:03:53s\n",
      "epoch 9  | loss: 0.03872 | val_0_matthews: 0.98402 |  0:04:19s\n",
      "epoch 10 | loss: 0.03847 | val_0_matthews: 0.984   |  0:04:45s\n",
      "epoch 11 | loss: 0.03824 | val_0_matthews: 0.98422 |  0:05:11s\n",
      "epoch 12 | loss: 0.03796 | val_0_matthews: 0.9841  |  0:05:36s\n",
      "epoch 13 | loss: 0.03778 | val_0_matthews: 0.98394 |  0:06:02s\n",
      "epoch 14 | loss: 0.03757 | val_0_matthews: 0.98413 |  0:06:28s\n",
      "epoch 15 | loss: 0.03737 | val_0_matthews: 0.98394 |  0:06:54s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 11 and best_val_0_matthews = 0.98422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 05:34:12,962] Trial 13 finished with value: 0.9842190146446228 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.005663005549620107, 'PRETRAIN_RATIO': 0.4145877830917317, 'N_D': 64, 'N_STEPS': 4, 'GAMMA': 1.314498325935449, 'N_INDEPENDENT': 5, 'N_SHARED': 3, 'BATCH_SIZE': 16384, 'VIRT_BATCH_SIZE': 1024}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.86406 | val_0_unsup_loss_numpy: 0.7875900268554688|  0:00:22s\n",
      "epoch 1  | loss: 0.61296 | val_0_unsup_loss_numpy: 0.547569990158081|  0:00:46s\n",
      "epoch 2  | loss: 0.48765 | val_0_unsup_loss_numpy: 0.39010998606681824|  0:01:09s\n",
      "epoch 3  | loss: 0.41894 | val_0_unsup_loss_numpy: 0.32256999611854553|  0:01:32s\n",
      "epoch 4  | loss: 0.38644 | val_0_unsup_loss_numpy: 0.2819499969482422|  0:01:56s\n",
      "epoch 5  | loss: 0.33843 | val_0_unsup_loss_numpy: 0.2743400037288666|  0:02:19s\n",
      "epoch 6  | loss: 0.31698 | val_0_unsup_loss_numpy: 0.27160000801086426|  0:02:43s\n",
      "epoch 7  | loss: 0.28844 | val_0_unsup_loss_numpy: 0.2817099988460541|  0:03:06s\n",
      "epoch 8  | loss: 0.27667 | val_0_unsup_loss_numpy: 0.28543999791145325|  0:03:30s\n",
      "epoch 9  | loss: 0.2514  | val_0_unsup_loss_numpy: 0.31718000769615173|  0:03:53s\n",
      "epoch 10 | loss: 0.2515  | val_0_unsup_loss_numpy: 0.31922000646591187|  0:04:17s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 6 and best_val_0_unsup_loss_numpy = 0.27160000801086426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.18932 | val_0_matthews: 0.97031 |  0:00:20s\n",
      "epoch 1  | loss: 0.05311 | val_0_matthews: 0.97771 |  0:00:41s\n",
      "epoch 2  | loss: 0.04784 | val_0_matthews: 0.97986 |  0:01:02s\n",
      "epoch 3  | loss: 0.04563 | val_0_matthews: 0.98031 |  0:01:23s\n",
      "epoch 4  | loss: 0.04418 | val_0_matthews: 0.98172 |  0:01:44s\n",
      "epoch 5  | loss: 0.04254 | val_0_matthews: 0.98247 |  0:02:05s\n",
      "epoch 6  | loss: 0.04158 | val_0_matthews: 0.98285 |  0:02:26s\n",
      "epoch 7  | loss: 0.04098 | val_0_matthews: 0.98311 |  0:02:46s\n",
      "epoch 8  | loss: 0.04041 | val_0_matthews: 0.98303 |  0:03:07s\n",
      "epoch 9  | loss: 0.04005 | val_0_matthews: 0.98327 |  0:03:27s\n",
      "epoch 10 | loss: 0.03961 | val_0_matthews: 0.9832  |  0:03:48s\n",
      "epoch 11 | loss: 0.03984 | val_0_matthews: 0.98333 |  0:04:09s\n",
      "epoch 12 | loss: 0.03921 | val_0_matthews: 0.98376 |  0:04:29s\n",
      "epoch 13 | loss: 0.03885 | val_0_matthews: 0.98387 |  0:04:50s\n",
      "epoch 14 | loss: 0.03855 | val_0_matthews: 0.98392 |  0:05:11s\n",
      "epoch 15 | loss: 0.03828 | val_0_matthews: 0.98361 |  0:05:32s\n",
      "epoch 16 | loss: 0.03819 | val_0_matthews: 0.98378 |  0:05:53s\n",
      "epoch 17 | loss: 0.03784 | val_0_matthews: 0.9836  |  0:06:14s\n",
      "epoch 18 | loss: 0.03838 | val_0_matthews: 0.98359 |  0:06:35s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 14 and best_val_0_matthews = 0.98392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 05:52:32,553] Trial 14 finished with value: 0.9839200377464294 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.03020337936732595, 'PRETRAIN_RATIO': 0.25579773143272894, 'N_D': 48, 'N_STEPS': 5, 'GAMMA': 1.1254722608882584, 'N_INDEPENDENT': 4, 'N_SHARED': 5, 'BATCH_SIZE': 28672, 'VIRT_BATCH_SIZE': 2048}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.98079 | val_0_unsup_loss_numpy: 0.994949996471405|  0:00:39s\n",
      "epoch 1  | loss: 0.9215  | val_0_unsup_loss_numpy: 0.9115099906921387|  0:01:18s\n",
      "epoch 2  | loss: 0.8723  | val_0_unsup_loss_numpy: 0.7656000256538391|  0:01:57s\n",
      "epoch 3  | loss: 0.82822 | val_0_unsup_loss_numpy: 0.6930199861526489|  0:02:36s\n",
      "epoch 4  | loss: 0.78736 | val_0_unsup_loss_numpy: 0.6645399928092957|  0:03:15s\n",
      "epoch 5  | loss: 0.75671 | val_0_unsup_loss_numpy: 0.6672000288963318|  0:03:54s\n",
      "epoch 6  | loss: 0.7307  | val_0_unsup_loss_numpy: 0.6423100233078003|  0:04:33s\n",
      "epoch 7  | loss: 0.71351 | val_0_unsup_loss_numpy: 0.6371099948883057|  0:05:12s\n",
      "epoch 8  | loss: 0.6991  | val_0_unsup_loss_numpy: 0.626579999923706|  0:05:51s\n",
      "epoch 9  | loss: 0.68629 | val_0_unsup_loss_numpy: 0.6345199942588806|  0:06:30s\n",
      "epoch 10 | loss: 0.67342 | val_0_unsup_loss_numpy: 0.6355699896812439|  0:07:09s\n",
      "epoch 11 | loss: 0.66514 | val_0_unsup_loss_numpy: 0.6053500175476074|  0:07:49s\n",
      "epoch 12 | loss: 0.6573  | val_0_unsup_loss_numpy: 0.6363000273704529|  0:08:28s\n",
      "epoch 13 | loss: 0.64699 | val_0_unsup_loss_numpy: 0.6140900254249573|  0:09:07s\n",
      "epoch 14 | loss: 0.64059 | val_0_unsup_loss_numpy: 0.6037899851799011|  0:09:46s\n",
      "epoch 15 | loss: 0.63331 | val_0_unsup_loss_numpy: 0.6215900182723999|  0:10:25s\n",
      "epoch 16 | loss: 0.62449 | val_0_unsup_loss_numpy: 0.6087499856948853|  0:11:04s\n",
      "epoch 17 | loss: 0.61857 | val_0_unsup_loss_numpy: 0.6013799905776978|  0:11:43s\n",
      "epoch 18 | loss: 0.61484 | val_0_unsup_loss_numpy: 0.6205899715423584|  0:12:23s\n",
      "epoch 19 | loss: 0.60867 | val_0_unsup_loss_numpy: 0.6224600076675415|  0:13:01s\n",
      "epoch 20 | loss: 0.60424 | val_0_unsup_loss_numpy: 0.6234599947929382|  0:13:41s\n",
      "epoch 21 | loss: 0.59989 | val_0_unsup_loss_numpy: 0.6271399855613708|  0:14:19s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 17 and best_val_0_unsup_loss_numpy = 0.6013799905776978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.11366 | val_0_matthews: 0.96235 |  0:00:32s\n",
      "epoch 1  | loss: 0.06115 | val_0_matthews: 0.97373 |  0:01:04s\n",
      "epoch 2  | loss: 0.05028 | val_0_matthews: 0.98025 |  0:01:37s\n",
      "epoch 3  | loss: 0.04563 | val_0_matthews: 0.98131 |  0:02:10s\n",
      "epoch 4  | loss: 0.04733 | val_0_matthews: 0.97999 |  0:02:42s\n",
      "epoch 5  | loss: 0.04495 | val_0_matthews: 0.982   |  0:03:14s\n",
      "epoch 6  | loss: 0.04266 | val_0_matthews: 0.98245 |  0:03:47s\n",
      "epoch 7  | loss: 0.04193 | val_0_matthews: 0.98275 |  0:04:19s\n",
      "epoch 8  | loss: 0.04149 | val_0_matthews: 0.98282 |  0:04:51s\n",
      "epoch 9  | loss: 0.04103 | val_0_matthews: 0.98292 |  0:05:24s\n",
      "epoch 10 | loss: 0.0406  | val_0_matthews: 0.98332 |  0:05:57s\n",
      "epoch 11 | loss: 0.04026 | val_0_matthews: 0.98348 |  0:06:29s\n",
      "epoch 12 | loss: 0.03991 | val_0_matthews: 0.98359 |  0:07:01s\n",
      "epoch 13 | loss: 0.03968 | val_0_matthews: 0.98378 |  0:07:34s\n",
      "epoch 14 | loss: 0.03953 | val_0_matthews: 0.98366 |  0:08:06s\n",
      "epoch 15 | loss: 0.03934 | val_0_matthews: 0.98359 |  0:08:38s\n",
      "epoch 16 | loss: 0.03924 | val_0_matthews: 0.9838  |  0:09:11s\n",
      "epoch 17 | loss: 0.03903 | val_0_matthews: 0.98399 |  0:09:43s\n",
      "epoch 18 | loss: 0.03906 | val_0_matthews: 0.984   |  0:10:16s\n",
      "epoch 19 | loss: 0.0387  | val_0_matthews: 0.98396 |  0:10:48s\n",
      "epoch 20 | loss: 0.03859 | val_0_matthews: 0.98379 |  0:11:20s\n",
      "epoch 21 | loss: 0.0386  | val_0_matthews: 0.98401 |  0:11:53s\n",
      "epoch 22 | loss: 0.03842 | val_0_matthews: 0.98392 |  0:12:25s\n",
      "epoch 23 | loss: 0.03818 | val_0_matthews: 0.98396 |  0:12:58s\n",
      "epoch 24 | loss: 0.03826 | val_0_matthews: 0.98397 |  0:13:30s\n",
      "epoch 25 | loss: 0.03792 | val_0_matthews: 0.98394 |  0:14:03s\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 21 and best_val_0_matthews = 0.98401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 06:29:13,342] Trial 15 finished with value: 0.9840121269226074 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.016702256181798356, 'PRETRAIN_RATIO': 0.7999868571697746, 'N_D': 32, 'N_STEPS': 4, 'GAMMA': 1.64827666817602, 'N_INDEPENDENT': 3, 'N_SHARED': 3, 'BATCH_SIZE': 16384, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 9.23465 | val_0_unsup_loss_numpy: 0.8072999715805054|  0:00:29s\n",
      "epoch 1  | loss: 0.71453 | val_0_unsup_loss_numpy: 0.542930006980896|  0:00:59s\n",
      "epoch 2  | loss: 0.53268 | val_0_unsup_loss_numpy: 0.39699000120162964|  0:01:29s\n",
      "epoch 3  | loss: 0.42271 | val_0_unsup_loss_numpy: 0.29596999287605286|  0:01:59s\n",
      "epoch 4  | loss: 0.35228 | val_0_unsup_loss_numpy: 0.26221001148223877|  0:02:29s\n",
      "epoch 5  | loss: 0.3033  | val_0_unsup_loss_numpy: 0.22846999764442444|  0:02:59s\n",
      "epoch 6  | loss: 0.2632  | val_0_unsup_loss_numpy: 0.20924000442028046|  0:03:29s\n",
      "epoch 7  | loss: 0.23299 | val_0_unsup_loss_numpy: 0.1987600028514862|  0:03:59s\n",
      "epoch 8  | loss: 0.20974 | val_0_unsup_loss_numpy: 0.19131000339984894|  0:04:29s\n",
      "epoch 9  | loss: 0.19598 | val_0_unsup_loss_numpy: 0.18630999326705933|  0:04:59s\n",
      "epoch 10 | loss: 0.17608 | val_0_unsup_loss_numpy: 0.18352000415325165|  0:05:29s\n",
      "epoch 11 | loss: 0.17861 | val_0_unsup_loss_numpy: 0.19267000257968903|  0:05:59s\n",
      "epoch 12 | loss: 0.16519 | val_0_unsup_loss_numpy: 0.17816999554634094|  0:06:29s\n",
      "epoch 13 | loss: 0.1462  | val_0_unsup_loss_numpy: 0.17949999868869781|  0:06:58s\n",
      "epoch 14 | loss: 0.13579 | val_0_unsup_loss_numpy: 0.18042999505996704|  0:07:28s\n",
      "epoch 15 | loss: 0.12847 | val_0_unsup_loss_numpy: 0.18490999937057495|  0:07:59s\n",
      "epoch 16 | loss: 0.12475 | val_0_unsup_loss_numpy: 0.18634000420570374|  0:08:28s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 12 and best_val_0_unsup_loss_numpy = 0.17816999554634094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.09371 | val_0_matthews: 0.98021 |  0:00:26s\n",
      "epoch 1  | loss: 0.04438 | val_0_matthews: 0.98243 |  0:00:53s\n",
      "epoch 2  | loss: 0.0426  | val_0_matthews: 0.98295 |  0:01:19s\n",
      "epoch 3  | loss: 0.04126 | val_0_matthews: 0.98362 |  0:01:46s\n",
      "epoch 4  | loss: 0.04069 | val_0_matthews: 0.98339 |  0:02:12s\n",
      "epoch 5  | loss: 0.04007 | val_0_matthews: 0.98396 |  0:02:39s\n",
      "epoch 6  | loss: 0.03975 | val_0_matthews: 0.98397 |  0:03:05s\n",
      "epoch 7  | loss: 0.03941 | val_0_matthews: 0.984   |  0:03:31s\n",
      "epoch 8  | loss: 0.03908 | val_0_matthews: 0.98401 |  0:03:58s\n",
      "epoch 9  | loss: 0.03902 | val_0_matthews: 0.98401 |  0:04:25s\n",
      "epoch 10 | loss: 0.03905 | val_0_matthews: 0.98396 |  0:04:51s\n",
      "epoch 11 | loss: 0.03866 | val_0_matthews: 0.98422 |  0:05:18s\n",
      "epoch 12 | loss: 0.03823 | val_0_matthews: 0.98423 |  0:05:44s\n",
      "epoch 13 | loss: 0.03809 | val_0_matthews: 0.98404 |  0:06:11s\n",
      "epoch 14 | loss: 0.03807 | val_0_matthews: 0.98427 |  0:06:37s\n",
      "epoch 15 | loss: 0.03794 | val_0_matthews: 0.98421 |  0:07:03s\n",
      "epoch 16 | loss: 0.03768 | val_0_matthews: 0.98429 |  0:07:30s\n",
      "epoch 17 | loss: 0.03739 | val_0_matthews: 0.98438 |  0:07:56s\n",
      "epoch 18 | loss: 0.03724 | val_0_matthews: 0.9841  |  0:08:22s\n",
      "epoch 19 | loss: 0.03695 | val_0_matthews: 0.98434 |  0:08:49s\n",
      "epoch 20 | loss: 0.03677 | val_0_matthews: 0.9841  |  0:09:16s\n",
      "epoch 21 | loss: 0.0366  | val_0_matthews: 0.98423 |  0:09:42s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 17 and best_val_0_matthews = 0.98438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 06:56:52,872] Trial 16 finished with value: 0.9843766093254089 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.006194136925369498, 'PRETRAIN_RATIO': 0.33362877660988904, 'N_D': 64, 'N_STEPS': 6, 'GAMMA': 1.3807046932303289, 'N_INDEPENDENT': 5, 'N_SHARED': 4, 'BATCH_SIZE': 24576, 'VIRT_BATCH_SIZE': 1536}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 8.00532 | val_0_unsup_loss_numpy: 0.862309992313385|  0:00:29s\n",
      "epoch 1  | loss: 0.78945 | val_0_unsup_loss_numpy: 0.6297000050544739|  0:00:58s\n",
      "epoch 2  | loss: 0.67418 | val_0_unsup_loss_numpy: 0.521340012550354|  0:01:27s\n",
      "epoch 3  | loss: 0.59338 | val_0_unsup_loss_numpy: 0.44604000449180603|  0:01:56s\n",
      "epoch 4  | loss: 0.5322  | val_0_unsup_loss_numpy: 0.3994300067424774|  0:02:25s\n",
      "epoch 5  | loss: 0.48579 | val_0_unsup_loss_numpy: 0.37817999720573425|  0:02:55s\n",
      "epoch 6  | loss: 0.44691 | val_0_unsup_loss_numpy: 0.3656199872493744|  0:03:24s\n",
      "epoch 7  | loss: 0.4157  | val_0_unsup_loss_numpy: 0.35229000449180603|  0:03:53s\n",
      "epoch 8  | loss: 0.38778 | val_0_unsup_loss_numpy: 0.3381800055503845|  0:04:22s\n",
      "epoch 9  | loss: 0.36287 | val_0_unsup_loss_numpy: 0.3262600004673004|  0:04:52s\n",
      "epoch 10 | loss: 0.34287 | val_0_unsup_loss_numpy: 0.31325000524520874|  0:05:21s\n",
      "epoch 11 | loss: 0.3248  | val_0_unsup_loss_numpy: 0.30542001128196716|  0:05:50s\n",
      "epoch 12 | loss: 0.30982 | val_0_unsup_loss_numpy: 0.29976001381874084|  0:06:19s\n",
      "epoch 13 | loss: 0.29655 | val_0_unsup_loss_numpy: 0.29787999391555786|  0:06:48s\n",
      "epoch 14 | loss: 0.28316 | val_0_unsup_loss_numpy: 0.29517000913619995|  0:07:18s\n",
      "epoch 15 | loss: 0.27148 | val_0_unsup_loss_numpy: 0.29308000206947327|  0:07:47s\n",
      "epoch 16 | loss: 0.26143 | val_0_unsup_loss_numpy: 0.29572001099586487|  0:08:16s\n",
      "epoch 17 | loss: 0.25215 | val_0_unsup_loss_numpy: 0.29771000146865845|  0:08:45s\n",
      "epoch 18 | loss: 0.24454 | val_0_unsup_loss_numpy: 0.302590012550354|  0:09:14s\n",
      "epoch 19 | loss: 0.23719 | val_0_unsup_loss_numpy: 0.2996799945831299|  0:09:43s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 15 and best_val_0_unsup_loss_numpy = 0.29308000206947327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.11895 | val_0_matthews: 0.97862 |  0:00:25s\n",
      "epoch 1  | loss: 0.0469  | val_0_matthews: 0.98021 |  0:00:50s\n",
      "epoch 2  | loss: 0.04411 | val_0_matthews: 0.98171 |  0:01:15s\n",
      "epoch 3  | loss: 0.04297 | val_0_matthews: 0.98218 |  0:01:40s\n",
      "epoch 4  | loss: 0.04317 | val_0_matthews: 0.98128 |  0:02:05s\n",
      "epoch 5  | loss: 0.04294 | val_0_matthews: 0.98244 |  0:02:30s\n",
      "epoch 6  | loss: 0.04166 | val_0_matthews: 0.98272 |  0:02:55s\n",
      "epoch 7  | loss: 0.04096 | val_0_matthews: 0.98295 |  0:03:20s\n",
      "epoch 8  | loss: 0.04041 | val_0_matthews: 0.98323 |  0:03:45s\n",
      "epoch 9  | loss: 0.04021 | val_0_matthews: 0.98341 |  0:04:11s\n",
      "epoch 10 | loss: 0.03986 | val_0_matthews: 0.98336 |  0:04:36s\n",
      "epoch 11 | loss: 0.03973 | val_0_matthews: 0.98335 |  0:05:01s\n",
      "epoch 12 | loss: 0.03954 | val_0_matthews: 0.98358 |  0:05:25s\n",
      "epoch 13 | loss: 0.03926 | val_0_matthews: 0.98359 |  0:05:50s\n",
      "epoch 14 | loss: 0.0391  | val_0_matthews: 0.98374 |  0:06:15s\n",
      "epoch 15 | loss: 0.03894 | val_0_matthews: 0.98376 |  0:06:41s\n",
      "epoch 16 | loss: 0.03875 | val_0_matthews: 0.98364 |  0:07:06s\n",
      "epoch 17 | loss: 0.03904 | val_0_matthews: 0.98366 |  0:07:31s\n",
      "epoch 18 | loss: 0.03881 | val_0_matthews: 0.98375 |  0:07:56s\n",
      "epoch 19 | loss: 0.03856 | val_0_matthews: 0.98383 |  0:08:21s\n",
      "epoch 20 | loss: 0.03835 | val_0_matthews: 0.98375 |  0:08:46s\n",
      "epoch 21 | loss: 0.03812 | val_0_matthews: 0.98381 |  0:09:11s\n",
      "epoch 22 | loss: 0.03801 | val_0_matthews: 0.98381 |  0:09:36s\n",
      "epoch 23 | loss: 0.03812 | val_0_matthews: 0.98394 |  0:10:02s\n",
      "epoch 24 | loss: 0.03796 | val_0_matthews: 0.98401 |  0:10:27s\n",
      "epoch 25 | loss: 0.03774 | val_0_matthews: 0.98398 |  0:10:52s\n",
      "epoch 26 | loss: 0.03757 | val_0_matthews: 0.98386 |  0:11:17s\n",
      "epoch 27 | loss: 0.03747 | val_0_matthews: 0.98391 |  0:11:42s\n",
      "epoch 28 | loss: 0.03732 | val_0_matthews: 0.98399 |  0:12:07s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 24 and best_val_0_matthews = 0.98401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 07:32:49,789] Trial 17 finished with value: 0.9840087890625 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.003180533161346018, 'PRETRAIN_RATIO': 0.3297562337226995, 'N_D': 48, 'N_STEPS': 6, 'GAMMA': 1.0972649445534441, 'N_INDEPENDENT': 5, 'N_SHARED': 2, 'BATCH_SIZE': 16384, 'VIRT_BATCH_SIZE': 1536}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 11.62206| val_0_unsup_loss_numpy: 0.9739400148391724|  0:00:30s\n",
      "epoch 1  | loss: 0.92402 | val_0_unsup_loss_numpy: 0.8657600283622742|  0:01:00s\n",
      "epoch 2  | loss: 0.85179 | val_0_unsup_loss_numpy: 0.8179000020027161|  0:01:29s\n",
      "epoch 3  | loss: 0.79086 | val_0_unsup_loss_numpy: 0.7864099740982056|  0:02:00s\n",
      "epoch 4  | loss: 0.73529 | val_0_unsup_loss_numpy: 0.6825000047683716|  0:02:30s\n",
      "epoch 5  | loss: 0.68799 | val_0_unsup_loss_numpy: 0.6933900117874146|  0:03:00s\n",
      "epoch 6  | loss: 0.64596 | val_0_unsup_loss_numpy: 0.6715099811553955|  0:03:30s\n",
      "epoch 7  | loss: 0.61394 | val_0_unsup_loss_numpy: 0.5893300175666809|  0:04:00s\n",
      "epoch 8  | loss: 0.5861  | val_0_unsup_loss_numpy: 0.5406100153923035|  0:04:30s\n",
      "epoch 9  | loss: 0.56593 | val_0_unsup_loss_numpy: 0.5891100168228149|  0:05:00s\n",
      "epoch 10 | loss: 0.54475 | val_0_unsup_loss_numpy: 0.5513899922370911|  0:05:30s\n",
      "epoch 11 | loss: 0.53533 | val_0_unsup_loss_numpy: 0.5340499877929688|  0:06:01s\n",
      "epoch 12 | loss: 0.51769 | val_0_unsup_loss_numpy: 0.4567599892616272|  0:06:31s\n",
      "epoch 13 | loss: 0.50249 | val_0_unsup_loss_numpy: 0.4376400113105774|  0:07:01s\n",
      "epoch 14 | loss: 0.49122 | val_0_unsup_loss_numpy: 0.448060005903244|  0:07:30s\n",
      "epoch 15 | loss: 0.47922 | val_0_unsup_loss_numpy: 0.45511001348495483|  0:08:01s\n",
      "epoch 16 | loss: 0.47055 | val_0_unsup_loss_numpy: 0.4276899993419647|  0:08:31s\n",
      "epoch 17 | loss: 0.45652 | val_0_unsup_loss_numpy: 0.4285599887371063|  0:09:00s\n",
      "epoch 18 | loss: 0.44678 | val_0_unsup_loss_numpy: 0.3990199863910675|  0:09:30s\n",
      "epoch 19 | loss: 0.44282 | val_0_unsup_loss_numpy: 0.40696999430656433|  0:10:01s\n",
      "epoch 20 | loss: 0.43251 | val_0_unsup_loss_numpy: 0.4048599898815155|  0:10:30s\n",
      "epoch 21 | loss: 0.41806 | val_0_unsup_loss_numpy: 0.3875400125980377|  0:11:00s\n",
      "epoch 22 | loss: 0.40799 | val_0_unsup_loss_numpy: 0.365090012550354|  0:11:30s\n",
      "epoch 23 | loss: 0.40039 | val_0_unsup_loss_numpy: 0.38067999482154846|  0:12:00s\n",
      "epoch 24 | loss: 0.39404 | val_0_unsup_loss_numpy: 0.3595399856567383|  0:12:30s\n",
      "epoch 25 | loss: 0.38392 | val_0_unsup_loss_numpy: 0.35888999700546265|  0:13:00s\n",
      "epoch 26 | loss: 0.37936 | val_0_unsup_loss_numpy: 0.3678100109100342|  0:13:30s\n",
      "epoch 27 | loss: 0.37496 | val_0_unsup_loss_numpy: 0.3951900005340576|  0:13:59s\n",
      "epoch 28 | loss: 0.36668 | val_0_unsup_loss_numpy: 0.33449000120162964|  0:14:30s\n",
      "epoch 29 | loss: 0.36096 | val_0_unsup_loss_numpy: 0.3312099874019623|  0:14:59s\n",
      "epoch 30 | loss: 0.35755 | val_0_unsup_loss_numpy: 0.2787100076675415|  0:15:29s\n",
      "epoch 31 | loss: 0.35315 | val_0_unsup_loss_numpy: 0.284280002117157|  0:15:59s\n",
      "Stop training because you reached max_epochs = 32 with best_epoch = 30 and best_val_0_unsup_loss_numpy = 0.2787100076675415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.15094 | val_0_matthews: 0.96653 |  0:00:25s\n",
      "epoch 1  | loss: 0.05742 | val_0_matthews: 0.97486 |  0:00:51s\n",
      "epoch 2  | loss: 0.05723 | val_0_matthews: 0.97328 |  0:01:18s\n",
      "epoch 3  | loss: 0.05805 | val_0_matthews: 0.97516 |  0:01:44s\n",
      "epoch 4  | loss: 0.05528 | val_0_matthews: 0.97681 |  0:02:10s\n",
      "epoch 5  | loss: 0.05172 | val_0_matthews: 0.97734 |  0:02:37s\n",
      "epoch 6  | loss: 0.04934 | val_0_matthews: 0.97832 |  0:03:03s\n",
      "epoch 7  | loss: 0.04788 | val_0_matthews: 0.97907 |  0:03:29s\n",
      "epoch 8  | loss: 0.0465  | val_0_matthews: 0.9795  |  0:03:55s\n",
      "epoch 9  | loss: 0.04642 | val_0_matthews: 0.97938 |  0:04:21s\n",
      "epoch 10 | loss: 0.04631 | val_0_matthews: 0.98015 |  0:04:47s\n",
      "epoch 11 | loss: 0.04465 | val_0_matthews: 0.97931 |  0:05:13s\n",
      "epoch 12 | loss: 0.04449 | val_0_matthews: 0.98103 |  0:05:39s\n",
      "epoch 13 | loss: 0.04326 | val_0_matthews: 0.98123 |  0:06:05s\n",
      "epoch 14 | loss: 0.04403 | val_0_matthews: 0.97932 |  0:06:32s\n",
      "epoch 15 | loss: 0.04529 | val_0_matthews: 0.98112 |  0:06:58s\n",
      "epoch 16 | loss: 0.04292 | val_0_matthews: 0.98161 |  0:07:23s\n",
      "epoch 17 | loss: 0.04207 | val_0_matthews: 0.98203 |  0:07:49s\n",
      "epoch 18 | loss: 0.04425 | val_0_matthews: 0.98034 |  0:08:15s\n",
      "epoch 19 | loss: 0.04382 | val_0_matthews: 0.98171 |  0:08:41s\n",
      "epoch 20 | loss: 0.04324 | val_0_matthews: 0.98084 |  0:09:07s\n",
      "epoch 21 | loss: 0.04377 | val_0_matthews: 0.98191 |  0:09:35s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 17 and best_val_0_matthews = 0.98203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 08:08:26,271] Trial 18 finished with value: 0.9820302128791809 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.002587392612860012, 'PRETRAIN_RATIO': 0.35164311333848935, 'N_D': 64, 'N_STEPS': 6, 'GAMMA': 1.3476519687005202, 'N_INDEPENDENT': 5, 'N_SHARED': 3, 'BATCH_SIZE': 20480, 'VIRT_BATCH_SIZE': 1536}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.50048 | val_0_unsup_loss_numpy: 0.5698999762535095|  0:00:32s\n",
      "epoch 1  | loss: 0.49871 | val_0_unsup_loss_numpy: 0.4354200065135956|  0:01:04s\n",
      "epoch 2  | loss: 0.37099 | val_0_unsup_loss_numpy: 0.38791000843048096|  0:01:37s\n",
      "epoch 3  | loss: 0.30307 | val_0_unsup_loss_numpy: 0.35683000087738037|  0:02:09s\n",
      "epoch 4  | loss: 0.2547  | val_0_unsup_loss_numpy: 0.3292900025844574|  0:02:41s\n",
      "epoch 5  | loss: 0.22038 | val_0_unsup_loss_numpy: 0.31845998764038086|  0:03:14s\n",
      "epoch 6  | loss: 0.19443 | val_0_unsup_loss_numpy: 0.31453999876976013|  0:03:46s\n",
      "epoch 7  | loss: 0.17399 | val_0_unsup_loss_numpy: 0.3158999979496002|  0:04:19s\n",
      "epoch 8  | loss: 0.15836 | val_0_unsup_loss_numpy: 0.31641000509262085|  0:04:51s\n",
      "epoch 9  | loss: 0.14737 | val_0_unsup_loss_numpy: 0.32106998562812805|  0:05:24s\n",
      "epoch 10 | loss: 0.13627 | val_0_unsup_loss_numpy: 0.3256399929523468|  0:05:57s\n",
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 6 and best_val_0_unsup_loss_numpy = 0.31453999876976013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.09526 | val_0_matthews: 0.98048 |  0:00:28s\n",
      "epoch 1  | loss: 0.04516 | val_0_matthews: 0.9824  |  0:00:56s\n",
      "epoch 2  | loss: 0.04281 | val_0_matthews: 0.98312 |  0:01:25s\n",
      "epoch 3  | loss: 0.04304 | val_0_matthews: 0.98216 |  0:01:53s\n",
      "epoch 4  | loss: 0.04227 | val_0_matthews: 0.98335 |  0:02:21s\n",
      "epoch 5  | loss: 0.04132 | val_0_matthews: 0.98285 |  0:02:49s\n",
      "epoch 6  | loss: 0.04096 | val_0_matthews: 0.98382 |  0:03:19s\n",
      "epoch 7  | loss: 0.04048 | val_0_matthews: 0.98375 |  0:03:48s\n",
      "epoch 8  | loss: 0.03981 | val_0_matthews: 0.98386 |  0:04:16s\n",
      "epoch 9  | loss: 0.0395  | val_0_matthews: 0.98405 |  0:04:45s\n",
      "epoch 10 | loss: 0.03918 | val_0_matthews: 0.98399 |  0:05:13s\n",
      "epoch 11 | loss: 0.03912 | val_0_matthews: 0.98404 |  0:05:41s\n",
      "epoch 12 | loss: 0.03887 | val_0_matthews: 0.98409 |  0:06:09s\n",
      "epoch 13 | loss: 0.03865 | val_0_matthews: 0.98418 |  0:06:37s\n",
      "epoch 14 | loss: 0.03855 | val_0_matthews: 0.98414 |  0:07:06s\n",
      "epoch 15 | loss: 0.03839 | val_0_matthews: 0.98418 |  0:07:34s\n",
      "epoch 16 | loss: 0.03829 | val_0_matthews: 0.98419 |  0:08:03s\n",
      "epoch 17 | loss: 0.03821 | val_0_matthews: 0.98403 |  0:08:31s\n",
      "epoch 18 | loss: 0.03797 | val_0_matthews: 0.98407 |  0:08:59s\n",
      "epoch 19 | loss: 0.03791 | val_0_matthews: 0.98408 |  0:09:27s\n",
      "epoch 20 | loss: 0.03774 | val_0_matthews: 0.98424 |  0:09:55s\n",
      "epoch 21 | loss: 0.03755 | val_0_matthews: 0.98422 |  0:10:24s\n",
      "epoch 22 | loss: 0.0376  | val_0_matthews: 0.98417 |  0:10:53s\n",
      "epoch 23 | loss: 0.03747 | val_0_matthews: 0.98391 |  0:11:21s\n",
      "epoch 24 | loss: 0.03727 | val_0_matthews: 0.98402 |  0:11:49s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 20 and best_val_0_matthews = 0.98424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 08:50:09,481] Trial 19 finished with value: 0.9842379093170166 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.005628031527250257, 'PRETRAIN_RATIO': 0.2331451385076096, 'N_D': 48, 'N_STEPS': 6, 'GAMMA': 1.2184648236988673, 'N_INDEPENDENT': 3, 'N_SHARED': 5, 'BATCH_SIZE': 12288, 'VIRT_BATCH_SIZE': 2048}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 11.75081| val_0_unsup_loss_numpy: 0.9845399856567383|  0:00:34s\n",
      "epoch 1  | loss: 0.91169 | val_0_unsup_loss_numpy: 0.79721999168396|  0:01:07s\n",
      "epoch 2  | loss: 0.78269 | val_0_unsup_loss_numpy: 0.6720600128173828|  0:01:41s\n",
      "epoch 3  | loss: 0.69418 | val_0_unsup_loss_numpy: 0.6045899987220764|  0:02:15s\n",
      "epoch 4  | loss: 0.63268 | val_0_unsup_loss_numpy: 0.5675600171089172|  0:02:49s\n",
      "epoch 5  | loss: 0.58798 | val_0_unsup_loss_numpy: 0.5561599731445312|  0:03:23s\n",
      "epoch 6  | loss: 0.55497 | val_0_unsup_loss_numpy: 0.5737000107765198|  0:03:57s\n",
      "epoch 7  | loss: 0.52715 | val_0_unsup_loss_numpy: 0.6079999804496765|  0:04:31s\n",
      "epoch 8  | loss: 0.51073 | val_0_unsup_loss_numpy: 0.6695299744606018|  0:05:05s\n",
      "epoch 9  | loss: 0.49011 | val_0_unsup_loss_numpy: 0.7868800163269043|  0:05:39s\n",
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 5 and best_val_0_unsup_loss_numpy = 0.5561599731445312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.18006 | val_0_matthews: 0.96785 |  0:00:29s\n",
      "epoch 1  | loss: 0.0536  | val_0_matthews: 0.97715 |  0:00:58s\n",
      "epoch 2  | loss: 0.04891 | val_0_matthews: 0.9788  |  0:01:28s\n",
      "epoch 3  | loss: 0.04693 | val_0_matthews: 0.97994 |  0:01:58s\n",
      "epoch 4  | loss: 0.04592 | val_0_matthews: 0.98066 |  0:02:28s\n",
      "epoch 5  | loss: 0.04543 | val_0_matthews: 0.9807  |  0:02:58s\n",
      "epoch 6  | loss: 0.04436 | val_0_matthews: 0.98118 |  0:03:27s\n",
      "epoch 7  | loss: 0.04399 | val_0_matthews: 0.98055 |  0:03:57s\n",
      "epoch 8  | loss: 0.04488 | val_0_matthews: 0.98143 |  0:04:26s\n",
      "epoch 9  | loss: 0.04331 | val_0_matthews: 0.98161 |  0:04:56s\n",
      "epoch 10 | loss: 0.04447 | val_0_matthews: 0.98135 |  0:05:26s\n",
      "epoch 11 | loss: 0.04334 | val_0_matthews: 0.98208 |  0:05:55s\n",
      "epoch 12 | loss: 0.04211 | val_0_matthews: 0.98183 |  0:06:25s\n",
      "epoch 13 | loss: 0.04147 | val_0_matthews: 0.98258 |  0:06:55s\n",
      "epoch 14 | loss: 0.04112 | val_0_matthews: 0.98243 |  0:07:24s\n",
      "epoch 15 | loss: 0.04156 | val_0_matthews: 0.98012 |  0:07:54s\n",
      "epoch 16 | loss: 0.04278 | val_0_matthews: 0.98193 |  0:08:24s\n",
      "epoch 17 | loss: 0.04203 | val_0_matthews: 0.98268 |  0:08:54s\n",
      "epoch 18 | loss: 0.04076 | val_0_matthews: 0.98295 |  0:09:24s\n",
      "epoch 19 | loss: 0.04059 | val_0_matthews: 0.98267 |  0:09:53s\n",
      "epoch 20 | loss: 0.04133 | val_0_matthews: 0.98276 |  0:10:22s\n",
      "epoch 21 | loss: 0.04013 | val_0_matthews: 0.98309 |  0:10:52s\n",
      "epoch 22 | loss: 0.03978 | val_0_matthews: 0.98316 |  0:11:22s\n",
      "epoch 23 | loss: 0.03949 | val_0_matthews: 0.98322 |  0:11:51s\n",
      "epoch 24 | loss: 0.03993 | val_0_matthews: 0.98322 |  0:12:21s\n",
      "epoch 25 | loss: 0.04028 | val_0_matthews: 0.98309 |  0:12:50s\n",
      "epoch 26 | loss: 0.04081 | val_0_matthews: 0.98244 |  0:13:20s\n",
      "epoch 27 | loss: 0.04065 | val_0_matthews: 0.98295 |  0:13:50s\n",
      "epoch 28 | loss: 0.03985 | val_0_matthews: 0.98334 |  0:14:19s\n",
      "epoch 29 | loss: 0.03928 | val_0_matthews: 0.9835  |  0:14:49s\n",
      "epoch 30 | loss: 0.0398  | val_0_matthews: 0.9831  |  0:15:19s\n",
      "epoch 31 | loss: 0.03952 | val_0_matthews: 0.98328 |  0:15:49s\n",
      "Stop training because you reached max_epochs = 32 with best_epoch = 29 and best_val_0_matthews = 0.9835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 09:21:34,340] Trial 20 finished with value: 0.9834973216056824 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.002886713387191909, 'PRETRAIN_RATIO': 0.29575047788146064, 'N_D': 32, 'N_STEPS': 5, 'GAMMA': 1.0775137388645561, 'N_INDEPENDENT': 5, 'N_SHARED': 4, 'BATCH_SIZE': 24576, 'VIRT_BATCH_SIZE': 1024}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.83658 | val_0_unsup_loss_numpy: 0.9282799959182739|  0:00:20s\n",
      "epoch 1  | loss: 0.82694 | val_0_unsup_loss_numpy: 0.7345899939537048|  0:00:40s\n",
      "epoch 2  | loss: 0.70008 | val_0_unsup_loss_numpy: 0.585349977016449|  0:01:01s\n",
      "epoch 3  | loss: 0.60786 | val_0_unsup_loss_numpy: 0.4814099967479706|  0:01:21s\n",
      "epoch 4  | loss: 0.53784 | val_0_unsup_loss_numpy: 0.42239001393318176|  0:01:42s\n",
      "epoch 5  | loss: 0.49509 | val_0_unsup_loss_numpy: 0.3740299940109253|  0:02:03s\n",
      "epoch 6  | loss: 0.44296 | val_0_unsup_loss_numpy: 0.35749998688697815|  0:02:24s\n",
      "epoch 7  | loss: 0.40546 | val_0_unsup_loss_numpy: 0.3252499997615814|  0:02:45s\n",
      "epoch 8  | loss: 0.37231 | val_0_unsup_loss_numpy: 0.3297500014305115|  0:03:06s\n",
      "epoch 9  | loss: 0.34095 | val_0_unsup_loss_numpy: 0.3359000086784363|  0:03:28s\n",
      "epoch 10 | loss: 0.31434 | val_0_unsup_loss_numpy: 0.31637999415397644|  0:03:48s\n",
      "epoch 11 | loss: 0.29246 | val_0_unsup_loss_numpy: 0.32308998703956604|  0:04:09s\n",
      "epoch 12 | loss: 0.28176 | val_0_unsup_loss_numpy: 0.31556999683380127|  0:04:30s\n",
      "epoch 13 | loss: 0.26788 | val_0_unsup_loss_numpy: 0.32857000827789307|  0:04:50s\n",
      "epoch 14 | loss: 0.25516 | val_0_unsup_loss_numpy: 0.33392998576164246|  0:05:11s\n",
      "epoch 15 | loss: 0.23565 | val_0_unsup_loss_numpy: 0.3352600038051605|  0:05:31s\n",
      "epoch 16 | loss: 0.2252  | val_0_unsup_loss_numpy: 0.3548099994659424|  0:05:52s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 12 and best_val_0_unsup_loss_numpy = 0.31556999683380127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.13996 | val_0_matthews: 0.97133 |  0:00:18s\n",
      "epoch 1  | loss: 0.04806 | val_0_matthews: 0.97998 |  0:00:37s\n",
      "epoch 2  | loss: 0.04417 | val_0_matthews: 0.98156 |  0:00:55s\n",
      "epoch 3  | loss: 0.0432  | val_0_matthews: 0.98214 |  0:01:13s\n",
      "epoch 4  | loss: 0.0419  | val_0_matthews: 0.98243 |  0:01:31s\n",
      "epoch 5  | loss: 0.04127 | val_0_matthews: 0.98281 |  0:01:49s\n",
      "epoch 6  | loss: 0.04079 | val_0_matthews: 0.98311 |  0:02:07s\n",
      "epoch 7  | loss: 0.04027 | val_0_matthews: 0.98333 |  0:02:26s\n",
      "epoch 8  | loss: 0.04011 | val_0_matthews: 0.98348 |  0:02:44s\n",
      "epoch 9  | loss: 0.03989 | val_0_matthews: 0.98329 |  0:03:03s\n",
      "epoch 10 | loss: 0.03961 | val_0_matthews: 0.98359 |  0:03:21s\n",
      "epoch 11 | loss: 0.03923 | val_0_matthews: 0.98356 |  0:03:40s\n",
      "epoch 12 | loss: 0.03916 | val_0_matthews: 0.98365 |  0:03:59s\n",
      "epoch 13 | loss: 0.03891 | val_0_matthews: 0.98358 |  0:04:17s\n",
      "epoch 14 | loss: 0.0386  | val_0_matthews: 0.98351 |  0:04:36s\n",
      "epoch 15 | loss: 0.03828 | val_0_matthews: 0.98366 |  0:04:54s\n",
      "epoch 16 | loss: 0.03816 | val_0_matthews: 0.984   |  0:05:12s\n",
      "epoch 17 | loss: 0.038   | val_0_matthews: 0.98387 |  0:05:30s\n",
      "epoch 18 | loss: 0.03789 | val_0_matthews: 0.98379 |  0:05:49s\n",
      "epoch 19 | loss: 0.0379  | val_0_matthews: 0.98397 |  0:06:07s\n",
      "epoch 20 | loss: 0.03761 | val_0_matthews: 0.98393 |  0:06:25s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 16 and best_val_0_matthews = 0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 09:38:55,848] Trial 21 finished with value: 0.9839977622032166 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.008137908306042444, 'PRETRAIN_RATIO': 0.41627064312560985, 'N_D': 64, 'N_STEPS': 4, 'GAMMA': 1.5464956271137225, 'N_INDEPENDENT': 4, 'N_SHARED': 4, 'BATCH_SIZE': 28672, 'VIRT_BATCH_SIZE': 1536}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.76027 | val_0_unsup_loss_numpy: 0.8467100262641907|  0:00:28s\n",
      "epoch 1  | loss: 0.76409 | val_0_unsup_loss_numpy: 0.7119200229644775|  0:00:56s\n",
      "epoch 2  | loss: 0.65553 | val_0_unsup_loss_numpy: 0.5343300104141235|  0:01:24s\n",
      "epoch 3  | loss: 0.56711 | val_0_unsup_loss_numpy: 0.3971099853515625|  0:01:53s\n",
      "epoch 4  | loss: 0.49105 | val_0_unsup_loss_numpy: 0.34446999430656433|  0:02:21s\n",
      "epoch 5  | loss: 0.43402 | val_0_unsup_loss_numpy: 0.29377999901771545|  0:02:49s\n",
      "epoch 6  | loss: 0.38014 | val_0_unsup_loss_numpy: 0.25718000531196594|  0:03:18s\n",
      "epoch 7  | loss: 0.33492 | val_0_unsup_loss_numpy: 0.24458999931812286|  0:03:46s\n",
      "epoch 8  | loss: 0.30089 | val_0_unsup_loss_numpy: 0.2597399950027466|  0:04:15s\n",
      "epoch 9  | loss: 0.27748 | val_0_unsup_loss_numpy: 0.24186000227928162|  0:04:43s\n",
      "epoch 10 | loss: 0.25017 | val_0_unsup_loss_numpy: 0.25409001111984253|  0:05:11s\n",
      "epoch 11 | loss: 0.23177 | val_0_unsup_loss_numpy: 0.24449999630451202|  0:05:39s\n",
      "epoch 12 | loss: 0.2135  | val_0_unsup_loss_numpy: 0.23892000317573547|  0:06:08s\n",
      "epoch 13 | loss: 0.19882 | val_0_unsup_loss_numpy: 0.23267999291419983|  0:06:36s\n",
      "epoch 14 | loss: 0.19004 | val_0_unsup_loss_numpy: 0.2369299978017807|  0:07:04s\n",
      "epoch 15 | loss: 0.1787  | val_0_unsup_loss_numpy: 0.22697000205516815|  0:07:33s\n",
      "epoch 16 | loss: 0.17008 | val_0_unsup_loss_numpy: 0.27869001030921936|  0:08:01s\n",
      "epoch 17 | loss: 0.16556 | val_0_unsup_loss_numpy: 0.27414000034332275|  0:08:30s\n",
      "epoch 18 | loss: 0.1594  | val_0_unsup_loss_numpy: 0.24478000402450562|  0:08:58s\n",
      "epoch 19 | loss: 0.15456 | val_0_unsup_loss_numpy: 0.25240999460220337|  0:09:26s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 15 and best_val_0_unsup_loss_numpy = 0.22697000205516815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.24625 | val_0_matthews: 0.97037 |  0:00:24s\n",
      "epoch 1  | loss: 0.04512 | val_0_matthews: 0.98065 |  0:00:49s\n",
      "epoch 2  | loss: 0.04253 | val_0_matthews: 0.98241 |  0:01:13s\n",
      "epoch 3  | loss: 0.04128 | val_0_matthews: 0.98287 |  0:01:37s\n",
      "epoch 4  | loss: 0.04023 | val_0_matthews: 0.98319 |  0:02:02s\n",
      "epoch 5  | loss: 0.03955 | val_0_matthews: 0.98379 |  0:02:26s\n",
      "epoch 6  | loss: 0.03941 | val_0_matthews: 0.98369 |  0:02:51s\n",
      "epoch 7  | loss: 0.03884 | val_0_matthews: 0.98393 |  0:03:16s\n",
      "epoch 8  | loss: 0.03868 | val_0_matthews: 0.98338 |  0:03:40s\n",
      "epoch 9  | loss: 0.03875 | val_0_matthews: 0.98412 |  0:04:04s\n",
      "epoch 10 | loss: 0.03809 | val_0_matthews: 0.98419 |  0:04:29s\n",
      "epoch 11 | loss: 0.03778 | val_0_matthews: 0.98401 |  0:04:54s\n",
      "epoch 12 | loss: 0.03768 | val_0_matthews: 0.98397 |  0:05:18s\n",
      "epoch 13 | loss: 0.03803 | val_0_matthews: 0.98412 |  0:05:42s\n",
      "epoch 14 | loss: 0.03749 | val_0_matthews: 0.9841  |  0:06:07s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 10 and best_val_0_matthews = 0.98419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 10:00:28,955] Trial 22 finished with value: 0.9841939210891724 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.04213174040038784, 'PRETRAIN_RATIO': 0.5599231252169645, 'N_D': 64, 'N_STEPS': 5, 'GAMMA': 1.7757822438304518, 'N_INDEPENDENT': 4, 'N_SHARED': 3, 'BATCH_SIZE': 28672, 'VIRT_BATCH_SIZE': 1024}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.99235 | val_0_unsup_loss_numpy: 0.733210027217865|  0:00:47s\n",
      "epoch 1  | loss: 0.58387 | val_0_unsup_loss_numpy: 0.44315001368522644|  0:01:35s\n",
      "epoch 2  | loss: 0.40865 | val_0_unsup_loss_numpy: 0.2972399890422821|  0:02:23s\n",
      "epoch 3  | loss: 0.30066 | val_0_unsup_loss_numpy: 0.24461999535560608|  0:03:11s\n",
      "epoch 4  | loss: 0.23616 | val_0_unsup_loss_numpy: 0.23799000680446625|  0:03:59s\n",
      "epoch 5  | loss: 0.1931  | val_0_unsup_loss_numpy: 0.25380998849868774|  0:04:47s\n",
      "epoch 6  | loss: 0.16319 | val_0_unsup_loss_numpy: 0.2863300144672394|  0:05:35s\n",
      "epoch 7  | loss: 0.1405  | val_0_unsup_loss_numpy: 0.3037300109863281|  0:06:24s\n",
      "epoch 8  | loss: 0.12309 | val_0_unsup_loss_numpy: 0.302480012178421|  0:07:12s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 4 and best_val_0_unsup_loss_numpy = 0.23799000680446625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.10462 | val_0_matthews: 0.98095 |  0:00:41s\n",
      "epoch 1  | loss: 0.0444  | val_0_matthews: 0.98242 |  0:01:23s\n",
      "epoch 2  | loss: 0.04269 | val_0_matthews: 0.98291 |  0:02:06s\n",
      "epoch 3  | loss: 0.04174 | val_0_matthews: 0.98348 |  0:02:48s\n",
      "epoch 4  | loss: 0.04104 | val_0_matthews: 0.98364 |  0:03:30s\n",
      "epoch 5  | loss: 0.0405  | val_0_matthews: 0.98367 |  0:04:12s\n",
      "epoch 6  | loss: 0.04016 | val_0_matthews: 0.98367 |  0:04:54s\n",
      "epoch 7  | loss: 0.03986 | val_0_matthews: 0.98393 |  0:05:36s\n",
      "epoch 8  | loss: 0.03958 | val_0_matthews: 0.98394 |  0:06:18s\n",
      "epoch 9  | loss: 0.03929 | val_0_matthews: 0.98394 |  0:07:00s\n",
      "epoch 10 | loss: 0.03904 | val_0_matthews: 0.98395 |  0:07:42s\n",
      "epoch 11 | loss: 0.03891 | val_0_matthews: 0.98402 |  0:08:24s\n",
      "epoch 12 | loss: 0.0387  | val_0_matthews: 0.98405 |  0:09:06s\n",
      "epoch 13 | loss: 0.03848 | val_0_matthews: 0.98403 |  0:09:49s\n",
      "epoch 14 | loss: 0.03832 | val_0_matthews: 0.98409 |  0:10:31s\n",
      "epoch 15 | loss: 0.03817 | val_0_matthews: 0.98403 |  0:11:12s\n",
      "epoch 16 | loss: 0.03806 | val_0_matthews: 0.9843  |  0:11:55s\n",
      "epoch 17 | loss: 0.03792 | val_0_matthews: 0.98402 |  0:12:37s\n",
      "epoch 18 | loss: 0.0377  | val_0_matthews: 0.98413 |  0:13:18s\n",
      "epoch 19 | loss: 0.03727 | val_0_matthews: 0.98414 |  0:14:00s\n",
      "epoch 20 | loss: 0.03719 | val_0_matthews: 0.98402 |  0:14:42s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 16 and best_val_0_matthews = 0.9843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 10:27:07,193] Trial 23 finished with value: 0.9843036532402039 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.016447022052991624, 'PRETRAIN_RATIO': 0.2636290023151164, 'N_D': 64, 'N_STEPS': 4, 'GAMMA': 1.390536856512239, 'N_INDEPENDENT': 5, 'N_SHARED': 4, 'BATCH_SIZE': 24576, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.88988 | val_0_unsup_loss_numpy: 0.7562800049781799|  0:00:37s\n",
      "epoch 1  | loss: 0.68051 | val_0_unsup_loss_numpy: 0.6021299958229065|  0:01:15s\n",
      "epoch 2  | loss: 0.55001 | val_0_unsup_loss_numpy: 0.46070998907089233|  0:01:52s\n",
      "epoch 3  | loss: 0.47704 | val_0_unsup_loss_numpy: 0.3342300057411194|  0:02:30s\n",
      "epoch 4  | loss: 0.4023  | val_0_unsup_loss_numpy: 0.3099200129508972|  0:03:08s\n",
      "epoch 5  | loss: 0.36104 | val_0_unsup_loss_numpy: 0.3105100095272064|  0:03:46s\n",
      "epoch 6  | loss: 0.33349 | val_0_unsup_loss_numpy: 0.324290007352829|  0:04:24s\n",
      "epoch 7  | loss: 0.30589 | val_0_unsup_loss_numpy: 0.32592999935150146|  0:05:02s\n",
      "epoch 8  | loss: 0.28655 | val_0_unsup_loss_numpy: 0.33652999997138977|  0:05:40s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 4 and best_val_0_unsup_loss_numpy = 0.3099200129508972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.24378 | val_0_matthews: 0.97412 |  0:00:33s\n",
      "epoch 1  | loss: 0.04693 | val_0_matthews: 0.98099 |  0:01:06s\n",
      "epoch 2  | loss: 0.04268 | val_0_matthews: 0.9823  |  0:01:39s\n",
      "epoch 3  | loss: 0.04125 | val_0_matthews: 0.9826  |  0:02:12s\n",
      "epoch 4  | loss: 0.04    | val_0_matthews: 0.98329 |  0:02:45s\n",
      "epoch 5  | loss: 0.03951 | val_0_matthews: 0.98333 |  0:03:18s\n",
      "epoch 6  | loss: 0.03939 | val_0_matthews: 0.98342 |  0:03:52s\n",
      "epoch 7  | loss: 0.03867 | val_0_matthews: 0.98387 |  0:04:25s\n",
      "epoch 8  | loss: 0.0382  | val_0_matthews: 0.98374 |  0:04:58s\n",
      "epoch 9  | loss: 0.03784 | val_0_matthews: 0.98387 |  0:05:31s\n",
      "epoch 10 | loss: 0.03757 | val_0_matthews: 0.98405 |  0:06:04s\n",
      "epoch 11 | loss: 0.03748 | val_0_matthews: 0.98381 |  0:06:37s\n",
      "epoch 12 | loss: 0.03727 | val_0_matthews: 0.9839  |  0:07:11s\n",
      "epoch 13 | loss: 0.03726 | val_0_matthews: 0.98415 |  0:07:44s\n",
      "epoch 14 | loss: 0.03691 | val_0_matthews: 0.98402 |  0:08:17s\n",
      "epoch 15 | loss: 0.03685 | val_0_matthews: 0.98406 |  0:08:50s\n",
      "epoch 16 | loss: 0.03655 | val_0_matthews: 0.98392 |  0:09:23s\n",
      "epoch 17 | loss: 0.03637 | val_0_matthews: 0.984   |  0:09:56s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 13 and best_val_0_matthews = 0.98415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 10:49:23,749] Trial 24 finished with value: 0.9841451644897461 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.035928624802915046, 'PRETRAIN_RATIO': 0.3156414411687494, 'N_D': 48, 'N_STEPS': 6, 'GAMMA': 1.5195229376764352, 'N_INDEPENDENT': 4, 'N_SHARED': 5, 'BATCH_SIZE': 28672, 'VIRT_BATCH_SIZE': 1024}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.10735 | val_0_unsup_loss_numpy: 0.7707599997520447|  0:00:28s\n",
      "epoch 1  | loss: 0.57459 | val_0_unsup_loss_numpy: 0.47446998953819275|  0:00:56s\n",
      "epoch 2  | loss: 0.43367 | val_0_unsup_loss_numpy: 0.28696000576019287|  0:01:24s\n",
      "epoch 3  | loss: 0.35658 | val_0_unsup_loss_numpy: 0.23989999294281006|  0:01:53s\n",
      "epoch 4  | loss: 0.29232 | val_0_unsup_loss_numpy: 0.20919999480247498|  0:02:20s\n",
      "epoch 5  | loss: 0.29192 | val_0_unsup_loss_numpy: 0.22116999328136444|  0:02:49s\n",
      "epoch 6  | loss: 0.25777 | val_0_unsup_loss_numpy: 0.17839999496936798|  0:03:17s\n",
      "epoch 7  | loss: 0.2151  | val_0_unsup_loss_numpy: 0.2323800027370453|  0:03:45s\n",
      "epoch 8  | loss: 0.19394 | val_0_unsup_loss_numpy: 0.16356000304222107|  0:04:13s\n",
      "epoch 9  | loss: 0.17707 | val_0_unsup_loss_numpy: 0.16322000324726105|  0:04:41s\n",
      "epoch 10 | loss: 0.17615 | val_0_unsup_loss_numpy: 0.1505099982023239|  0:05:09s\n",
      "epoch 11 | loss: 0.15935 | val_0_unsup_loss_numpy: 0.1624699980020523|  0:05:37s\n",
      "epoch 12 | loss: 0.14755 | val_0_unsup_loss_numpy: 0.14294999837875366|  0:06:05s\n",
      "epoch 13 | loss: 0.13565 | val_0_unsup_loss_numpy: 0.16143999993801117|  0:06:33s\n",
      "epoch 14 | loss: 0.12671 | val_0_unsup_loss_numpy: 0.157710000872612|  0:07:01s\n",
      "epoch 15 | loss: 0.11983 | val_0_unsup_loss_numpy: 0.1742199957370758|  0:07:29s\n",
      "epoch 16 | loss: 0.11977 | val_0_unsup_loss_numpy: 0.2103399932384491|  0:07:58s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 12 and best_val_0_unsup_loss_numpy = 0.14294999837875366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.22018 | val_0_matthews: 0.97981 |  0:00:24s\n",
      "epoch 1  | loss: 0.04327 | val_0_matthews: 0.98229 |  0:00:49s\n",
      "epoch 2  | loss: 0.04106 | val_0_matthews: 0.98304 |  0:01:13s\n",
      "epoch 3  | loss: 0.03971 | val_0_matthews: 0.98344 |  0:01:38s\n",
      "epoch 4  | loss: 0.03921 | val_0_matthews: 0.9837  |  0:02:03s\n",
      "epoch 5  | loss: 0.03882 | val_0_matthews: 0.98367 |  0:02:29s\n",
      "epoch 6  | loss: 0.03821 | val_0_matthews: 0.98384 |  0:02:54s\n",
      "epoch 7  | loss: 0.03797 | val_0_matthews: 0.98415 |  0:03:19s\n",
      "epoch 8  | loss: 0.03759 | val_0_matthews: 0.98405 |  0:03:43s\n",
      "epoch 9  | loss: 0.03725 | val_0_matthews: 0.98393 |  0:04:08s\n",
      "epoch 10 | loss: 0.03741 | val_0_matthews: 0.98429 |  0:04:33s\n",
      "epoch 11 | loss: 0.03696 | val_0_matthews: 0.98409 |  0:04:57s\n",
      "epoch 12 | loss: 0.03671 | val_0_matthews: 0.9841  |  0:05:22s\n",
      "epoch 13 | loss: 0.03661 | val_0_matthews: 0.98404 |  0:05:47s\n",
      "epoch 14 | loss: 0.03623 | val_0_matthews: 0.9841  |  0:06:12s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 10 and best_val_0_matthews = 0.98429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 11:11:01,435] Trial 25 finished with value: 0.9842923283576965 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.06159356759916198, 'PRETRAIN_RATIO': 0.37210990281860357, 'N_D': 64, 'N_STEPS': 5, 'GAMMA': 1.7847484731883285, 'N_INDEPENDENT': 5, 'N_SHARED': 4, 'BATCH_SIZE': 20480, 'VIRT_BATCH_SIZE': 1536}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.1787  | val_0_unsup_loss_numpy: 0.8519600033760071|  0:00:33s\n",
      "epoch 1  | loss: 0.77526 | val_0_unsup_loss_numpy: 0.6941499710083008|  0:01:06s\n",
      "epoch 2  | loss: 0.63317 | val_0_unsup_loss_numpy: 0.5088099837303162|  0:01:40s\n",
      "epoch 3  | loss: 0.53492 | val_0_unsup_loss_numpy: 0.41280001401901245|  0:02:13s\n",
      "epoch 4  | loss: 0.46067 | val_0_unsup_loss_numpy: 0.3429200053215027|  0:02:46s\n",
      "epoch 5  | loss: 0.40096 | val_0_unsup_loss_numpy: 0.3046700060367584|  0:03:20s\n",
      "epoch 6  | loss: 0.35724 | val_0_unsup_loss_numpy: 0.2782000005245209|  0:03:53s\n",
      "epoch 7  | loss: 0.32411 | val_0_unsup_loss_numpy: 0.2681199908256531|  0:04:26s\n",
      "epoch 8  | loss: 0.29796 | val_0_unsup_loss_numpy: 0.2576099932193756|  0:04:59s\n",
      "epoch 9  | loss: 0.2803  | val_0_unsup_loss_numpy: 0.2536500096321106|  0:05:33s\n",
      "epoch 10 | loss: 0.26116 | val_0_unsup_loss_numpy: 0.24539999663829803|  0:06:06s\n",
      "epoch 11 | loss: 0.2445  | val_0_unsup_loss_numpy: 0.24834999442100525|  0:06:39s\n",
      "epoch 12 | loss: 0.23103 | val_0_unsup_loss_numpy: 0.24371999502182007|  0:07:12s\n",
      "epoch 13 | loss: 0.22146 | val_0_unsup_loss_numpy: 0.24379000067710876|  0:07:45s\n",
      "epoch 14 | loss: 0.21488 | val_0_unsup_loss_numpy: 0.2412700057029724|  0:08:18s\n",
      "epoch 15 | loss: 0.20352 | val_0_unsup_loss_numpy: 0.24626000225543976|  0:08:51s\n",
      "epoch 16 | loss: 0.19921 | val_0_unsup_loss_numpy: 0.2434999942779541|  0:09:25s\n",
      "epoch 17 | loss: 0.19121 | val_0_unsup_loss_numpy: 0.24724000692367554|  0:09:58s\n",
      "epoch 18 | loss: 0.18558 | val_0_unsup_loss_numpy: 0.24583999812602997|  0:10:31s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 14 and best_val_0_unsup_loss_numpy = 0.2412700057029724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.10939 | val_0_matthews: 0.98049 |  0:00:27s\n",
      "epoch 1  | loss: 0.04374 | val_0_matthews: 0.98276 |  0:00:55s\n",
      "epoch 2  | loss: 0.04231 | val_0_matthews: 0.98328 |  0:01:22s\n",
      "epoch 3  | loss: 0.04127 | val_0_matthews: 0.98369 |  0:01:50s\n",
      "epoch 4  | loss: 0.0407  | val_0_matthews: 0.98349 |  0:02:18s\n",
      "epoch 5  | loss: 0.04013 | val_0_matthews: 0.98393 |  0:02:45s\n",
      "epoch 6  | loss: 0.03976 | val_0_matthews: 0.98375 |  0:03:13s\n",
      "epoch 7  | loss: 0.03926 | val_0_matthews: 0.98395 |  0:03:41s\n",
      "epoch 8  | loss: 0.03928 | val_0_matthews: 0.98383 |  0:04:08s\n",
      "epoch 9  | loss: 0.03922 | val_0_matthews: 0.98399 |  0:04:36s\n",
      "epoch 10 | loss: 0.03862 | val_0_matthews: 0.98398 |  0:05:04s\n",
      "epoch 11 | loss: 0.0382  | val_0_matthews: 0.98421 |  0:05:32s\n",
      "epoch 12 | loss: 0.03794 | val_0_matthews: 0.98414 |  0:06:00s\n",
      "epoch 13 | loss: 0.03784 | val_0_matthews: 0.98412 |  0:06:27s\n",
      "epoch 14 | loss: 0.0377  | val_0_matthews: 0.98403 |  0:06:54s\n",
      "epoch 15 | loss: 0.03758 | val_0_matthews: 0.98422 |  0:07:22s\n",
      "epoch 16 | loss: 0.03742 | val_0_matthews: 0.98408 |  0:07:50s\n",
      "epoch 17 | loss: 0.03728 | val_0_matthews: 0.98423 |  0:08:17s\n",
      "epoch 18 | loss: 0.03695 | val_0_matthews: 0.98391 |  0:08:45s\n",
      "epoch 19 | loss: 0.03696 | val_0_matthews: 0.98412 |  0:09:13s\n",
      "epoch 20 | loss: 0.03667 | val_0_matthews: 0.98413 |  0:09:40s\n",
      "epoch 21 | loss: 0.03666 | val_0_matthews: 0.98416 |  0:10:08s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 17 and best_val_0_matthews = 0.98423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 11:36:19,108] Trial 26 finished with value: 0.9842337965965271 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.02093579859503632, 'PRETRAIN_RATIO': 0.459717202950811, 'N_D': 48, 'N_STEPS': 4, 'GAMMA': 1.4126584525314252, 'N_INDEPENDENT': 3, 'N_SHARED': 2, 'BATCH_SIZE': 24576, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.36365 | val_0_unsup_loss_numpy: 0.9721199870109558|  0:00:20s\n",
      "epoch 1  | loss: 0.87385 | val_0_unsup_loss_numpy: 0.8472499847412109|  0:00:41s\n",
      "epoch 2  | loss: 0.78632 | val_0_unsup_loss_numpy: 0.7091100215911865|  0:01:01s\n",
      "epoch 3  | loss: 0.71754 | val_0_unsup_loss_numpy: 0.5979499816894531|  0:01:21s\n",
      "epoch 4  | loss: 0.6548  | val_0_unsup_loss_numpy: 0.517769992351532|  0:01:42s\n",
      "epoch 5  | loss: 0.61976 | val_0_unsup_loss_numpy: 0.49202001094818115|  0:02:02s\n",
      "epoch 6  | loss: 0.56071 | val_0_unsup_loss_numpy: 0.42941001057624817|  0:02:22s\n",
      "epoch 7  | loss: 0.52038 | val_0_unsup_loss_numpy: 0.40015000104904175|  0:02:42s\n",
      "epoch 8  | loss: 0.48726 | val_0_unsup_loss_numpy: 0.3816699981689453|  0:03:03s\n",
      "epoch 9  | loss: 0.46093 | val_0_unsup_loss_numpy: 0.35561999678611755|  0:03:24s\n",
      "epoch 10 | loss: 0.43837 | val_0_unsup_loss_numpy: 0.34292998909950256|  0:03:45s\n",
      "epoch 11 | loss: 0.42918 | val_0_unsup_loss_numpy: 0.3467099964618683|  0:04:05s\n",
      "epoch 12 | loss: 0.40764 | val_0_unsup_loss_numpy: 0.3352000117301941|  0:04:26s\n",
      "epoch 13 | loss: 0.38623 | val_0_unsup_loss_numpy: 0.32354000210762024|  0:04:46s\n",
      "epoch 14 | loss: 0.36679 | val_0_unsup_loss_numpy: 0.3086700141429901|  0:05:06s\n",
      "epoch 15 | loss: 0.3609  | val_0_unsup_loss_numpy: 0.3154999911785126|  0:05:26s\n",
      "epoch 16 | loss: 0.34918 | val_0_unsup_loss_numpy: 0.28870001435279846|  0:05:47s\n",
      "epoch 17 | loss: 0.33816 | val_0_unsup_loss_numpy: 0.30866000056266785|  0:06:07s\n",
      "epoch 18 | loss: 0.32454 | val_0_unsup_loss_numpy: 0.2722899913787842|  0:06:28s\n",
      "epoch 19 | loss: 0.31262 | val_0_unsup_loss_numpy: 0.27077001333236694|  0:06:48s\n",
      "epoch 20 | loss: 0.30799 | val_0_unsup_loss_numpy: 0.2708599865436554|  0:07:09s\n",
      "epoch 21 | loss: 0.30813 | val_0_unsup_loss_numpy: 0.26627999544143677|  0:07:30s\n",
      "epoch 22 | loss: 0.29499 | val_0_unsup_loss_numpy: 0.2502099871635437|  0:07:50s\n",
      "epoch 23 | loss: 0.29041 | val_0_unsup_loss_numpy: 0.24278999865055084|  0:08:11s\n",
      "epoch 24 | loss: 0.27528 | val_0_unsup_loss_numpy: 0.24320000410079956|  0:08:31s\n",
      "epoch 25 | loss: 0.28041 | val_0_unsup_loss_numpy: 0.2429800033569336|  0:08:51s\n",
      "epoch 26 | loss: 0.26751 | val_0_unsup_loss_numpy: 0.23893000185489655|  0:09:12s\n",
      "epoch 27 | loss: 0.25978 | val_0_unsup_loss_numpy: 0.2372100055217743|  0:09:32s\n",
      "epoch 28 | loss: 0.25861 | val_0_unsup_loss_numpy: 0.2526000142097473|  0:09:52s\n",
      "epoch 29 | loss: 0.25928 | val_0_unsup_loss_numpy: 0.22653000056743622|  0:10:13s\n",
      "epoch 30 | loss: 0.24727 | val_0_unsup_loss_numpy: 0.2197200059890747|  0:10:33s\n",
      "epoch 31 | loss: 0.24444 | val_0_unsup_loss_numpy: 0.2278199940919876|  0:10:54s\n",
      "Stop training because you reached max_epochs = 32 with best_epoch = 30 and best_val_0_unsup_loss_numpy = 0.2197200059890747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.09849 | val_0_matthews: 0.96098 |  0:00:17s\n",
      "epoch 1  | loss: 0.04353 | val_0_matthews: 0.98207 |  0:00:35s\n",
      "epoch 2  | loss: 0.04211 | val_0_matthews: 0.98285 |  0:00:52s\n",
      "epoch 3  | loss: 0.04137 | val_0_matthews: 0.98308 |  0:01:10s\n",
      "epoch 4  | loss: 0.04081 | val_0_matthews: 0.98346 |  0:01:28s\n",
      "epoch 5  | loss: 0.04028 | val_0_matthews: 0.98364 |  0:01:46s\n",
      "epoch 6  | loss: 0.04012 | val_0_matthews: 0.98378 |  0:02:04s\n",
      "epoch 7  | loss: 0.03989 | val_0_matthews: 0.98392 |  0:02:22s\n",
      "epoch 8  | loss: 0.03954 | val_0_matthews: 0.98386 |  0:02:40s\n",
      "epoch 9  | loss: 0.03921 | val_0_matthews: 0.98387 |  0:02:58s\n",
      "epoch 10 | loss: 0.03899 | val_0_matthews: 0.9841  |  0:03:16s\n",
      "epoch 11 | loss: 0.0388  | val_0_matthews: 0.98415 |  0:03:34s\n",
      "epoch 12 | loss: 0.03852 | val_0_matthews: 0.984   |  0:03:52s\n",
      "epoch 13 | loss: 0.03849 | val_0_matthews: 0.98416 |  0:04:10s\n",
      "epoch 14 | loss: 0.03823 | val_0_matthews: 0.98417 |  0:04:27s\n",
      "epoch 15 | loss: 0.03811 | val_0_matthews: 0.9841  |  0:04:45s\n",
      "epoch 16 | loss: 0.03793 | val_0_matthews: 0.98387 |  0:05:03s\n",
      "epoch 17 | loss: 0.0379  | val_0_matthews: 0.98428 |  0:05:21s\n",
      "epoch 18 | loss: 0.0377  | val_0_matthews: 0.9841  |  0:05:39s\n",
      "epoch 19 | loss: 0.03795 | val_0_matthews: 0.98437 |  0:05:57s\n",
      "epoch 20 | loss: 0.03747 | val_0_matthews: 0.98405 |  0:06:15s\n",
      "epoch 21 | loss: 0.03734 | val_0_matthews: 0.98406 |  0:06:33s\n",
      "epoch 22 | loss: 0.03712 | val_0_matthews: 0.98443 |  0:06:51s\n",
      "epoch 23 | loss: 0.03696 | val_0_matthews: 0.98432 |  0:07:10s\n",
      "epoch 24 | loss: 0.03691 | val_0_matthews: 0.98401 |  0:07:28s\n",
      "epoch 25 | loss: 0.03673 | val_0_matthews: 0.98405 |  0:07:45s\n",
      "epoch 26 | loss: 0.03689 | val_0_matthews: 0.98415 |  0:08:03s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 22 and best_val_0_matthews = 0.98443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 12:00:04,695] Trial 27 finished with value: 0.9844347834587097 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.009455017760220272, 'PRETRAIN_RATIO': 0.6069394203276622, 'N_D': 64, 'N_STEPS': 5, 'GAMMA': 1.5536294653496454, 'N_INDEPENDENT': 4, 'N_SHARED': 3, 'BATCH_SIZE': 28672, 'VIRT_BATCH_SIZE': 2048}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 3.60178 | val_0_unsup_loss_numpy: 0.8803799748420715|  0:00:25s\n",
      "epoch 1  | loss: 0.81728 | val_0_unsup_loss_numpy: 0.7169100046157837|  0:00:52s\n",
      "epoch 2  | loss: 0.69587 | val_0_unsup_loss_numpy: 0.5940999984741211|  0:01:18s\n",
      "epoch 3  | loss: 0.61379 | val_0_unsup_loss_numpy: 0.5241000056266785|  0:01:44s\n",
      "epoch 4  | loss: 0.56117 | val_0_unsup_loss_numpy: 0.46682000160217285|  0:02:10s\n",
      "epoch 5  | loss: 0.51841 | val_0_unsup_loss_numpy: 0.4895800054073334|  0:02:36s\n",
      "epoch 6  | loss: 0.48661 | val_0_unsup_loss_numpy: 0.5471199750900269|  0:03:03s\n",
      "epoch 7  | loss: 0.47349 | val_0_unsup_loss_numpy: 0.4704500138759613|  0:03:29s\n",
      "epoch 8  | loss: 0.43813 | val_0_unsup_loss_numpy: 0.5678600072860718|  0:03:55s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 4 and best_val_0_unsup_loss_numpy = 0.46682000160217285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.10925 | val_0_matthews: 0.97452 |  0:00:22s\n",
      "epoch 1  | loss: 0.0501  | val_0_matthews: 0.97964 |  0:00:44s\n",
      "epoch 2  | loss: 0.04612 | val_0_matthews: 0.98083 |  0:01:06s\n",
      "epoch 3  | loss: 0.04459 | val_0_matthews: 0.98176 |  0:01:29s\n",
      "epoch 4  | loss: 0.04316 | val_0_matthews: 0.98162 |  0:01:51s\n",
      "epoch 5  | loss: 0.04223 | val_0_matthews: 0.98236 |  0:02:14s\n",
      "epoch 6  | loss: 0.04169 | val_0_matthews: 0.98284 |  0:02:37s\n",
      "epoch 7  | loss: 0.04076 | val_0_matthews: 0.98321 |  0:03:00s\n",
      "epoch 8  | loss: 0.04022 | val_0_matthews: 0.98334 |  0:03:22s\n",
      "epoch 9  | loss: 0.04    | val_0_matthews: 0.98355 |  0:03:44s\n",
      "epoch 10 | loss: 0.03979 | val_0_matthews: 0.98358 |  0:04:06s\n",
      "epoch 11 | loss: 0.0398  | val_0_matthews: 0.98358 |  0:04:29s\n",
      "epoch 12 | loss: 0.03922 | val_0_matthews: 0.9839  |  0:04:51s\n",
      "epoch 13 | loss: 0.03897 | val_0_matthews: 0.98378 |  0:05:14s\n",
      "epoch 14 | loss: 0.0388  | val_0_matthews: 0.9838  |  0:05:37s\n",
      "epoch 15 | loss: 0.03875 | val_0_matthews: 0.98387 |  0:05:59s\n",
      "epoch 16 | loss: 0.03864 | val_0_matthews: 0.98413 |  0:06:21s\n",
      "epoch 17 | loss: 0.0382  | val_0_matthews: 0.98354 |  0:06:43s\n",
      "epoch 18 | loss: 0.0382  | val_0_matthews: 0.98361 |  0:07:05s\n",
      "epoch 19 | loss: 0.03795 | val_0_matthews: 0.98382 |  0:07:28s\n",
      "epoch 20 | loss: 0.03789 | val_0_matthews: 0.98396 |  0:07:51s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 16 and best_val_0_matthews = 0.98413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 12:19:27,842] Trial 28 finished with value: 0.9841285347938538 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.008921861634883652, 'PRETRAIN_RATIO': 0.6733576106932886, 'N_D': 64, 'N_STEPS': 5, 'GAMMA': 1.2098961350480093, 'N_INDEPENDENT': 5, 'N_SHARED': 3, 'BATCH_SIZE': 16384, 'VIRT_BATCH_SIZE': 2048}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 10.93691| val_0_unsup_loss_numpy: 0.9609500169754028|  0:00:23s\n",
      "epoch 1  | loss: 0.85688 | val_0_unsup_loss_numpy: 0.7841399908065796|  0:00:46s\n",
      "epoch 2  | loss: 0.74852 | val_0_unsup_loss_numpy: 0.6861699819564819|  0:01:10s\n",
      "epoch 3  | loss: 0.69735 | val_0_unsup_loss_numpy: 0.625249981880188|  0:01:33s\n",
      "epoch 4  | loss: 0.6528  | val_0_unsup_loss_numpy: 0.6212499737739563|  0:01:56s\n",
      "epoch 5  | loss: 0.619   | val_0_unsup_loss_numpy: 0.5776299834251404|  0:02:19s\n",
      "epoch 6  | loss: 0.58391 | val_0_unsup_loss_numpy: 0.6214100122451782|  0:02:43s\n",
      "epoch 7  | loss: 0.5485  | val_0_unsup_loss_numpy: 0.655019998550415|  0:03:07s\n",
      "epoch 8  | loss: 0.51588 | val_0_unsup_loss_numpy: 0.49553000926971436|  0:03:30s\n",
      "epoch 9  | loss: 0.48959 | val_0_unsup_loss_numpy: 0.4390200078487396|  0:03:54s\n",
      "epoch 10 | loss: 0.46017 | val_0_unsup_loss_numpy: 0.40136000514030457|  0:04:18s\n",
      "epoch 11 | loss: 0.43659 | val_0_unsup_loss_numpy: 0.3949100077152252|  0:04:41s\n",
      "epoch 12 | loss: 0.41229 | val_0_unsup_loss_numpy: 0.3761399984359741|  0:05:04s\n",
      "epoch 13 | loss: 0.39307 | val_0_unsup_loss_numpy: 0.38001999258995056|  0:05:27s\n",
      "epoch 14 | loss: 0.37589 | val_0_unsup_loss_numpy: 0.3818100094795227|  0:05:50s\n",
      "epoch 15 | loss: 0.36211 | val_0_unsup_loss_numpy: 0.38920000195503235|  0:06:13s\n",
      "epoch 16 | loss: 0.34776 | val_0_unsup_loss_numpy: 0.41339999437332153|  0:06:37s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 12 and best_val_0_unsup_loss_numpy = 0.3761399984359741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.1633  | val_0_matthews: 0.96676 |  0:00:20s\n",
      "epoch 1  | loss: 0.05357 | val_0_matthews: 0.97744 |  0:00:40s\n",
      "epoch 2  | loss: 0.04944 | val_0_matthews: 0.97867 |  0:01:00s\n",
      "epoch 3  | loss: 0.04781 | val_0_matthews: 0.98005 |  0:01:20s\n",
      "epoch 4  | loss: 0.04635 | val_0_matthews: 0.97981 |  0:01:40s\n",
      "epoch 5  | loss: 0.04774 | val_0_matthews: 0.98091 |  0:01:59s\n",
      "epoch 6  | loss: 0.04397 | val_0_matthews: 0.98191 |  0:02:19s\n",
      "epoch 7  | loss: 0.04319 | val_0_matthews: 0.98219 |  0:02:39s\n",
      "epoch 8  | loss: 0.04179 | val_0_matthews: 0.98232 |  0:02:59s\n",
      "epoch 9  | loss: 0.04221 | val_0_matthews: 0.982   |  0:03:19s\n",
      "epoch 10 | loss: 0.04267 | val_0_matthews: 0.98202 |  0:03:40s\n",
      "epoch 11 | loss: 0.0419  | val_0_matthews: 0.98262 |  0:04:00s\n",
      "epoch 12 | loss: 0.04224 | val_0_matthews: 0.98225 |  0:04:20s\n",
      "epoch 13 | loss: 0.04322 | val_0_matthews: 0.98253 |  0:04:40s\n",
      "epoch 14 | loss: 0.04169 | val_0_matthews: 0.98253 |  0:05:01s\n",
      "epoch 15 | loss: 0.04238 | val_0_matthews: 0.98227 |  0:05:21s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 11 and best_val_0_matthews = 0.98262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 12:38:36,009] Trial 29 finished with value: 0.9826229214668274 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.004183517701398176, 'PRETRAIN_RATIO': 0.2508010046415097, 'N_D': 64, 'N_STEPS': 6, 'GAMMA': 1.5812725309054223, 'N_INDEPENDENT': 4, 'N_SHARED': 3, 'BATCH_SIZE': 28672, 'VIRT_BATCH_SIZE': 2048}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 14.0488 | val_0_unsup_loss_numpy: 1.0481699705123901|  0:00:19s\n",
      "epoch 1  | loss: 0.99806 | val_0_unsup_loss_numpy: 0.9459900259971619|  0:00:39s\n",
      "epoch 2  | loss: 0.94354 | val_0_unsup_loss_numpy: 0.8970800042152405|  0:00:59s\n",
      "epoch 3  | loss: 0.90061 | val_0_unsup_loss_numpy: 0.8422200083732605|  0:01:19s\n",
      "epoch 4  | loss: 0.85475 | val_0_unsup_loss_numpy: 0.7900699973106384|  0:01:39s\n",
      "epoch 5  | loss: 0.8157  | val_0_unsup_loss_numpy: 0.7494699954986572|  0:01:58s\n",
      "epoch 6  | loss: 0.78286 | val_0_unsup_loss_numpy: 0.707859992980957|  0:02:18s\n",
      "epoch 7  | loss: 0.75315 | val_0_unsup_loss_numpy: 0.6827800273895264|  0:02:38s\n",
      "epoch 8  | loss: 0.72526 | val_0_unsup_loss_numpy: 0.645330011844635|  0:02:57s\n",
      "epoch 9  | loss: 0.69979 | val_0_unsup_loss_numpy: 0.6269299983978271|  0:03:17s\n",
      "epoch 10 | loss: 0.67738 | val_0_unsup_loss_numpy: 0.6130200028419495|  0:03:37s\n",
      "epoch 11 | loss: 0.65773 | val_0_unsup_loss_numpy: 0.602940022945404|  0:03:56s\n",
      "epoch 12 | loss: 0.64017 | val_0_unsup_loss_numpy: 0.5934399962425232|  0:04:16s\n",
      "epoch 13 | loss: 0.62388 | val_0_unsup_loss_numpy: 0.5801200270652771|  0:04:37s\n",
      "epoch 14 | loss: 0.6105  | val_0_unsup_loss_numpy: 0.58160001039505|  0:04:57s\n",
      "epoch 15 | loss: 0.59483 | val_0_unsup_loss_numpy: 0.5649700164794922|  0:05:17s\n",
      "epoch 16 | loss: 0.58217 | val_0_unsup_loss_numpy: 0.5578699707984924|  0:05:36s\n",
      "epoch 17 | loss: 0.5686  | val_0_unsup_loss_numpy: 0.5516700148582458|  0:05:56s\n",
      "epoch 18 | loss: 0.5569  | val_0_unsup_loss_numpy: 0.5483199954032898|  0:06:16s\n",
      "epoch 19 | loss: 0.54578 | val_0_unsup_loss_numpy: 0.560230016708374|  0:06:35s\n",
      "epoch 20 | loss: 0.53396 | val_0_unsup_loss_numpy: 0.5563899874687195|  0:06:55s\n",
      "epoch 21 | loss: 0.52358 | val_0_unsup_loss_numpy: 0.5467100143432617|  0:07:15s\n",
      "epoch 22 | loss: 0.51402 | val_0_unsup_loss_numpy: 0.5479699969291687|  0:07:36s\n",
      "epoch 23 | loss: 0.50449 | val_0_unsup_loss_numpy: 0.5455700159072876|  0:07:56s\n",
      "epoch 24 | loss: 0.49786 | val_0_unsup_loss_numpy: 0.5581200122833252|  0:08:15s\n",
      "epoch 25 | loss: 0.4931  | val_0_unsup_loss_numpy: 0.5374299883842468|  0:08:35s\n",
      "epoch 26 | loss: 0.48223 | val_0_unsup_loss_numpy: 0.5214499831199646|  0:08:55s\n",
      "epoch 27 | loss: 0.47332 | val_0_unsup_loss_numpy: 0.5200300216674805|  0:09:14s\n",
      "epoch 28 | loss: 0.4657  | val_0_unsup_loss_numpy: 0.5084199905395508|  0:09:34s\n",
      "epoch 29 | loss: 0.45732 | val_0_unsup_loss_numpy: 0.5008100271224976|  0:09:54s\n",
      "epoch 30 | loss: 0.45048 | val_0_unsup_loss_numpy: 0.49616000056266785|  0:10:14s\n",
      "epoch 31 | loss: 0.44472 | val_0_unsup_loss_numpy: 0.49476000666618347|  0:10:34s\n",
      "Stop training because you reached max_epochs = 32 with best_epoch = 31 and best_val_0_unsup_loss_numpy = 0.49476000666618347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.14152 | val_0_matthews: 0.95367 |  0:00:17s\n",
      "epoch 1  | loss: 0.04965 | val_0_matthews: 0.97874 |  0:00:34s\n",
      "epoch 2  | loss: 0.046   | val_0_matthews: 0.98072 |  0:00:51s\n",
      "epoch 3  | loss: 0.0445  | val_0_matthews: 0.98131 |  0:01:07s\n",
      "epoch 4  | loss: 0.04331 | val_0_matthews: 0.98207 |  0:01:24s\n",
      "epoch 5  | loss: 0.04232 | val_0_matthews: 0.98253 |  0:01:41s\n",
      "epoch 6  | loss: 0.04173 | val_0_matthews: 0.98275 |  0:01:58s\n",
      "epoch 7  | loss: 0.04125 | val_0_matthews: 0.983   |  0:02:16s\n",
      "epoch 8  | loss: 0.04087 | val_0_matthews: 0.98294 |  0:02:33s\n",
      "epoch 9  | loss: 0.04053 | val_0_matthews: 0.98314 |  0:02:50s\n",
      "epoch 10 | loss: 0.0402  | val_0_matthews: 0.98326 |  0:03:07s\n",
      "epoch 11 | loss: 0.03993 | val_0_matthews: 0.98351 |  0:03:25s\n",
      "epoch 12 | loss: 0.03971 | val_0_matthews: 0.98331 |  0:03:41s\n",
      "epoch 13 | loss: 0.03942 | val_0_matthews: 0.98366 |  0:03:58s\n",
      "epoch 14 | loss: 0.03918 | val_0_matthews: 0.98357 |  0:04:15s\n",
      "epoch 15 | loss: 0.03906 | val_0_matthews: 0.98378 |  0:04:32s\n",
      "epoch 16 | loss: 0.03888 | val_0_matthews: 0.98367 |  0:04:49s\n",
      "epoch 17 | loss: 0.03879 | val_0_matthews: 0.98372 |  0:05:06s\n",
      "epoch 18 | loss: 0.03851 | val_0_matthews: 0.98378 |  0:05:23s\n",
      "epoch 19 | loss: 0.03845 | val_0_matthews: 0.9839  |  0:05:40s\n",
      "epoch 20 | loss: 0.03835 | val_0_matthews: 0.98381 |  0:05:58s\n",
      "epoch 21 | loss: 0.03812 | val_0_matthews: 0.9838  |  0:06:15s\n",
      "epoch 22 | loss: 0.03808 | val_0_matthews: 0.98375 |  0:06:32s\n",
      "epoch 23 | loss: 0.03793 | val_0_matthews: 0.98381 |  0:06:49s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 19 and best_val_0_matthews = 0.9839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 13:03:51,507] Trial 30 finished with value: 0.983896791934967 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.001857076340505676, 'PRETRAIN_RATIO': 0.5960392210993827, 'N_D': 48, 'N_STEPS': 5, 'GAMMA': 1.0535247050588514, 'N_INDEPENDENT': 4, 'N_SHARED': 2, 'BATCH_SIZE': 24576, 'VIRT_BATCH_SIZE': 2048}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.88849 | val_0_unsup_loss_numpy: 0.9936800003051758|  0:00:40s\n",
      "epoch 1  | loss: 0.94307 | val_0_unsup_loss_numpy: 0.967710018157959|  0:01:20s\n",
      "epoch 2  | loss: 0.89883 | val_0_unsup_loss_numpy: 0.922249972820282|  0:02:00s\n",
      "epoch 3  | loss: 0.851   | val_0_unsup_loss_numpy: 0.8320599794387817|  0:02:39s\n",
      "epoch 4  | loss: 0.806   | val_0_unsup_loss_numpy: 0.7554799914360046|  0:03:19s\n",
      "epoch 5  | loss: 0.7674  | val_0_unsup_loss_numpy: 0.6835799813270569|  0:03:59s\n",
      "epoch 6  | loss: 0.732   | val_0_unsup_loss_numpy: 0.633870005607605|  0:04:39s\n",
      "epoch 7  | loss: 0.70181 | val_0_unsup_loss_numpy: 0.6256300210952759|  0:05:19s\n",
      "epoch 8  | loss: 0.67506 | val_0_unsup_loss_numpy: 0.5879499912261963|  0:05:59s\n",
      "epoch 9  | loss: 0.64927 | val_0_unsup_loss_numpy: 0.5675699710845947|  0:06:39s\n",
      "epoch 10 | loss: 0.62871 | val_0_unsup_loss_numpy: 0.5473099946975708|  0:07:19s\n",
      "epoch 11 | loss: 0.60796 | val_0_unsup_loss_numpy: 0.5477499961853027|  0:07:59s\n",
      "epoch 12 | loss: 0.5895  | val_0_unsup_loss_numpy: 0.5275099873542786|  0:08:40s\n",
      "epoch 13 | loss: 0.56838 | val_0_unsup_loss_numpy: 0.5173699855804443|  0:09:19s\n",
      "epoch 14 | loss: 0.55035 | val_0_unsup_loss_numpy: 0.5037599802017212|  0:09:59s\n",
      "epoch 15 | loss: 0.53181 | val_0_unsup_loss_numpy: 0.5027700066566467|  0:10:39s\n",
      "epoch 16 | loss: 0.51458 | val_0_unsup_loss_numpy: 0.5020899772644043|  0:11:19s\n",
      "epoch 17 | loss: 0.49952 | val_0_unsup_loss_numpy: 0.47777000069618225|  0:11:59s\n",
      "epoch 18 | loss: 0.48233 | val_0_unsup_loss_numpy: 0.4878300130367279|  0:12:39s\n",
      "epoch 19 | loss: 0.46869 | val_0_unsup_loss_numpy: 0.4761900007724762|  0:13:18s\n",
      "epoch 20 | loss: 0.45558 | val_0_unsup_loss_numpy: 0.47328999638557434|  0:13:58s\n",
      "epoch 21 | loss: 0.44399 | val_0_unsup_loss_numpy: 0.46873000264167786|  0:14:38s\n",
      "epoch 22 | loss: 0.45624 | val_0_unsup_loss_numpy: 0.5245599746704102|  0:15:18s\n",
      "epoch 23 | loss: 0.43647 | val_0_unsup_loss_numpy: 0.4787899851799011|  0:15:58s\n",
      "epoch 24 | loss: 0.41841 | val_0_unsup_loss_numpy: 0.5128399729728699|  0:16:38s\n",
      "epoch 25 | loss: 0.40631 | val_0_unsup_loss_numpy: 0.5542200207710266|  0:17:18s\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 21 and best_val_0_unsup_loss_numpy = 0.46873000264167786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.18368 | val_0_matthews: 0.81089 |  0:00:34s\n",
      "epoch 1  | loss: 0.05306 | val_0_matthews: 0.97619 |  0:01:08s\n",
      "epoch 2  | loss: 0.04782 | val_0_matthews: 0.9796  |  0:01:41s\n",
      "epoch 3  | loss: 0.04528 | val_0_matthews: 0.98164 |  0:02:15s\n",
      "epoch 4  | loss: 0.04433 | val_0_matthews: 0.98223 |  0:02:49s\n",
      "epoch 5  | loss: 0.04272 | val_0_matthews: 0.98283 |  0:03:23s\n",
      "epoch 6  | loss: 0.04191 | val_0_matthews: 0.98307 |  0:03:57s\n",
      "epoch 7  | loss: 0.0412  | val_0_matthews: 0.98314 |  0:04:31s\n",
      "epoch 8  | loss: 0.04073 | val_0_matthews: 0.98345 |  0:05:06s\n",
      "epoch 9  | loss: 0.0404  | val_0_matthews: 0.98374 |  0:05:40s\n",
      "epoch 10 | loss: 0.04008 | val_0_matthews: 0.98365 |  0:06:14s\n",
      "epoch 11 | loss: 0.03979 | val_0_matthews: 0.98374 |  0:06:48s\n",
      "epoch 12 | loss: 0.03946 | val_0_matthews: 0.98393 |  0:07:21s\n",
      "epoch 13 | loss: 0.03922 | val_0_matthews: 0.98411 |  0:07:55s\n",
      "epoch 14 | loss: 0.03913 | val_0_matthews: 0.98406 |  0:08:29s\n",
      "epoch 15 | loss: 0.0389  | val_0_matthews: 0.98416 |  0:09:03s\n",
      "epoch 16 | loss: 0.03864 | val_0_matthews: 0.98415 |  0:09:37s\n",
      "epoch 17 | loss: 0.03884 | val_0_matthews: 0.98394 |  0:10:11s\n",
      "epoch 18 | loss: 0.0386  | val_0_matthews: 0.98411 |  0:10:44s\n",
      "epoch 19 | loss: 0.03834 | val_0_matthews: 0.98416 |  0:11:18s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 15 and best_val_0_matthews = 0.98416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 13:36:59,285] Trial 31 finished with value: 0.9841606020927429 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.0069086029673481285, 'PRETRAIN_RATIO': 0.6849528408268879, 'N_D': 64, 'N_STEPS': 4, 'GAMMA': 1.7123024367803872, 'N_INDEPENDENT': 3, 'N_SHARED': 4, 'BATCH_SIZE': 32768, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 5.32387 | val_0_unsup_loss_numpy: 0.9839699864387512|  0:00:23s\n",
      "epoch 1  | loss: 0.91502 | val_0_unsup_loss_numpy: 0.9300199747085571|  0:00:48s\n",
      "epoch 2  | loss: 0.83512 | val_0_unsup_loss_numpy: 0.8297200202941895|  0:01:12s\n",
      "epoch 3  | loss: 0.76954 | val_0_unsup_loss_numpy: 0.7752299904823303|  0:01:36s\n",
      "epoch 4  | loss: 0.71546 | val_0_unsup_loss_numpy: 0.6403800249099731|  0:02:00s\n",
      "epoch 5  | loss: 0.67707 | val_0_unsup_loss_numpy: 0.5692800283432007|  0:02:24s\n",
      "epoch 6  | loss: 0.64724 | val_0_unsup_loss_numpy: 0.5508999824523926|  0:02:48s\n",
      "epoch 7  | loss: 0.62085 | val_0_unsup_loss_numpy: 0.5321900248527527|  0:03:13s\n",
      "epoch 8  | loss: 0.59763 | val_0_unsup_loss_numpy: 0.5166299939155579|  0:03:37s\n",
      "epoch 9  | loss: 0.57863 | val_0_unsup_loss_numpy: 0.5042399764060974|  0:04:01s\n",
      "epoch 10 | loss: 0.56141 | val_0_unsup_loss_numpy: 0.5051100254058838|  0:04:25s\n",
      "epoch 11 | loss: 0.57187 | val_0_unsup_loss_numpy: 0.5547500252723694|  0:04:50s\n",
      "epoch 12 | loss: 0.55307 | val_0_unsup_loss_numpy: 0.4879800081253052|  0:05:13s\n",
      "epoch 13 | loss: 0.52878 | val_0_unsup_loss_numpy: 0.48541000485420227|  0:05:38s\n",
      "epoch 14 | loss: 0.51472 | val_0_unsup_loss_numpy: 0.4797700047492981|  0:06:02s\n",
      "epoch 15 | loss: 0.50197 | val_0_unsup_loss_numpy: 0.4859499931335449|  0:06:27s\n",
      "epoch 16 | loss: 0.49232 | val_0_unsup_loss_numpy: 0.4732399880886078|  0:06:50s\n",
      "epoch 17 | loss: 0.48083 | val_0_unsup_loss_numpy: 0.47679001092910767|  0:07:14s\n",
      "epoch 18 | loss: 0.47267 | val_0_unsup_loss_numpy: 0.4717099964618683|  0:07:38s\n",
      "epoch 19 | loss: 0.46385 | val_0_unsup_loss_numpy: 0.47870001196861267|  0:08:03s\n",
      "epoch 20 | loss: 0.4591  | val_0_unsup_loss_numpy: 0.47912999987602234|  0:08:27s\n",
      "epoch 21 | loss: 0.45316 | val_0_unsup_loss_numpy: 0.48012998700141907|  0:08:50s\n",
      "epoch 22 | loss: 0.44484 | val_0_unsup_loss_numpy: 0.4707599878311157|  0:09:14s\n",
      "epoch 23 | loss: 0.43619 | val_0_unsup_loss_numpy: 0.46320998668670654|  0:09:39s\n",
      "epoch 24 | loss: 0.43079 | val_0_unsup_loss_numpy: 0.46566998958587646|  0:10:04s\n",
      "epoch 25 | loss: 0.42624 | val_0_unsup_loss_numpy: 0.470550000667572|  0:10:28s\n",
      "epoch 26 | loss: 0.42509 | val_0_unsup_loss_numpy: 0.4885999858379364|  0:10:51s\n",
      "epoch 27 | loss: 0.41601 | val_0_unsup_loss_numpy: 0.48695001006126404|  0:11:16s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 23 and best_val_0_unsup_loss_numpy = 0.46320998668670654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.18568 | val_0_matthews: 0.92838 |  0:00:21s\n",
      "epoch 1  | loss: 0.04965 | val_0_matthews: 0.97821 |  0:00:41s\n",
      "epoch 2  | loss: 0.04576 | val_0_matthews: 0.98117 |  0:01:02s\n",
      "epoch 3  | loss: 0.04493 | val_0_matthews: 0.98146 |  0:01:23s\n",
      "epoch 4  | loss: 0.04359 | val_0_matthews: 0.98211 |  0:01:44s\n",
      "epoch 5  | loss: 0.04281 | val_0_matthews: 0.98243 |  0:02:05s\n",
      "epoch 6  | loss: 0.04235 | val_0_matthews: 0.98254 |  0:02:26s\n",
      "epoch 7  | loss: 0.04138 | val_0_matthews: 0.983   |  0:02:48s\n",
      "epoch 8  | loss: 0.04093 | val_0_matthews: 0.98271 |  0:03:09s\n",
      "epoch 9  | loss: 0.04098 | val_0_matthews: 0.98288 |  0:03:30s\n",
      "epoch 10 | loss: 0.04067 | val_0_matthews: 0.98354 |  0:03:51s\n",
      "epoch 11 | loss: 0.04002 | val_0_matthews: 0.98348 |  0:04:13s\n",
      "epoch 12 | loss: 0.03982 | val_0_matthews: 0.98351 |  0:04:34s\n",
      "epoch 13 | loss: 0.03961 | val_0_matthews: 0.98409 |  0:04:54s\n",
      "epoch 14 | loss: 0.03945 | val_0_matthews: 0.98361 |  0:05:15s\n",
      "epoch 15 | loss: 0.03926 | val_0_matthews: 0.9838  |  0:05:36s\n",
      "epoch 16 | loss: 0.03901 | val_0_matthews: 0.98387 |  0:05:57s\n",
      "epoch 17 | loss: 0.03881 | val_0_matthews: 0.98383 |  0:06:18s\n",
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 13 and best_val_0_matthews = 0.98409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 14:00:49,857] Trial 32 finished with value: 0.9840894341468811 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.0109565890067268, 'PRETRAIN_RATIO': 0.7209800616011073, 'N_D': 48, 'N_STEPS': 5, 'GAMMA': 1.3390487937800193, 'N_INDEPENDENT': 5, 'N_SHARED': 3, 'BATCH_SIZE': 28672, 'VIRT_BATCH_SIZE': 1536}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.2393  | val_0_unsup_loss_numpy: 0.9389399886131287|  0:00:23s\n",
      "epoch 1  | loss: 0.83843 | val_0_unsup_loss_numpy: 0.8856199979782104|  0:00:47s\n",
      "epoch 2  | loss: 0.74906 | val_0_unsup_loss_numpy: 0.7801799774169922|  0:01:11s\n",
      "epoch 3  | loss: 0.68338 | val_0_unsup_loss_numpy: 0.6786999702453613|  0:01:35s\n",
      "epoch 4  | loss: 0.63212 | val_0_unsup_loss_numpy: 0.5717300176620483|  0:01:59s\n",
      "epoch 5  | loss: 0.58545 | val_0_unsup_loss_numpy: 0.47661998867988586|  0:02:24s\n",
      "epoch 6  | loss: 0.54912 | val_0_unsup_loss_numpy: 0.41266998648643494|  0:02:47s\n",
      "epoch 7  | loss: 0.51342 | val_0_unsup_loss_numpy: 0.38655000925064087|  0:03:11s\n",
      "epoch 8  | loss: 0.4774  | val_0_unsup_loss_numpy: 0.34558001160621643|  0:03:36s\n",
      "epoch 9  | loss: 0.44077 | val_0_unsup_loss_numpy: 0.3303599953651428|  0:04:00s\n",
      "epoch 10 | loss: 0.41297 | val_0_unsup_loss_numpy: 0.32881999015808105|  0:04:25s\n",
      "epoch 11 | loss: 0.38115 | val_0_unsup_loss_numpy: 0.3193899989128113|  0:04:49s\n",
      "epoch 12 | loss: 0.36404 | val_0_unsup_loss_numpy: 0.26381999254226685|  0:05:13s\n",
      "epoch 13 | loss: 0.34169 | val_0_unsup_loss_numpy: 0.25971999764442444|  0:05:37s\n",
      "epoch 14 | loss: 0.32376 | val_0_unsup_loss_numpy: 0.29434001445770264|  0:06:02s\n",
      "epoch 15 | loss: 0.32572 | val_0_unsup_loss_numpy: 0.2849099934101105|  0:06:26s\n",
      "epoch 16 | loss: 0.31354 | val_0_unsup_loss_numpy: 0.25501999258995056|  0:06:50s\n",
      "epoch 17 | loss: 0.29343 | val_0_unsup_loss_numpy: 0.21818000078201294|  0:07:14s\n",
      "epoch 18 | loss: 0.30227 | val_0_unsup_loss_numpy: 0.20228999853134155|  0:07:38s\n",
      "epoch 19 | loss: 0.27163 | val_0_unsup_loss_numpy: 0.20423999428749084|  0:08:03s\n",
      "epoch 20 | loss: 0.25823 | val_0_unsup_loss_numpy: 0.19957000017166138|  0:08:27s\n",
      "epoch 21 | loss: 0.24844 | val_0_unsup_loss_numpy: 0.18345999717712402|  0:08:50s\n",
      "epoch 22 | loss: 0.2393  | val_0_unsup_loss_numpy: 0.1985899955034256|  0:09:14s\n",
      "epoch 23 | loss: 0.23444 | val_0_unsup_loss_numpy: 0.1811400055885315|  0:09:39s\n",
      "epoch 24 | loss: 0.23347 | val_0_unsup_loss_numpy: 0.17816999554634094|  0:10:03s\n",
      "epoch 25 | loss: 0.22574 | val_0_unsup_loss_numpy: 0.195700004696846|  0:10:27s\n",
      "epoch 26 | loss: 0.21873 | val_0_unsup_loss_numpy: 0.1744299978017807|  0:10:51s\n",
      "epoch 27 | loss: 0.2142  | val_0_unsup_loss_numpy: 0.17987999320030212|  0:11:15s\n",
      "epoch 28 | loss: 0.20765 | val_0_unsup_loss_numpy: 0.16728000342845917|  0:11:40s\n",
      "epoch 29 | loss: 0.20374 | val_0_unsup_loss_numpy: 0.18637999892234802|  0:12:04s\n",
      "epoch 30 | loss: 0.20642 | val_0_unsup_loss_numpy: 0.19062000513076782|  0:12:28s\n",
      "epoch 31 | loss: 0.19403 | val_0_unsup_loss_numpy: 0.17855000495910645|  0:12:52s\n",
      "Stop training because you reached max_epochs = 32 with best_epoch = 28 and best_val_0_unsup_loss_numpy = 0.16728000342845917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.19359 | val_0_matthews: 0.87942 |  0:00:21s\n",
      "epoch 1  | loss: 0.0466  | val_0_matthews: 0.97729 |  0:00:42s\n",
      "epoch 2  | loss: 0.04301 | val_0_matthews: 0.98165 |  0:01:03s\n",
      "epoch 3  | loss: 0.04138 | val_0_matthews: 0.9826  |  0:01:24s\n",
      "epoch 4  | loss: 0.04046 | val_0_matthews: 0.98284 |  0:01:46s\n",
      "epoch 5  | loss: 0.03992 | val_0_matthews: 0.98334 |  0:02:07s\n",
      "epoch 6  | loss: 0.03935 | val_0_matthews: 0.98338 |  0:02:28s\n",
      "epoch 7  | loss: 0.04166 | val_0_matthews: 0.9825  |  0:02:50s\n",
      "epoch 8  | loss: 0.0401  | val_0_matthews: 0.983   |  0:03:10s\n",
      "epoch 9  | loss: 0.03909 | val_0_matthews: 0.98348 |  0:03:31s\n",
      "epoch 10 | loss: 0.03895 | val_0_matthews: 0.98361 |  0:03:53s\n",
      "epoch 11 | loss: 0.03909 | val_0_matthews: 0.98185 |  0:04:14s\n",
      "epoch 12 | loss: 0.04223 | val_0_matthews: 0.98258 |  0:04:35s\n",
      "epoch 13 | loss: 0.03984 | val_0_matthews: 0.98353 |  0:04:56s\n",
      "epoch 14 | loss: 0.06556 | val_0_matthews: 0.98152 |  0:05:17s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 10 and best_val_0_matthews = 0.98361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 14:24:00,770] Trial 33 finished with value: 0.9836074709892273 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.017600148538125884, 'PRETRAIN_RATIO': 0.6239816804467341, 'N_D': 64, 'N_STEPS': 5, 'GAMMA': 1.508324293095727, 'N_INDEPENDENT': 4, 'N_SHARED': 4, 'BATCH_SIZE': 32768, 'VIRT_BATCH_SIZE': 1536}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.45998 | val_0_unsup_loss_numpy: 0.4886600077152252|  0:01:21s\n",
      "epoch 1  | loss: 0.44507 | val_0_unsup_loss_numpy: 0.2527399957180023|  0:02:42s\n",
      "epoch 2  | loss: 0.31934 | val_0_unsup_loss_numpy: 0.18241000175476074|  0:04:03s\n",
      "epoch 3  | loss: 0.25163 | val_0_unsup_loss_numpy: 0.14868000149726868|  0:05:25s\n",
      "epoch 4  | loss: 0.21411 | val_0_unsup_loss_numpy: 0.1342799961566925|  0:06:46s\n",
      "epoch 5  | loss: 0.18617 | val_0_unsup_loss_numpy: 0.13583000004291534|  0:08:08s\n",
      "epoch 6  | loss: 0.17815 | val_0_unsup_loss_numpy: 0.13752000033855438|  0:09:29s\n",
      "epoch 7  | loss: 0.16414 | val_0_unsup_loss_numpy: 0.14890000224113464|  0:10:51s\n",
      "epoch 8  | loss: 0.1549  | val_0_unsup_loss_numpy: 0.1345899999141693|  0:12:12s\n",
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 4 and best_val_0_unsup_loss_numpy = 0.1342799961566925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.08544 | val_0_matthews: 0.98083 |  0:01:09s\n",
      "epoch 1  | loss: 0.04427 | val_0_matthews: 0.9699  |  0:02:19s\n",
      "epoch 2  | loss: 0.043   | val_0_matthews: 0.98326 |  0:03:29s\n",
      "epoch 3  | loss: 0.04096 | val_0_matthews: 0.9834  |  0:04:39s\n",
      "epoch 4  | loss: 0.04011 | val_0_matthews: 0.98317 |  0:05:48s\n",
      "epoch 5  | loss: 0.03942 | val_0_matthews: 0.98382 |  0:06:58s\n",
      "epoch 6  | loss: 0.03903 | val_0_matthews: 0.98382 |  0:08:07s\n",
      "epoch 7  | loss: 0.03922 | val_0_matthews: 0.9839  |  0:09:17s\n",
      "epoch 8  | loss: 0.03883 | val_0_matthews: 0.98398 |  0:10:26s\n",
      "epoch 9  | loss: 0.03828 | val_0_matthews: 0.98386 |  0:11:35s\n",
      "epoch 10 | loss: 0.03808 | val_0_matthews: 0.98402 |  0:12:44s\n",
      "epoch 11 | loss: 0.0379  | val_0_matthews: 0.98371 |  0:13:54s\n",
      "epoch 12 | loss: 0.03873 | val_0_matthews: 0.98271 |  0:15:03s\n",
      "epoch 13 | loss: 0.03892 | val_0_matthews: 0.98401 |  0:16:13s\n",
      "epoch 14 | loss: 0.03826 | val_0_matthews: 0.98417 |  0:17:22s\n",
      "epoch 15 | loss: 0.03738 | val_0_matthews: 0.98434 |  0:18:31s\n",
      "epoch 16 | loss: 0.03718 | val_0_matthews: 0.98411 |  0:19:40s\n",
      "epoch 17 | loss: 0.03705 | val_0_matthews: 0.98417 |  0:20:49s\n",
      "epoch 18 | loss: 0.03672 | val_0_matthews: 0.98437 |  0:21:58s\n",
      "epoch 19 | loss: 0.03643 | val_0_matthews: 0.98413 |  0:23:08s\n",
      "epoch 20 | loss: 0.03633 | val_0_matthews: 0.98364 |  0:24:17s\n",
      "epoch 21 | loss: 0.03684 | val_0_matthews: 0.98416 |  0:25:27s\n",
      "epoch 22 | loss: 0.03639 | val_0_matthews: 0.98391 |  0:26:36s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 18 and best_val_0_matthews = 0.98437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 15:25:40,260] Trial 34 finished with value: 0.9843671321868896 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.033231158067555076, 'PRETRAIN_RATIO': 0.5249044735209788, 'N_D': 64, 'N_STEPS': 6, 'GAMMA': 1.4340765228296772, 'N_INDEPENDENT': 4, 'N_SHARED': 5, 'BATCH_SIZE': 8192, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.33293 | val_0_unsup_loss_numpy: 0.49215999245643616|  0:01:26s\n",
      "epoch 1  | loss: 0.43905 | val_0_unsup_loss_numpy: 0.2746100127696991|  0:02:52s\n",
      "epoch 2  | loss: 0.32454 | val_0_unsup_loss_numpy: 0.2367600053548813|  0:04:19s\n",
      "epoch 3  | loss: 0.26257 | val_0_unsup_loss_numpy: 0.24063000082969666|  0:05:46s\n",
      "epoch 4  | loss: 0.22645 | val_0_unsup_loss_numpy: 0.2093999981880188|  0:07:13s\n",
      "epoch 5  | loss: 0.22041 | val_0_unsup_loss_numpy: 0.1975799947977066|  0:08:40s\n",
      "epoch 6  | loss: 0.20982 | val_0_unsup_loss_numpy: 0.2429399937391281|  0:10:07s\n",
      "epoch 7  | loss: 0.18795 | val_0_unsup_loss_numpy: 0.1843400001525879|  0:11:34s\n",
      "epoch 8  | loss: 0.16872 | val_0_unsup_loss_numpy: 0.19351999461650848|  0:13:01s\n",
      "epoch 9  | loss: 0.15669 | val_0_unsup_loss_numpy: 0.17419999837875366|  0:14:28s\n",
      "epoch 10 | loss: 0.14623 | val_0_unsup_loss_numpy: 0.16106000542640686|  0:15:55s\n",
      "epoch 11 | loss: 0.14098 | val_0_unsup_loss_numpy: 0.17642000317573547|  0:17:22s\n",
      "epoch 12 | loss: 0.14988 | val_0_unsup_loss_numpy: 0.1902800053358078|  0:18:49s\n",
      "epoch 13 | loss: 0.14312 | val_0_unsup_loss_numpy: 0.18772000074386597|  0:20:16s\n",
      "epoch 14 | loss: 0.13774 | val_0_unsup_loss_numpy: 0.16572000086307526|  0:21:43s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 10 and best_val_0_unsup_loss_numpy = 0.16106000542640686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.06895 | val_0_matthews: 0.98192 |  0:01:15s\n",
      "epoch 1  | loss: 0.04434 | val_0_matthews: 0.98305 |  0:02:30s\n",
      "epoch 2  | loss: 0.04195 | val_0_matthews: 0.98349 |  0:03:46s\n",
      "epoch 3  | loss: 0.0412  | val_0_matthews: 0.98383 |  0:05:01s\n",
      "epoch 4  | loss: 0.04026 | val_0_matthews: 0.98384 |  0:06:16s\n",
      "epoch 5  | loss: 0.04035 | val_0_matthews: 0.98388 |  0:07:32s\n",
      "epoch 6  | loss: 0.03973 | val_0_matthews: 0.98364 |  0:08:47s\n",
      "epoch 7  | loss: 0.0397  | val_0_matthews: 0.98366 |  0:10:02s\n",
      "epoch 8  | loss: 0.0392  | val_0_matthews: 0.98412 |  0:11:18s\n",
      "epoch 9  | loss: 0.03888 | val_0_matthews: 0.98413 |  0:12:33s\n",
      "epoch 10 | loss: 0.03871 | val_0_matthews: 0.98394 |  0:13:49s\n",
      "epoch 11 | loss: 0.03856 | val_0_matthews: 0.98372 |  0:15:05s\n",
      "epoch 12 | loss: 0.03827 | val_0_matthews: 0.98381 |  0:16:20s\n",
      "epoch 13 | loss: 0.03824 | val_0_matthews: 0.98353 |  0:17:36s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 9 and best_val_0_matthews = 0.98413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 16:27:27,806] Trial 35 finished with value: 0.9841291904449463 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.01276935550122239, 'PRETRAIN_RATIO': 0.5419406746097819, 'N_D': 64, 'N_STEPS': 6, 'GAMMA': 1.2925695524248726, 'N_INDEPENDENT': 5, 'N_SHARED': 5, 'BATCH_SIZE': 8192, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.23932 | val_0_unsup_loss_numpy: 0.4798400104045868|  0:00:41s\n",
      "epoch 1  | loss: 0.47351 | val_0_unsup_loss_numpy: 0.3572799861431122|  0:01:22s\n",
      "epoch 2  | loss: 0.36411 | val_0_unsup_loss_numpy: 0.2829599976539612|  0:02:04s\n",
      "epoch 3  | loss: 0.30466 | val_0_unsup_loss_numpy: 0.2859100103378296|  0:02:46s\n",
      "epoch 4  | loss: 0.26769 | val_0_unsup_loss_numpy: 0.29673001170158386|  0:03:28s\n",
      "epoch 5  | loss: 0.2523  | val_0_unsup_loss_numpy: 0.26131001114845276|  0:04:10s\n",
      "epoch 6  | loss: 0.23063 | val_0_unsup_loss_numpy: 0.2656799852848053|  0:04:52s\n",
      "epoch 7  | loss: 0.25872 | val_0_unsup_loss_numpy: 0.27862000465393066|  0:05:33s\n",
      "epoch 8  | loss: 0.23349 | val_0_unsup_loss_numpy: 0.2698799967765808|  0:06:15s\n",
      "epoch 9  | loss: 0.21399 | val_0_unsup_loss_numpy: 0.3224300146102905|  0:06:57s\n",
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 5 and best_val_0_unsup_loss_numpy = 0.26131001114845276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.13858 | val_0_matthews: 0.97949 |  0:00:35s\n",
      "epoch 1  | loss: 0.04701 | val_0_matthews: 0.98145 |  0:01:11s\n",
      "epoch 2  | loss: 0.06053 | val_0_matthews: 0.88464 |  0:01:48s\n",
      "epoch 3  | loss: 0.05088 | val_0_matthews: 0.98181 |  0:02:24s\n",
      "epoch 4  | loss: 0.0429  | val_0_matthews: 0.98197 |  0:03:00s\n",
      "epoch 5  | loss: 0.04028 | val_0_matthews: 0.98324 |  0:03:36s\n",
      "epoch 6  | loss: 0.03884 | val_0_matthews: 0.98349 |  0:04:12s\n",
      "epoch 7  | loss: 0.03845 | val_0_matthews: 0.98376 |  0:04:48s\n",
      "epoch 8  | loss: 0.03823 | val_0_matthews: 0.98392 |  0:05:24s\n",
      "epoch 9  | loss: 0.03788 | val_0_matthews: 0.98361 |  0:06:00s\n",
      "epoch 10 | loss: 0.03761 | val_0_matthews: 0.9839  |  0:06:36s\n",
      "epoch 11 | loss: 0.03737 | val_0_matthews: 0.98401 |  0:07:12s\n",
      "epoch 12 | loss: 0.0372  | val_0_matthews: 0.98395 |  0:07:48s\n",
      "epoch 13 | loss: 0.03694 | val_0_matthews: 0.98397 |  0:08:24s\n",
      "epoch 14 | loss: 0.0377  | val_0_matthews: 0.98378 |  0:09:00s\n",
      "epoch 15 | loss: 0.03698 | val_0_matthews: 0.98388 |  0:09:36s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 11 and best_val_0_matthews = 0.98401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 17:08:01,727] Trial 36 finished with value: 0.9840121865272522 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.06043109034968528, 'PRETRAIN_RATIO': 0.4888862851092519, 'N_D': 48, 'N_STEPS': 6, 'GAMMA': 1.3816691263021421, 'N_INDEPENDENT': 4, 'N_SHARED': 5, 'BATCH_SIZE': 8192, 'VIRT_BATCH_SIZE': 2048}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 4.89402 | val_0_unsup_loss_numpy: 0.7171099781990051|  0:01:21s\n",
      "epoch 1  | loss: 0.71874 | val_0_unsup_loss_numpy: 0.5462300181388855|  0:02:42s\n",
      "epoch 2  | loss: 0.60504 | val_0_unsup_loss_numpy: 0.45263999700546265|  0:04:04s\n",
      "epoch 3  | loss: 0.53089 | val_0_unsup_loss_numpy: 0.3870599865913391|  0:05:26s\n",
      "epoch 4  | loss: 0.47427 | val_0_unsup_loss_numpy: 0.35227999091148376|  0:06:47s\n",
      "epoch 5  | loss: 0.43151 | val_0_unsup_loss_numpy: 0.34251999855041504|  0:08:09s\n",
      "epoch 6  | loss: 0.40022 | val_0_unsup_loss_numpy: 0.32927000522613525|  0:09:30s\n",
      "epoch 7  | loss: 0.37938 | val_0_unsup_loss_numpy: 0.3061099946498871|  0:10:50s\n",
      "epoch 8  | loss: 0.35627 | val_0_unsup_loss_numpy: 0.2902100086212158|  0:12:11s\n",
      "epoch 9  | loss: 0.33632 | val_0_unsup_loss_numpy: 0.2748500108718872|  0:13:32s\n",
      "epoch 10 | loss: 0.31481 | val_0_unsup_loss_numpy: 0.2649500072002411|  0:14:53s\n",
      "epoch 11 | loss: 0.29413 | val_0_unsup_loss_numpy: 0.24883000552654266|  0:16:13s\n",
      "epoch 12 | loss: 0.27855 | val_0_unsup_loss_numpy: 0.22126999497413635|  0:17:34s\n",
      "epoch 13 | loss: 0.25999 | val_0_unsup_loss_numpy: 0.23422999680042267|  0:18:55s\n",
      "epoch 14 | loss: 0.24617 | val_0_unsup_loss_numpy: 0.23362000286579132|  0:20:16s\n",
      "epoch 15 | loss: 0.23605 | val_0_unsup_loss_numpy: 0.20878000557422638|  0:21:36s\n",
      "epoch 16 | loss: 0.2267  | val_0_unsup_loss_numpy: 0.22806000709533691|  0:22:57s\n",
      "epoch 17 | loss: 0.21322 | val_0_unsup_loss_numpy: 0.21883000433444977|  0:24:17s\n",
      "epoch 18 | loss: 0.20491 | val_0_unsup_loss_numpy: 0.20135000348091125|  0:25:38s\n",
      "epoch 19 | loss: 0.19572 | val_0_unsup_loss_numpy: 0.2173299938440323|  0:26:59s\n",
      "epoch 20 | loss: 0.18802 | val_0_unsup_loss_numpy: 0.21028000116348267|  0:28:20s\n",
      "epoch 21 | loss: 0.18139 | val_0_unsup_loss_numpy: 0.21160000562667847|  0:29:40s\n",
      "epoch 22 | loss: 0.17596 | val_0_unsup_loss_numpy: 0.20716999471187592|  0:31:01s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 18 and best_val_0_unsup_loss_numpy = 0.20135000348091125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.10027 | val_0_matthews: 0.98035 |  0:01:09s\n",
      "epoch 1  | loss: 0.0466  | val_0_matthews: 0.97883 |  0:02:19s\n",
      "epoch 2  | loss: 0.04566 | val_0_matthews: 0.98229 |  0:03:28s\n",
      "epoch 3  | loss: 0.04243 | val_0_matthews: 0.98286 |  0:04:37s\n",
      "epoch 4  | loss: 0.04162 | val_0_matthews: 0.98275 |  0:05:46s\n",
      "epoch 5  | loss: 0.04131 | val_0_matthews: 0.983   |  0:06:55s\n",
      "epoch 6  | loss: 0.04117 | val_0_matthews: 0.98333 |  0:08:04s\n",
      "epoch 7  | loss: 0.03979 | val_0_matthews: 0.9837  |  0:09:13s\n",
      "epoch 8  | loss: 0.03985 | val_0_matthews: 0.98351 |  0:10:22s\n",
      "epoch 9  | loss: 0.03962 | val_0_matthews: 0.98364 |  0:11:32s\n",
      "epoch 10 | loss: 0.0388  | val_0_matthews: 0.98403 |  0:12:41s\n",
      "epoch 11 | loss: 0.03825 | val_0_matthews: 0.98402 |  0:13:50s\n",
      "epoch 12 | loss: 0.03792 | val_0_matthews: 0.984   |  0:14:59s\n",
      "epoch 13 | loss: 0.03776 | val_0_matthews: 0.98406 |  0:16:09s\n",
      "epoch 14 | loss: 0.03759 | val_0_matthews: 0.98402 |  0:17:18s\n",
      "epoch 15 | loss: 0.03742 | val_0_matthews: 0.98417 |  0:18:27s\n",
      "epoch 16 | loss: 0.03729 | val_0_matthews: 0.98413 |  0:19:37s\n",
      "epoch 17 | loss: 0.03764 | val_0_matthews: 0.98399 |  0:20:46s\n",
      "epoch 18 | loss: 0.03739 | val_0_matthews: 0.98414 |  0:21:55s\n",
      "epoch 19 | loss: 0.03732 | val_0_matthews: 0.98382 |  0:23:04s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 15 and best_val_0_matthews = 0.98417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 18:27:34,465] Trial 37 finished with value: 0.9841725826263428 and parameters: {'MASK_TYPE': 'sparsemax', 'LR': 0.004336589083005467, 'PRETRAIN_RATIO': 0.20161218651854507, 'N_D': 64, 'N_STEPS': 6, 'GAMMA': 1.4468071120637354, 'N_INDEPENDENT': 4, 'N_SHARED': 5, 'BATCH_SIZE': 8192, 'VIRT_BATCH_SIZE': 512}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.57443 | val_0_unsup_loss_numpy: 0.5666599869728088|  0:00:43s\n",
      "epoch 1  | loss: 0.5268  | val_0_unsup_loss_numpy: 0.4343799948692322|  0:01:26s\n",
      "epoch 2  | loss: 0.44146 | val_0_unsup_loss_numpy: 0.41635000705718994|  0:02:10s\n",
      "epoch 3  | loss: 0.39534 | val_0_unsup_loss_numpy: 0.4011000096797943|  0:02:52s\n",
      "epoch 4  | loss: 0.35592 | val_0_unsup_loss_numpy: 0.4153499901294708|  0:03:34s\n",
      "epoch 5  | loss: 0.33919 | val_0_unsup_loss_numpy: 0.6183000206947327|  0:04:15s\n",
      "epoch 6  | loss: 0.32704 | val_0_unsup_loss_numpy: 0.42208999395370483|  0:04:57s\n",
      "epoch 7  | loss: 0.31253 | val_0_unsup_loss_numpy: 0.41885000467300415|  0:05:38s\n",
      "\n",
      "Early stopping occurred at epoch 7 with best_epoch = 3 and best_val_0_unsup_loss_numpy = 0.4011000096797943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.12545 | val_0_matthews: 0.97667 |  0:00:34s\n",
      "epoch 1  | loss: 0.04789 | val_0_matthews: 0.98025 |  0:01:09s\n",
      "epoch 2  | loss: 0.04458 | val_0_matthews: 0.98159 |  0:01:44s\n",
      "epoch 3  | loss: 0.04314 | val_0_matthews: 0.98179 |  0:02:19s\n",
      "epoch 4  | loss: 0.0421  | val_0_matthews: 0.98264 |  0:02:55s\n",
      "epoch 5  | loss: 0.04129 | val_0_matthews: 0.98294 |  0:03:31s\n",
      "epoch 6  | loss: 0.04076 | val_0_matthews: 0.98286 |  0:04:07s\n",
      "epoch 7  | loss: 0.04029 | val_0_matthews: 0.98331 |  0:04:44s\n",
      "epoch 8  | loss: 0.04024 | val_0_matthews: 0.98345 |  0:05:20s\n",
      "epoch 9  | loss: 0.04049 | val_0_matthews: 0.98108 |  0:05:56s\n",
      "epoch 10 | loss: 0.04167 | val_0_matthews: 0.98295 |  0:06:32s\n",
      "epoch 11 | loss: 0.03968 | val_0_matthews: 0.98338 |  0:07:08s\n",
      "epoch 12 | loss: 0.04561 | val_0_matthews: 0.98238 |  0:07:43s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 8 and best_val_0_matthews = 0.98345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2024-08-27 18:57:20,812] Trial 38 finished with value: 0.9834517240524292 and parameters: {'MASK_TYPE': 'entmax', 'LR': 0.03596624603487079, 'PRETRAIN_RATIO': 0.4488193307896435, 'N_D': 32, 'N_STEPS': 6, 'GAMMA': 1.2540991633779877, 'N_INDEPENDENT': 5, 'N_SHARED': 2, 'BATCH_SIZE': 12288, 'VIRT_BATCH_SIZE': 1024}. Best is trial 5 with value: 0.9844775795936584.\n",
      "/home/gin/anaconda3/envs/env0/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 10.68997| val_0_unsup_loss_numpy: 0.9256200194358826|  0:00:45s\n",
      "epoch 1  | loss: 0.90927 | val_0_unsup_loss_numpy: 0.800819993019104|  0:01:29s\n",
      "epoch 2  | loss: 0.83755 | val_0_unsup_loss_numpy: 0.7321799993515015|  0:02:11s\n",
      "epoch 3  | loss: 0.78804 | val_0_unsup_loss_numpy: 0.6801900267601013|  0:02:53s\n",
      "epoch 4  | loss: 0.74276 | val_0_unsup_loss_numpy: 0.6269599795341492|  0:03:35s\n",
      "epoch 5  | loss: 0.70276 | val_0_unsup_loss_numpy: 0.5743200182914734|  0:04:17s\n",
      "epoch 6  | loss: 0.6666  | val_0_unsup_loss_numpy: 0.5295600295066833|  0:04:59s\n",
      "epoch 7  | loss: 0.63477 | val_0_unsup_loss_numpy: 0.4880799949169159|  0:05:40s\n",
      "epoch 8  | loss: 0.60644 | val_0_unsup_loss_numpy: 0.46779999136924744|  0:06:22s\n",
      "epoch 9  | loss: 0.57965 | val_0_unsup_loss_numpy: 0.4389300048351288|  0:07:04s\n",
      "epoch 10 | loss: 0.55494 | val_0_unsup_loss_numpy: 0.43615999817848206|  0:07:46s\n",
      "epoch 11 | loss: 0.53269 | val_0_unsup_loss_numpy: 0.40639999508857727|  0:08:28s\n",
      "epoch 12 | loss: 0.51163 | val_0_unsup_loss_numpy: 0.39939001202583313|  0:09:12s\n",
      "epoch 13 | loss: 0.49274 | val_0_unsup_loss_numpy: 0.4059999883174896|  0:09:52s\n"
     ]
    }
   ],
   "source": [
    "# sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "# # storage = optuna.storages.InMemoryStorage()\n",
    "\n",
    "# study = optuna.create_study(direction='maximize', sampler=sampler,\n",
    "#                             study_name='tabnet', storage='sqlite:///fcnn-study_matthews.db', load_if_exists=True,\n",
    "#                            )\n",
    "# study.optimize(objective, n_trials=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9534f-c916-4566-b15d-d1bea6197be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d30967-0b58-4674-b0aa-e01cc39f2500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f65b5-2d2c-489b-a1bd-a11f3f612093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d726df-f5b3-4608-8c49-fb82bea0b0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a541a6-0770-451b-8fad-f32f3045e70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc673187-9e52-425a-a6ff-cc1c3c3e4e3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "09c5c338-2716-4172-b689-f07882cc03cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2077964"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_test = Base_Dataset(df_test, is_test=True)\n",
    "display(len(dataset_test))\n",
    "BATCH_SIZE = 4096\n",
    "testloader = torch.utils.data.DataLoader(dataset_test,\n",
    "                                         batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                         num_workers=8, drop_last=False)\n",
    "X_test, y_test = make_embedded(embeddings, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7b96e1e4-4b30-4e2b-86b0-9bf22ec71711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = clf.predict(X_test)\n",
    "sub = (clf.predict_proba(X_test)[:, 1] > 0.47).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "51039419-6bfe-4580-b78d-a084ad6af668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3084814/954786539.py:2: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  submission.loc[:, 'class'] = sub\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('sample_submission.csv', index_col='id')\n",
    "submission.loc[:, 'class'] = sub\n",
    "idx2class = {0:'e', 1:'p'}\n",
    "submission.replace(idx2class, inplace=True)\n",
    "submission.to_csv(PATH+'submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d71c09-c05d-432f-9b75-0a3b04b300c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
